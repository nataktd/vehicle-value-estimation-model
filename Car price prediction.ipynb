{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5119733",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Изучение-данных-и-подготовка-к-работе\" data-toc-modified-id=\"Изучение-данных-и-подготовка-к-работе-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Изучение данных и подготовка к работе</a></span><ul class=\"toc-item\"><li><span><a href=\"#Методы-и-константы\" data-toc-modified-id=\"Методы-и-константы-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Методы и константы</a></span></li><li><span><a href=\"#Общая-информация-о-датасете-и-данных.\" data-toc-modified-id=\"Общая-информация-о-датасете-и-данных.-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Общая информация о датасете и данных.</a></span></li><li><span><a href=\"#Предобработка-данных.\" data-toc-modified-id=\"Предобработка-данных.-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Предобработка данных.</a></span><ul class=\"toc-item\"><li><span><a href=\"#Обработка-пропусков\" data-toc-modified-id=\"Обработка-пропусков-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Обработка пропусков</a></span></li><li><span><a href=\"#Дубликаты\" data-toc-modified-id=\"Дубликаты-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Дубликаты</a></span></li><li><span><a href=\"#Выбросы-и-аномальные-значения\" data-toc-modified-id=\"Выбросы-и-аномальные-значения-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Выбросы и аномальные значения</a></span></li><li><span><a href=\"#Смена-типа-данных\" data-toc-modified-id=\"Смена-типа-данных-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>Смена типа данных</a></span></li><li><span><a href=\"#Корреляция\" data-toc-modified-id=\"Корреляция-1.3.5\"><span class=\"toc-item-num\">1.3.5&nbsp;&nbsp;</span>Корреляция</a></span></li></ul></li></ul></li><li><span><a href=\"#Обучение-моделей\" data-toc-modified-id=\"Обучение-моделей-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение моделей</a></span><ul class=\"toc-item\"><li><span><a href=\"#Подготовка-выборок\" data-toc-modified-id=\"Подготовка-выборок-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Подготовка выборок</a></span></li><li><span><a href=\"#Обучение-алгоритмов\" data-toc-modified-id=\"Обучение-алгоритмов-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Обучение алгоритмов</a></span></li><li><span><a href=\"#Проверка-лучшей-модели-на-адекватность\" data-toc-modified-id=\"Проверка-лучшей-модели-на-адекватность-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Проверка лучшей модели на адекватность</a></span></li></ul></li><li><span><a href=\"#Проверка-лучшей-модели-на-тестовой-выборке\" data-toc-modified-id=\"Проверка-лучшей-модели-на-тестовой-выборке-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Проверка лучшей модели на тестовой выборке</a></span></li><li><span><a href=\"#Итоги-и-выводы\" data-toc-modified-id=\"Итоги-и-выводы-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Итоги и выводы</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d92fdba",
   "metadata": {},
   "source": [
    "# Описание проекта"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6832500",
   "metadata": {},
   "source": [
    "В рамках проекта предоставлены данные сервиса по продаже автомобилей с пробегом «Не бит, не крашен». В вашем распоряжении данные о технических характеристиках, комплектации и ценах других автомобилей.\n",
    "\n",
    "Сервис разрабатывает приложение, чтобы привлечь новых клиентов. В приложении можно будет узнать рыночную стоимость своего автомобиля. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61434a40",
   "metadata": {},
   "source": [
    "# Цель:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06750b85",
   "metadata": {},
   "source": [
    "Необходимо построить модель, которая умеет определять цену. \n",
    "\n",
    "Необходимо в работе учесть следующие критерии, которые важны заказчику:\n",
    "    \n",
    "- качество предсказания;\n",
    "\n",
    "- время обучения модели;\n",
    "\n",
    "- время предсказания модели."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b04f2",
   "metadata": {},
   "source": [
    "# *Примечания:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee5721c",
   "metadata": {},
   "source": [
    "Для оценки качества моделей нужно применить метрику `RMSE`.\n",
    "\n",
    "Освоить библиотеку `LightGBM` и её средствами построить модели градиентного бустинга.\n",
    "\n",
    "Найдем специальную команду для получения времени выполнения ячейки кода `Jupyter Notebook`.\n",
    "\n",
    "Поскольку модель градиентного бустинга может обучаться долго, изменим у неё только два-три параметра.\n",
    "\n",
    "Удалим лишние переменные оператором `del`: `del features_train`, если перестанет работать `Jupyter Notebook`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b92c95",
   "metadata": {},
   "source": [
    "# Описание данных:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02232f85",
   "metadata": {},
   "source": [
    "Данные находятся в файле `/datasets/autos.csv`. \n",
    "\n",
    "Признаки\n",
    "\n",
    "- `DateCrawled` — дата скачивания анкеты из базы\n",
    "\n",
    "- `VehicleType` — тип автомобильного кузова\n",
    "\n",
    "- `RegistrationYear` — год регистрации автомобиля\n",
    "\n",
    "- `Gearbox` — тип коробки передач\n",
    "\n",
    "- `Power` — мощность (л. с.)\n",
    "\n",
    "- `Model` — модель автомобиля\n",
    "\n",
    "- `Kilometer` — пробег (км)\n",
    "\n",
    "- `RegistrationMonth` — месяц регистрации автомобиля\n",
    "\n",
    "- `FuelType` — тип топлива\n",
    "\n",
    "- `Brand` — марка автомобиля\n",
    "\n",
    "- `Repaired` — была машина в ремонте или нет\n",
    "\n",
    "- `DateCreated` — дата создания анкеты\n",
    "\n",
    "- `NumberOfPictures` — количество фотографий автомобиля\n",
    "\n",
    "- `PostalCode` — почтовый индекс владельца анкеты (пользователя)\n",
    "\n",
    "- `LastSeen` — дата последней активности пользователя\n",
    "\n",
    "Целевой признак\n",
    "\n",
    "- `Price` — цена (евро)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6353125",
   "metadata": {},
   "source": [
    "# Краткий план работы:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a91202a",
   "metadata": {},
   "source": [
    "- Загрузим данные и проведем предобработку.\n",
    "\n",
    "- Выполним сравнение моделей с использованием различных наборов гиперпараметров.\n",
    "\n",
    "- Выберем лучшую модель по результатам метрики `RMSE` и времени обучения.\n",
    "\n",
    "- Подготовим выводы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fe0f64",
   "metadata": {},
   "source": [
    "## Изучение данных и подготовка к работе"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4886ae6",
   "metadata": {},
   "source": [
    "Настроим окружение: импортируем все необходимые библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96e5626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e4cb0f",
   "metadata": {},
   "source": [
    "### Методы и константы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cb30dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89eaaf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# метод покажет количество и выведет уникальные значения столбцов датасета\n",
    "def check_columns(df):    \n",
    "    for column in df.columns:        \n",
    "        print('Количество уникальных значений столбца', column, '-', df[column].nunique())        \n",
    "        print(df[column].sort_values().unique())         \n",
    "        print('____________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc0d8fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#боксплот для признаков датасета\n",
    "def boxplot_df(df, title):    \n",
    "    names = df.columns    \n",
    "    ncols = len(names)      \n",
    "    fig, axes = plt.subplots(1,ncols, figsize=(60, 15))    \n",
    "    for name, ax in zip( names, axes.flatten()):        \n",
    "        sns.boxplot(y=name, data=df, orient='v', ax=ax)        \n",
    "        sns.set (font_scale= 3)         \n",
    "        plt.suptitle(title)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ac9264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# метод найдет и посчитает выбросы\n",
    "def find_outliers(df,column):\n",
    "    \n",
    "    # определим 25 и 75% квантили\n",
    "    q25=np.array(df[column].quantile(0.25))    \n",
    "    q75=np.array(df[column].quantile(0.75))\n",
    "    \n",
    "    # границы нормального размаха\n",
    "    limit_of_normal_range_left=q25-1.5*(q75-q25)    \n",
    "    limit_of_normal_range_right=q75+1.5*(q75-q25)\n",
    "    \n",
    "    # определим индексы объектов - выбросов\n",
    "    del_index = []    \n",
    "    for index_value, value in zip(df[column].index,df[column]):        \n",
    "        if limit_of_normal_range_right <= value or value <= limit_of_normal_range_left:            \n",
    "            del_index.append(index_value)\n",
    "    \n",
    "    print('Количество строк, выбранных для удаления в столбце', column, ': ', len(del_index))    \n",
    "    print('Нижняя граница нормального размаха', column, ': ', limit_of_normal_range_left)    \n",
    "    print('Верхняя граница нормального размаха', column, ': ', limit_of_normal_range_right)    \n",
    "    print('Процент строк, выбранных для удаления в столбце {:.2f}'.format(len(del_index)*100/df.shape[0]))    \n",
    "    print('___________________________________________________________')    \n",
    "    return del_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61f8eb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, cat_features):\n",
    "        self.cat_features = ['VehicleType', 'Gearbox', 'Model', 'FuelType', 'Brand', 'Repaired']\n",
    "        self.best_params = {}\n",
    "        self.best_model = None\n",
    "    \n",
    "    # Метод для разделения данных на признаки и целевую переменную\n",
    "    def select_target(self, data):\n",
    "        X = data.drop('Price', axis=1)\n",
    "        y = data['Price']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed)\n",
    "        X_y_set = [X_train, y_train, X_test, y_test]\n",
    "        return X_y_set\n",
    "    \n",
    "    # Метод для масштабирования признаков (использует StandardScaler)\n",
    "    def scaled_features(self, X_y_set_train_test):\n",
    "        # признаки для масштабирования\n",
    "        numeric = ['RegistrationYear', 'Power', 'Kilometer'] \n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_y_set_train_test[0][numeric])  \n",
    "        \n",
    "        # Масштабирование набора данных ( обучающая и тестовая выборки)\n",
    "        X_train_scaled = scaler.transform(X_y_set_train_test[0][numeric])     \n",
    "        X_test_scaled = scaler.transform(X_y_set_train_test[2][numeric])\n",
    "    \n",
    "        # Замена оригинальных значений на скалированные\n",
    "        X_y_set_train_test[0][numeric] = X_train_scaled    \n",
    "        X_y_set_train_test[2][numeric] = X_test_scaled\n",
    "    \n",
    "      \n",
    "        # создаем наборы для обучения с масштабированными данными \n",
    "        X_y_scaled = [X_y_set_train_test[0], X_y_set_train_test[1], X_y_set_train_test[2], X_y_set_train_test[3]]            \n",
    "        return X_y_scaled\n",
    "    \n",
    "    \n",
    "    # метод для кодирования категориальных признаков (для обучающей и тестовой выборки)\n",
    "    def encode_method_ohe(self, X_y_set_train_test):        \n",
    "        # Создание объекта OneHotEncoder для категориальных признаков\n",
    "        ohe = ce.OneHotEncoder(cols = self.cat_features,  handle_unknown='unknown', use_cat_names=True)    \n",
    "        ohe.fit(X_y_set_train_test[0])\n",
    "        ohe_train = ohe.transform(X_y_set_train_test[0])\n",
    "        ohe_test = ohe.transform(X_y_set_train_test[2])\n",
    "        \n",
    "        # Удаление по одному столбцу из каждого дамми-признака\n",
    "        for col in cat_features:\n",
    "            ohe_train = ohe_train.drop(columns=ohe_train.filter(like=f\"{col}_\").columns[0])\n",
    "            ohe_test = ohe_test.drop(columns=ohe_test.filter(like=f\"{col}_\").columns[0])\n",
    "    \n",
    "        # Создаем наборы для обучения с закодированными категориальными признаками\n",
    "        X_y_coder = [ohe_train, X_y_set_train_test[1], ohe_test, X_y_set_train_test[3]]    \n",
    "        return X_y_coder\n",
    "\n",
    "    # метод для кодирования категориальных признаков (для обучающей и тестовой выборки)\n",
    "    def encode_method_ordinal(self, X_y_set_train_test):\n",
    "        \n",
    "        # Создание объекта OneHotEncoder для категориальных признаков\n",
    "        ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "        ordinal_encoder.fit(X_y_set_train_test[0].loc[:, self.cat_features])\n",
    "        X_y_set_train_test[0][self.cat_features] = ordinal_encoder.transform(X_y_set_train_test[0][self.cat_features]) \n",
    "        X_y_set_train_test[2][self.cat_features] = ordinal_encoder.transform(X_y_set_train_test[2][self.cat_features])                \n",
    "        \n",
    "        # создаем наборы для обучения с закодированными категориальными признаками\n",
    "        X_y_coder = [X_y_set_train_test[0], X_y_set_train_test[1], X_y_set_train_test[2], X_y_set_train_test[3]]        \n",
    "        return X_y_coder       \n",
    "    \n",
    "    # метод обучения алгоритма\n",
    "    def train_and_evaluate_model(self, algoritm, X_y_train, param_grid, dataset_name):\n",
    "        grid_search = GridSearchCV(algoritm, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "                \n",
    "        if algoritm == lgbm_regressor:            \n",
    "            grid_search.fit(X_y_train[0], X_y_train[1], categorical_feature = self.cat_features)            \n",
    "        else:            \n",
    "            grid_search.fit(X_y_train[0], X_y_train[1])            \n",
    "        \n",
    "        start_time_fit = time.time()\n",
    "        best_model = grid_search.best_estimator_        \n",
    "        # Обучение модели с лучшими параметрами\n",
    "        if algoritm == lgbm_regressor:            \n",
    "            best_model.fit(X_y_train[0], X_y_train[1], categorical_feature = self.cat_features)            \n",
    "        else:            \n",
    "            best_model.fit(X_y_train[0], X_y_train[1])        \n",
    "        end_time_fit = time.time()        \n",
    "        time_fit = end_time_fit - start_time_fit\n",
    "\n",
    "        \n",
    "        start_time_prediction = time.time()\n",
    "        # Предсказание с моделью с лучшими параметрами\n",
    "        y_pred = best_model.predict(X_y_train[0])\n",
    "        end_time_prediction = time.time()\n",
    "        time_prediction = end_time_prediction - start_time_prediction       \n",
    "\n",
    "        # Получение лучших параметров и оценки RMSE для обучающих данных\n",
    "        best_params = grid_search.best_params_        \n",
    "        self.best_params = best_params        \n",
    "        print(f'Параметры лучшей модели алгоритма {algoritm}', self.best_params)        \n",
    "        best_rmse_fit = np.sqrt(-grid_search.best_score_)        \n",
    "        # Округление значения RMSE_fit для удобного отображения\n",
    "        best_rmse_fit_rounded = round(best_rmse_fit, 2)\n",
    "        # Сохранение лучшей модели в атрибуте best_model\n",
    "        self.best_model = grid_search.best_estimator_\n",
    "\n",
    "        row_tab_metrics = [algoritm, dataset_name, best_rmse_fit_rounded, time_fit, time_prediction]        \n",
    "        # Возвращение результатов в виде словаря\n",
    "        return row_tab_metrics, self.best_model\n",
    "    \n",
    "    \n",
    "    # метод переберет наборы данных, передаст в обучающий метод параметры алгоритма\n",
    "    def evaluate_models(self, datasets, algoritm, model_params):    \n",
    "        for dataset_name, dataset in datasets.items():\n",
    "            # Вызов метода train_and_evaluate_model для обучения и оценки\n",
    "            metrics_tab, best_model = \\\n",
    "            self.train_and_evaluate_model(algoritm, dataset, model_params, dataset_name)      \n",
    "            # Добавление метрики в таблицу tab_metrics\n",
    "            tab_metrics.loc[len(tab_metrics.index)] = metrics_tab      \n",
    "            # Добавление лучших моделей в список\n",
    "            best_model_fit.append(best_model)         \n",
    "        return tab_metrics, best_model_fit\n",
    "    \n",
    "    # Метод для проверки модели на адекватность\n",
    "    def predict_dummy_model(self, X_y_train, best_model):        \n",
    "        # Создание константной модели\n",
    "        dummy_model = DummyRegressor(strategy='median')        \n",
    "        dummy_model.fit(X_y_train[0], X_y_train[1])\n",
    "        y_pred_best = best_model.predict(X_y_train[2])\n",
    "        y_pred_dummy = dummy_model.predict(X_y_train[2])\n",
    "        rmse_best = np.sqrt(mean_squared_error(X_y_train[3], y_pred_best))        \n",
    "        rmse_dummy = np.sqrt(mean_squared_error(X_y_train[3], y_pred_dummy))\n",
    "        \n",
    "        # Вывод результатов\n",
    "        print(f'Лучшая модель RMSE: {rmse_best:.2f}')        \n",
    "        print(f'Константная модель RMSE: {rmse_dummy:.2f}')\n",
    "        \n",
    "        \n",
    "    # метод проверит результаты работы лучшего алгоритма на тестовой выборке                    \n",
    "    def test_best_model(self, X_y_test, best_model):\n",
    "        start_time_prediction = time.time()\n",
    "        y_pred = best_model.predict(X_y_test[2])        \n",
    "        test_rmse = np.sqrt(mean_squared_error(X_y_test[3], y_pred))\n",
    "        end_time_prediction = time.time()\n",
    "        \n",
    "        print('Test RMSE', test_rmse)\n",
    "        print('Prediction Time', end_time_prediction - start_time_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5017e5",
   "metadata": {},
   "source": [
    "<a id = 'df'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b9515d",
   "metadata": {},
   "source": [
    "### Общая информация о датасете и данных. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bb80a1",
   "metadata": {},
   "source": [
    "Ознакомимся с данными таблицы и их структурой.\n",
    "\n",
    "Загрузим в датафрейм данные, с которыми будем работать.\n",
    "\n",
    "Посмотрим на несколько первых записей в датафрейме c помощью метода `head()`.\n",
    "\n",
    "Выведем общую информацию по таблице c помощью метода `info()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1b4ee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pth1 = '/datasets/autos.csv'\n",
    "pth2 = 'autos.csv'\n",
    "if os.path.exists(pth1):    \n",
    "    df = pd.read_csv(pth1)    \n",
    "elif os.path.exists(pth2):    \n",
    "    df = pd.read_csv(pth2)    \n",
    "else:    \n",
    "    print('Something is wrong') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f08498f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>354359</th>\n",
       "      <th>354360</th>\n",
       "      <th>354361</th>\n",
       "      <th>354362</th>\n",
       "      <th>354363</th>\n",
       "      <th>354364</th>\n",
       "      <th>354365</th>\n",
       "      <th>354366</th>\n",
       "      <th>354367</th>\n",
       "      <th>354368</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DateCrawled</th>\n",
       "      <td>2016-03-24 11:52:17</td>\n",
       "      <td>2016-03-24 10:58:45</td>\n",
       "      <td>2016-03-14 12:52:21</td>\n",
       "      <td>2016-03-17 16:54:04</td>\n",
       "      <td>2016-03-31 17:25:20</td>\n",
       "      <td>2016-04-04 17:36:23</td>\n",
       "      <td>2016-04-01 20:48:51</td>\n",
       "      <td>2016-03-21 18:54:38</td>\n",
       "      <td>2016-04-04 23:42:13</td>\n",
       "      <td>2016-03-17 10:53:50</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-03-28 13:48:07</td>\n",
       "      <td>2016-04-02 20:37:03</td>\n",
       "      <td>2016-03-09 13:37:43</td>\n",
       "      <td>2016-03-19 19:53:49</td>\n",
       "      <td>2016-03-27 20:36:20</td>\n",
       "      <td>2016-03-21 09:50:58</td>\n",
       "      <td>2016-03-14 17:48:27</td>\n",
       "      <td>2016-03-05 19:56:21</td>\n",
       "      <td>2016-03-19 18:57:12</td>\n",
       "      <td>2016-03-20 19:41:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <td>480</td>\n",
       "      <td>18300</td>\n",
       "      <td>9800</td>\n",
       "      <td>1500</td>\n",
       "      <td>3600</td>\n",
       "      <td>650</td>\n",
       "      <td>2200</td>\n",
       "      <td>0</td>\n",
       "      <td>14500</td>\n",
       "      <td>999</td>\n",
       "      <td>...</td>\n",
       "      <td>7900</td>\n",
       "      <td>3999</td>\n",
       "      <td>5250</td>\n",
       "      <td>3200</td>\n",
       "      <td>1150</td>\n",
       "      <td>0</td>\n",
       "      <td>2200</td>\n",
       "      <td>1199</td>\n",
       "      <td>9200</td>\n",
       "      <td>3400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VehicleType</th>\n",
       "      <td>NaN</td>\n",
       "      <td>coupe</td>\n",
       "      <td>suv</td>\n",
       "      <td>small</td>\n",
       "      <td>small</td>\n",
       "      <td>sedan</td>\n",
       "      <td>convertible</td>\n",
       "      <td>sedan</td>\n",
       "      <td>bus</td>\n",
       "      <td>small</td>\n",
       "      <td>...</td>\n",
       "      <td>sedan</td>\n",
       "      <td>wagon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sedan</td>\n",
       "      <td>bus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>convertible</td>\n",
       "      <td>bus</td>\n",
       "      <td>wagon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RegistrationYear</th>\n",
       "      <td>1993</td>\n",
       "      <td>2011</td>\n",
       "      <td>2004</td>\n",
       "      <td>2001</td>\n",
       "      <td>2008</td>\n",
       "      <td>1995</td>\n",
       "      <td>2004</td>\n",
       "      <td>1980</td>\n",
       "      <td>2014</td>\n",
       "      <td>1998</td>\n",
       "      <td>...</td>\n",
       "      <td>2010</td>\n",
       "      <td>2005</td>\n",
       "      <td>2016</td>\n",
       "      <td>2004</td>\n",
       "      <td>2000</td>\n",
       "      <td>2005</td>\n",
       "      <td>2005</td>\n",
       "      <td>2000</td>\n",
       "      <td>1996</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gearbox</th>\n",
       "      <td>manual</td>\n",
       "      <td>manual</td>\n",
       "      <td>auto</td>\n",
       "      <td>manual</td>\n",
       "      <td>manual</td>\n",
       "      <td>manual</td>\n",
       "      <td>manual</td>\n",
       "      <td>manual</td>\n",
       "      <td>manual</td>\n",
       "      <td>manual</td>\n",
       "      <td>...</td>\n",
       "      <td>manual</td>\n",
       "      <td>manual</td>\n",
       "      <td>auto</td>\n",
       "      <td>manual</td>\n",
       "      <td>manual</td>\n",
       "      <td>manual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>auto</td>\n",
       "      <td>manual</td>\n",
       "      <td>manual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power</th>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>163</td>\n",
       "      <td>75</td>\n",
       "      <td>69</td>\n",
       "      <td>102</td>\n",
       "      <td>109</td>\n",
       "      <td>50</td>\n",
       "      <td>125</td>\n",
       "      <td>101</td>\n",
       "      <td>...</td>\n",
       "      <td>140</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>225</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>102</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <td>golf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>grand</td>\n",
       "      <td>golf</td>\n",
       "      <td>fabia</td>\n",
       "      <td>3er</td>\n",
       "      <td>2_reihe</td>\n",
       "      <td>other</td>\n",
       "      <td>c_max</td>\n",
       "      <td>golf</td>\n",
       "      <td>...</td>\n",
       "      <td>golf</td>\n",
       "      <td>3er</td>\n",
       "      <td>159</td>\n",
       "      <td>leon</td>\n",
       "      <td>zafira</td>\n",
       "      <td>colt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fortwo</td>\n",
       "      <td>transporter</td>\n",
       "      <td>golf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kilometer</th>\n",
       "      <td>150000</td>\n",
       "      <td>125000</td>\n",
       "      <td>125000</td>\n",
       "      <td>150000</td>\n",
       "      <td>90000</td>\n",
       "      <td>150000</td>\n",
       "      <td>150000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30000</td>\n",
       "      <td>150000</td>\n",
       "      <td>...</td>\n",
       "      <td>150000</td>\n",
       "      <td>150000</td>\n",
       "      <td>150000</td>\n",
       "      <td>150000</td>\n",
       "      <td>150000</td>\n",
       "      <td>150000</td>\n",
       "      <td>20000</td>\n",
       "      <td>125000</td>\n",
       "      <td>150000</td>\n",
       "      <td>150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RegistrationMonth</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FuelType</th>\n",
       "      <td>petrol</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>petrol</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>petrol</td>\n",
       "      <td>petrol</td>\n",
       "      <td>petrol</td>\n",
       "      <td>petrol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>petrol</td>\n",
       "      <td>petrol</td>\n",
       "      <td>petrol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>petrol</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>gasoline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brand</th>\n",
       "      <td>volkswagen</td>\n",
       "      <td>audi</td>\n",
       "      <td>jeep</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>skoda</td>\n",
       "      <td>bmw</td>\n",
       "      <td>peugeot</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>ford</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>...</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>bmw</td>\n",
       "      <td>alfa_romeo</td>\n",
       "      <td>seat</td>\n",
       "      <td>opel</td>\n",
       "      <td>mitsubishi</td>\n",
       "      <td>sonstige_autos</td>\n",
       "      <td>smart</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>volkswagen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Repaired</th>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateCreated</th>\n",
       "      <td>2016-03-24 00:00:00</td>\n",
       "      <td>2016-03-24 00:00:00</td>\n",
       "      <td>2016-03-14 00:00:00</td>\n",
       "      <td>2016-03-17 00:00:00</td>\n",
       "      <td>2016-03-31 00:00:00</td>\n",
       "      <td>2016-04-04 00:00:00</td>\n",
       "      <td>2016-04-01 00:00:00</td>\n",
       "      <td>2016-03-21 00:00:00</td>\n",
       "      <td>2016-04-04 00:00:00</td>\n",
       "      <td>2016-03-17 00:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-03-28 00:00:00</td>\n",
       "      <td>2016-04-02 00:00:00</td>\n",
       "      <td>2016-03-09 00:00:00</td>\n",
       "      <td>2016-03-19 00:00:00</td>\n",
       "      <td>2016-03-27 00:00:00</td>\n",
       "      <td>2016-03-21 00:00:00</td>\n",
       "      <td>2016-03-14 00:00:00</td>\n",
       "      <td>2016-03-05 00:00:00</td>\n",
       "      <td>2016-03-19 00:00:00</td>\n",
       "      <td>2016-03-20 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumberOfPictures</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PostalCode</th>\n",
       "      <td>70435</td>\n",
       "      <td>66954</td>\n",
       "      <td>90480</td>\n",
       "      <td>91074</td>\n",
       "      <td>60437</td>\n",
       "      <td>33775</td>\n",
       "      <td>67112</td>\n",
       "      <td>19348</td>\n",
       "      <td>94505</td>\n",
       "      <td>27472</td>\n",
       "      <td>...</td>\n",
       "      <td>75223</td>\n",
       "      <td>81825</td>\n",
       "      <td>51371</td>\n",
       "      <td>96465</td>\n",
       "      <td>26624</td>\n",
       "      <td>2694</td>\n",
       "      <td>39576</td>\n",
       "      <td>26135</td>\n",
       "      <td>87439</td>\n",
       "      <td>40764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LastSeen</th>\n",
       "      <td>2016-04-07 03:16:57</td>\n",
       "      <td>2016-04-07 01:46:50</td>\n",
       "      <td>2016-04-05 12:47:46</td>\n",
       "      <td>2016-03-17 17:40:17</td>\n",
       "      <td>2016-04-06 10:17:21</td>\n",
       "      <td>2016-04-06 19:17:07</td>\n",
       "      <td>2016-04-05 18:18:39</td>\n",
       "      <td>2016-03-25 16:47:58</td>\n",
       "      <td>2016-04-04 23:42:13</td>\n",
       "      <td>2016-03-31 17:17:06</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-04-02 18:16:20</td>\n",
       "      <td>2016-04-06 20:47:12</td>\n",
       "      <td>2016-03-13 01:44:13</td>\n",
       "      <td>2016-03-19 20:44:43</td>\n",
       "      <td>2016-03-29 10:17:23</td>\n",
       "      <td>2016-03-21 10:42:49</td>\n",
       "      <td>2016-04-06 00:46:52</td>\n",
       "      <td>2016-03-11 18:17:12</td>\n",
       "      <td>2016-04-07 07:15:26</td>\n",
       "      <td>2016-03-24 12:45:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows × 354369 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                0                    1       \\\n",
       "DateCrawled        2016-03-24 11:52:17  2016-03-24 10:58:45   \n",
       "Price                              480                18300   \n",
       "VehicleType                        NaN                coupe   \n",
       "RegistrationYear                  1993                 2011   \n",
       "Gearbox                         manual               manual   \n",
       "Power                                0                  190   \n",
       "Model                             golf                  NaN   \n",
       "Kilometer                       150000               125000   \n",
       "RegistrationMonth                    0                    5   \n",
       "FuelType                        petrol             gasoline   \n",
       "Brand                       volkswagen                 audi   \n",
       "Repaired                           NaN                  yes   \n",
       "DateCreated        2016-03-24 00:00:00  2016-03-24 00:00:00   \n",
       "NumberOfPictures                     0                    0   \n",
       "PostalCode                       70435                66954   \n",
       "LastSeen           2016-04-07 03:16:57  2016-04-07 01:46:50   \n",
       "\n",
       "                                2                    3       \\\n",
       "DateCrawled        2016-03-14 12:52:21  2016-03-17 16:54:04   \n",
       "Price                             9800                 1500   \n",
       "VehicleType                        suv                small   \n",
       "RegistrationYear                  2004                 2001   \n",
       "Gearbox                           auto               manual   \n",
       "Power                              163                   75   \n",
       "Model                            grand                 golf   \n",
       "Kilometer                       125000               150000   \n",
       "RegistrationMonth                    8                    6   \n",
       "FuelType                      gasoline               petrol   \n",
       "Brand                             jeep           volkswagen   \n",
       "Repaired                           NaN                   no   \n",
       "DateCreated        2016-03-14 00:00:00  2016-03-17 00:00:00   \n",
       "NumberOfPictures                     0                    0   \n",
       "PostalCode                       90480                91074   \n",
       "LastSeen           2016-04-05 12:47:46  2016-03-17 17:40:17   \n",
       "\n",
       "                                4                    5       \\\n",
       "DateCrawled        2016-03-31 17:25:20  2016-04-04 17:36:23   \n",
       "Price                             3600                  650   \n",
       "VehicleType                      small                sedan   \n",
       "RegistrationYear                  2008                 1995   \n",
       "Gearbox                         manual               manual   \n",
       "Power                               69                  102   \n",
       "Model                            fabia                  3er   \n",
       "Kilometer                        90000               150000   \n",
       "RegistrationMonth                    7                   10   \n",
       "FuelType                      gasoline               petrol   \n",
       "Brand                            skoda                  bmw   \n",
       "Repaired                            no                  yes   \n",
       "DateCreated        2016-03-31 00:00:00  2016-04-04 00:00:00   \n",
       "NumberOfPictures                     0                    0   \n",
       "PostalCode                       60437                33775   \n",
       "LastSeen           2016-04-06 10:17:21  2016-04-06 19:17:07   \n",
       "\n",
       "                                6                    7       \\\n",
       "DateCrawled        2016-04-01 20:48:51  2016-03-21 18:54:38   \n",
       "Price                             2200                    0   \n",
       "VehicleType                convertible                sedan   \n",
       "RegistrationYear                  2004                 1980   \n",
       "Gearbox                         manual               manual   \n",
       "Power                              109                   50   \n",
       "Model                          2_reihe                other   \n",
       "Kilometer                       150000                40000   \n",
       "RegistrationMonth                    8                    7   \n",
       "FuelType                        petrol               petrol   \n",
       "Brand                          peugeot           volkswagen   \n",
       "Repaired                            no                   no   \n",
       "DateCreated        2016-04-01 00:00:00  2016-03-21 00:00:00   \n",
       "NumberOfPictures                     0                    0   \n",
       "PostalCode                       67112                19348   \n",
       "LastSeen           2016-04-05 18:18:39  2016-03-25 16:47:58   \n",
       "\n",
       "                                8                    9       ...  \\\n",
       "DateCrawled        2016-04-04 23:42:13  2016-03-17 10:53:50  ...   \n",
       "Price                            14500                  999  ...   \n",
       "VehicleType                        bus                small  ...   \n",
       "RegistrationYear                  2014                 1998  ...   \n",
       "Gearbox                         manual               manual  ...   \n",
       "Power                              125                  101  ...   \n",
       "Model                            c_max                 golf  ...   \n",
       "Kilometer                        30000               150000  ...   \n",
       "RegistrationMonth                    8                    0  ...   \n",
       "FuelType                        petrol                  NaN  ...   \n",
       "Brand                             ford           volkswagen  ...   \n",
       "Repaired                           NaN                  NaN  ...   \n",
       "DateCreated        2016-04-04 00:00:00  2016-03-17 00:00:00  ...   \n",
       "NumberOfPictures                     0                    0  ...   \n",
       "PostalCode                       94505                27472  ...   \n",
       "LastSeen           2016-04-04 23:42:13  2016-03-31 17:17:06  ...   \n",
       "\n",
       "                                354359               354360  \\\n",
       "DateCrawled        2016-03-28 13:48:07  2016-04-02 20:37:03   \n",
       "Price                             7900                 3999   \n",
       "VehicleType                      sedan                wagon   \n",
       "RegistrationYear                  2010                 2005   \n",
       "Gearbox                         manual               manual   \n",
       "Power                              140                    3   \n",
       "Model                             golf                  3er   \n",
       "Kilometer                       150000               150000   \n",
       "RegistrationMonth                    7                    5   \n",
       "FuelType                      gasoline             gasoline   \n",
       "Brand                       volkswagen                  bmw   \n",
       "Repaired                            no                   no   \n",
       "DateCreated        2016-03-28 00:00:00  2016-04-02 00:00:00   \n",
       "NumberOfPictures                     0                    0   \n",
       "PostalCode                       75223                81825   \n",
       "LastSeen           2016-04-02 18:16:20  2016-04-06 20:47:12   \n",
       "\n",
       "                                354361               354362  \\\n",
       "DateCrawled        2016-03-09 13:37:43  2016-03-19 19:53:49   \n",
       "Price                             5250                 3200   \n",
       "VehicleType                        NaN                sedan   \n",
       "RegistrationYear                  2016                 2004   \n",
       "Gearbox                           auto               manual   \n",
       "Power                              150                  225   \n",
       "Model                              159                 leon   \n",
       "Kilometer                       150000               150000   \n",
       "RegistrationMonth                   12                    5   \n",
       "FuelType                           NaN               petrol   \n",
       "Brand                       alfa_romeo                 seat   \n",
       "Repaired                            no                  yes   \n",
       "DateCreated        2016-03-09 00:00:00  2016-03-19 00:00:00   \n",
       "NumberOfPictures                     0                    0   \n",
       "PostalCode                       51371                96465   \n",
       "LastSeen           2016-03-13 01:44:13  2016-03-19 20:44:43   \n",
       "\n",
       "                                354363               354364  \\\n",
       "DateCrawled        2016-03-27 20:36:20  2016-03-21 09:50:58   \n",
       "Price                             1150                    0   \n",
       "VehicleType                        bus                  NaN   \n",
       "RegistrationYear                  2000                 2005   \n",
       "Gearbox                         manual               manual   \n",
       "Power                                0                    0   \n",
       "Model                           zafira                 colt   \n",
       "Kilometer                       150000               150000   \n",
       "RegistrationMonth                    3                    7   \n",
       "FuelType                        petrol               petrol   \n",
       "Brand                             opel           mitsubishi   \n",
       "Repaired                            no                  yes   \n",
       "DateCreated        2016-03-27 00:00:00  2016-03-21 00:00:00   \n",
       "NumberOfPictures                     0                    0   \n",
       "PostalCode                       26624                 2694   \n",
       "LastSeen           2016-03-29 10:17:23  2016-03-21 10:42:49   \n",
       "\n",
       "                                354365               354366  \\\n",
       "DateCrawled        2016-03-14 17:48:27  2016-03-05 19:56:21   \n",
       "Price                             2200                 1199   \n",
       "VehicleType                        NaN          convertible   \n",
       "RegistrationYear                  2005                 2000   \n",
       "Gearbox                            NaN                 auto   \n",
       "Power                                0                  101   \n",
       "Model                              NaN               fortwo   \n",
       "Kilometer                        20000               125000   \n",
       "RegistrationMonth                    1                    3   \n",
       "FuelType                           NaN               petrol   \n",
       "Brand                   sonstige_autos                smart   \n",
       "Repaired                           NaN                   no   \n",
       "DateCreated        2016-03-14 00:00:00  2016-03-05 00:00:00   \n",
       "NumberOfPictures                     0                    0   \n",
       "PostalCode                       39576                26135   \n",
       "LastSeen           2016-04-06 00:46:52  2016-03-11 18:17:12   \n",
       "\n",
       "                                354367               354368  \n",
       "DateCrawled        2016-03-19 18:57:12  2016-03-20 19:41:08  \n",
       "Price                             9200                 3400  \n",
       "VehicleType                        bus                wagon  \n",
       "RegistrationYear                  1996                 2002  \n",
       "Gearbox                         manual               manual  \n",
       "Power                              102                  100  \n",
       "Model                      transporter                 golf  \n",
       "Kilometer                       150000               150000  \n",
       "RegistrationMonth                    3                    6  \n",
       "FuelType                      gasoline             gasoline  \n",
       "Brand                       volkswagen           volkswagen  \n",
       "Repaired                            no                  NaN  \n",
       "DateCreated        2016-03-19 00:00:00  2016-03-20 00:00:00  \n",
       "NumberOfPictures                     0                    0  \n",
       "PostalCode                       87439                40764  \n",
       "LastSeen           2016-04-07 07:15:26  2016-03-24 12:45:21  \n",
       "\n",
       "[16 rows x 354369 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.T.head(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb474596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 354369 entries, 0 to 354368\n",
      "Data columns (total 16 columns):\n",
      " #   Column             Non-Null Count   Dtype \n",
      "---  ------             --------------   ----- \n",
      " 0   DateCrawled        354369 non-null  object\n",
      " 1   Price              354369 non-null  int64 \n",
      " 2   VehicleType        316879 non-null  object\n",
      " 3   RegistrationYear   354369 non-null  int64 \n",
      " 4   Gearbox            334536 non-null  object\n",
      " 5   Power              354369 non-null  int64 \n",
      " 6   Model              334664 non-null  object\n",
      " 7   Kilometer          354369 non-null  int64 \n",
      " 8   RegistrationMonth  354369 non-null  int64 \n",
      " 9   FuelType           321474 non-null  object\n",
      " 10  Brand              354369 non-null  object\n",
      " 11  Repaired           283215 non-null  object\n",
      " 12  DateCreated        354369 non-null  object\n",
      " 13  NumberOfPictures   354369 non-null  int64 \n",
      " 14  PostalCode         354369 non-null  int64 \n",
      " 15  LastSeen           354369 non-null  object\n",
      "dtypes: int64(7), object(9)\n",
      "memory usage: 43.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_old = df.shape[0]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10ae95ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_old"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37068f74",
   "metadata": {},
   "source": [
    "Типы данных в датафрейме:\n",
    "\n",
    "Категориальные: `VehicleType`, `Gearbox`, `Model`, `FuelType`, `Brand`, `Repaired`, `PostalCode`.\n",
    "\n",
    "Количественные: `Power`, `Kilometer`, `NumberOfPictures`, `Price`, тип `int`.\n",
    "\n",
    "Логические: логических нет.\n",
    "\n",
    "Дата: `DateCrawled`, `DateCreated`, `LastSeen`, `RegistrationYear`, `RegistrationMonth`.\n",
    "\n",
    "В датафрейме 354369 объектов и 16 признаков (характеристик) для них. \n",
    "\n",
    "Целвой признак `Price`.\n",
    "\n",
    "В 5 столбцах есть пропущенные данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c46f0340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество строк - объектов с пропусками 181077\n",
      "Процент пропусков по всему датафрейму 51.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>процент пропусков</th>\n",
       "      <th>количество пропусков</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RegistrationYear</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kilometer</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RegistrationMonth</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brand</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateCreated</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumberOfPictures</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PostalCode</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LastSeen</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gearbox</th>\n",
       "      <td>6.0</td>\n",
       "      <td>19833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <td>6.0</td>\n",
       "      <td>19705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FuelType</th>\n",
       "      <td>9.0</td>\n",
       "      <td>32895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VehicleType</th>\n",
       "      <td>11.0</td>\n",
       "      <td>37490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Repaired</th>\n",
       "      <td>20.0</td>\n",
       "      <td>71154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   процент пропусков  количество пропусков\n",
       "Price                            0.0                     0\n",
       "RegistrationYear                 0.0                     0\n",
       "Power                            0.0                     0\n",
       "Kilometer                        0.0                     0\n",
       "RegistrationMonth                0.0                     0\n",
       "Brand                            0.0                     0\n",
       "DateCreated                      0.0                     0\n",
       "NumberOfPictures                 0.0                     0\n",
       "PostalCode                       0.0                     0\n",
       "LastSeen                         0.0                     0\n",
       "Gearbox                          6.0                 19833\n",
       "Model                            6.0                 19705\n",
       "FuelType                         9.0                 32895\n",
       "VehicleType                     11.0                 37490\n",
       "Repaired                        20.0                 71154"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Количество строк - объектов с пропусками', df.isnull().sum().sum())    \n",
    "print('Процент пропусков по всему датафрейму', (df.isnull().sum().sum ()/len(df)*100).round(2))  \n",
    "\n",
    "pd.DataFrame({'процент пропусков':round((df.isna().mean()*100),), \n",
    "              'количество пропусков': df.isnull().sum()}).sort_values(by = 'процент пропусков').tail(15) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71893c1",
   "metadata": {},
   "source": [
    "Самое большой процент пропущенных значений имеем в столбце `Repaired` - 20%, в столбце `VehicleType` - 11%, `FuelType` - 9%, а столбцах `Gearbox` и `Model` - 6%. Позже рассмотрим возможность замены пустых значений, если адекватных вариантов не будет, удалим объекты с пустыми значениями."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bad0d6",
   "metadata": {},
   "source": [
    "Можем предположить, что столбцы `DateCrawled`, `DateCreated`, `LastSeen`, `RegistrationMonth`, `NumberOfPictures`, `PostalCode` (дата скачивания анкеты из базы, дата создания анкеты, дата последней активности пользователя, месяц регистрации автомобиля, количество фотографий автомобиля, почтовый индекс владельца анкеты (пользователя)) являются  бесполезными для предсказания. \n",
    "\n",
    "Удалим эти столбцы. Сделаем это позже, после поиска явных и неявных дубликатов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2def67be",
   "metadata": {},
   "source": [
    "Выведем уникальные значения по столбцам и статистические показатели, вызовем метод `check_columns()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f71cd981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных значений столбца DateCrawled - 271174\n",
      "['2016-03-05 14:06:22' '2016-03-05 14:06:23' '2016-03-05 14:06:24' ...\n",
      " '2016-04-07 14:36:55' '2016-04-07 14:36:56' '2016-04-07 14:36:58']\n",
      "____________________________________\n",
      "Количество уникальных значений столбца Price - 3731\n",
      "[    0     1     2 ... 19998 19999 20000]\n",
      "____________________________________\n",
      "Количество уникальных значений столбца VehicleType - 8\n",
      "['bus' 'convertible' 'coupe' 'other' 'sedan' 'small' 'suv' 'wagon' nan]\n",
      "____________________________________\n",
      "Количество уникальных значений столбца RegistrationYear - 151\n",
      "[1000 1001 1039 1111 1200 1234 1253 1255 1300 1400 1500 1600 1602 1688\n",
      " 1800 1910 1915 1919 1920 1923 1925 1927 1928 1929 1930 1931 1932 1933\n",
      " 1934 1935 1936 1937 1938 1940 1941 1942 1943 1944 1945 1946 1947 1948\n",
      " 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962\n",
      " 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976\n",
      " 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990\n",
      " 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004\n",
      " 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018\n",
      " 2019 2066 2200 2222 2290 2500 2800 2900 3000 3200 3500 3700 3800 4000\n",
      " 4100 4500 4800 5000 5300 5555 5600 5900 5911 6000 6500 7000 7100 7500\n",
      " 7800 8000 8200 8455 8500 8888 9000 9229 9450 9996 9999]\n",
      "____________________________________\n",
      "Количество уникальных значений столбца Gearbox - 2\n",
      "['auto' 'manual' nan]\n",
      "____________________________________\n",
      "Количество уникальных значений столбца Power - 712\n",
      "[    0     1     2     3     4     5     6     7     8     9    10    11\n",
      "    12    13    14    15    16    17    18    19    20    21    22    23\n",
      "    24    25    26    27    28    29    30    31    32    33    34    35\n",
      "    36    37    38    39    40    41    42    43    44    45    46    47\n",
      "    48    49    50    51    52    53    54    55    56    57    58    59\n",
      "    60    61    62    63    64    65    66    67    68    69    70    71\n",
      "    72    73    74    75    76    77    78    79    80    81    82    83\n",
      "    84    85    86    87    88    89    90    91    92    93    94    95\n",
      "    96    97    98    99   100   101   102   103   104   105   106   107\n",
      "   108   109   110   111   112   113   114   115   116   117   118   119\n",
      "   120   121   122   123   124   125   126   127   128   129   130   131\n",
      "   132   133   134   135   136   137   138   139   140   141   142   143\n",
      "   144   145   146   147   148   149   150   151   152   153   154   155\n",
      "   156   157   158   159   160   161   162   163   164   165   166   167\n",
      "   168   169   170   171   172   173   174   175   176   177   178   179\n",
      "   180   181   182   183   184   185   186   187   188   189   190   191\n",
      "   192   193   194   195   196   197   198   199   200   201   202   203\n",
      "   204   205   206   207   208   209   210   211   212   213   214   215\n",
      "   216   217   218   219   220   221   222   223   224   225   226   227\n",
      "   228   229   230   231   232   233   234   235   236   237   238   239\n",
      "   240   241   242   243   244   245   246   247   248   249   250   251\n",
      "   252   253   254   255   256   257   258   259   260   261   262   264\n",
      "   265   266   267   268   269   270   271   272   273   274   275   276\n",
      "   277   278   279   280   281   282   283   284   285   286   287   288\n",
      "   289   290   292   293   294   295   296   297   298   299   300   301\n",
      "   303   304   305   306   307   308   309   310   311   313   314   315\n",
      "   316   317   318   320   321   322   323   324   325   326   327   328\n",
      "   329   330   331   332   333   334   335   336   337   338   339   340\n",
      "   341   343   344   345   346   347   348   349   350   351   352   353\n",
      "   354   355   356   357   358   360   361   362   363   364   365   367\n",
      "   368   370   371   374   375   376   377   379   380   381   382   385\n",
      "   386   387   388   390   392   394   396   398   399   400   401   402\n",
      "   405   408   409   411   416   420   421   425   426   428   430   431\n",
      "   435   440   442   445   449   450   454   457   459   460   475   476\n",
      "   485   487   489   490   500   504   505   507   508   510   514   515\n",
      "   517   519   520   521   525   530   540   541   544   550   551   553\n",
      "   560   572   574   579   580   584   585   599   600   601   602   603\n",
      "   604   606   607   610   612   620   640   645   650   651   671   678\n",
      "   682   685   696   700   702   703   732   743   750   751   754   771\n",
      "   776   800   805   808   850   851   871   900   901   902   903   907\n",
      "   909   923   950   952   953   960   998   999  1000  1001  1002  1003\n",
      "  1004  1005  1011  1012  1016  1017  1021  1024  1054  1055  1056  1062\n",
      "  1079  1082  1090  1100  1102  1103  1105  1111  1115  1120  1149  1151\n",
      "  1158  1160  1162  1164  1199  1200  1202  1221  1223  1230  1239  1240\n",
      "  1241  1250  1252  1256  1275  1288  1299  1300  1312  1317  1324  1339\n",
      "  1351  1360  1362  1363  1367  1390  1394  1398  1399  1400  1401  1403\n",
      "  1405  1416  1432  1433  1436  1500  1501  1502  1503  1506  1521  1548\n",
      "  1595  1596  1597  1598  1600  1625  1631  1653  1659  1689  1700  1701\n",
      "  1703  1704  1707  1753  1771  1779  1780  1781  1783  1793  1796  1799\n",
      "  1800  1801  1870  1895  1896  1900  1910  1920  1922  1933  1937  1968\n",
      "  1986  1988  1992  1993  1995  1998  1999  2000  2004  2005  2007  2009\n",
      "  2016  2017  2018  2172  2200  2201  2331  2340  2389  2402  2461  2598\n",
      "  2729  2789  2792  2799  3000  3199  3454  3500  3750  4400  4507  4700\n",
      "  5000  5411  5420  5575  5809  5815  5867  6006  6010  6011  6012  6018\n",
      "  6045  6062  6226  6512  6920  7508  7511  7512  7515  7518  7529  7544\n",
      "  8011  8259  8404  8500  9000  9007  9010  9011  9012  9013  9710 10000\n",
      " 10110 10218 10311 10317 10520 10522 10710 10910 10912 11011 11025 11111\n",
      " 11509 11530 11635 12012 12510 12512 12684 13616 13636 14009 15001 15016\n",
      " 15017 15020 15033 16011 16051 16311 16312 17011 17019 17410 17700 17932\n",
      " 19208 19211 19312 20000]\n",
      "____________________________________\n",
      "Количество уникальных значений столбца Model - 250\n",
      "['100' '145' '147' '156' '159' '1_reihe' '1er' '200' '2_reihe' '300c'\n",
      " '3_reihe' '3er' '4_reihe' '500' '5_reihe' '5er' '601' '6_reihe' '6er'\n",
      " '7er' '80' '850' '90' '900' '9000' '911' 'a1' 'a2' 'a3' 'a4' 'a5' 'a6'\n",
      " 'a8' 'a_klasse' 'accord' 'agila' 'alhambra' 'almera' 'altea' 'amarok'\n",
      " 'antara' 'arosa' 'astra' 'auris' 'avensis' 'aveo' 'aygo' 'b_klasse'\n",
      " 'b_max' 'beetle' 'berlingo' 'bora' 'boxster' 'bravo' 'c1' 'c2' 'c3' 'c4'\n",
      " 'c5' 'c_klasse' 'c_max' 'c_reihe' 'caddy' 'calibra' 'captiva' 'carisma'\n",
      " 'carnival' 'cayenne' 'cc' 'ceed' 'charade' 'cherokee' 'citigo' 'civic'\n",
      " 'cl' 'clio' 'clk' 'clubman' 'colt' 'combo' 'cooper' 'cordoba' 'corolla'\n",
      " 'corsa' 'cr_reihe' 'croma' 'crossfire' 'cuore' 'cx_reihe' 'defender'\n",
      " 'delta' 'discovery' 'doblo' 'ducato' 'duster' 'e_klasse' 'elefantino'\n",
      " 'eos' 'escort' 'espace' 'exeo' 'fabia' 'fiesta' 'focus' 'forester'\n",
      " 'forfour' 'fortwo' 'fox' 'freelander' 'fusion' 'g_klasse' 'galant'\n",
      " 'galaxy' 'getz' 'gl' 'glk' 'golf' 'grand' 'i3' 'i_reihe' 'ibiza'\n",
      " 'impreza' 'insignia' 'jazz' 'jetta' 'jimny' 'juke' 'justy' 'ka' 'kadett'\n",
      " 'kaefer' 'kalina' 'kalos' 'kangoo' 'kappa' 'kuga' 'laguna' 'lancer'\n",
      " 'lanos' 'legacy' 'leon' 'lodgy' 'logan' 'lupo' 'lybra' 'm_klasse'\n",
      " 'm_reihe' 'materia' 'matiz' 'megane' 'meriva' 'micra' 'mii' 'modus'\n",
      " 'mondeo' 'move' 'musa' 'mustang' 'mx_reihe' 'navara' 'niva' 'note'\n",
      " 'nubira' 'octavia' 'omega' 'one' 'other' 'outlander' 'pajero' 'panda'\n",
      " 'passat' 'phaeton' 'picanto' 'polo' 'primera' 'ptcruiser' 'punto' 'q3'\n",
      " 'q5' 'q7' 'qashqai' 'r19' 'range_rover' 'range_rover_evoque'\n",
      " 'range_rover_sport' 'rangerover' 'rav' 'rio' 'roadster' 'roomster'\n",
      " 'rx_reihe' 's60' 's_klasse' 's_max' 's_type' 'samara' 'sandero' 'santa'\n",
      " 'scenic' 'scirocco' 'seicento' 'serie_1' 'serie_2' 'serie_3' 'sharan'\n",
      " 'signum' 'sirion' 'sl' 'slk' 'sorento' 'spark' 'spider' 'sportage'\n",
      " 'sprinter' 'stilo' 'superb' 'swift' 'terios' 'tigra' 'tiguan' 'toledo'\n",
      " 'touareg' 'touran' 'transit' 'transporter' 'tt' 'tucson' 'twingo' 'up'\n",
      " 'v40' 'v50' 'v60' 'v70' 'v_klasse' 'vectra' 'verso' 'viano' 'vito'\n",
      " 'vivaro' 'voyager' 'wrangler' 'x_reihe' 'x_trail' 'x_type' 'xc_reihe'\n",
      " 'yaris' 'yeti' 'ypsilon' 'z_reihe' 'zafira' nan]\n",
      "____________________________________\n",
      "Количество уникальных значений столбца Kilometer - 13\n",
      "[  5000  10000  20000  30000  40000  50000  60000  70000  80000  90000\n",
      " 100000 125000 150000]\n",
      "____________________________________\n",
      "Количество уникальных значений столбца RegistrationMonth - 13\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "____________________________________\n",
      "Количество уникальных значений столбца FuelType - 7\n",
      "['cng' 'electric' 'gasoline' 'hybrid' 'lpg' 'other' 'petrol' nan]\n",
      "____________________________________\n",
      "Количество уникальных значений столбца Brand - 40\n",
      "['alfa_romeo' 'audi' 'bmw' 'chevrolet' 'chrysler' 'citroen' 'dacia'\n",
      " 'daewoo' 'daihatsu' 'fiat' 'ford' 'honda' 'hyundai' 'jaguar' 'jeep' 'kia'\n",
      " 'lada' 'lancia' 'land_rover' 'mazda' 'mercedes_benz' 'mini' 'mitsubishi'\n",
      " 'nissan' 'opel' 'peugeot' 'porsche' 'renault' 'rover' 'saab' 'seat'\n",
      " 'skoda' 'smart' 'sonstige_autos' 'subaru' 'suzuki' 'toyota' 'trabant'\n",
      " 'volkswagen' 'volvo']\n",
      "____________________________________\n",
      "Количество уникальных значений столбца Repaired - 2\n",
      "['no' 'yes' nan]\n",
      "____________________________________\n",
      "Количество уникальных значений столбца DateCreated - 109\n",
      "['2014-03-10 00:00:00' '2015-03-20 00:00:00' '2015-06-18 00:00:00'\n",
      " '2015-08-07 00:00:00' '2015-08-10 00:00:00' '2015-09-04 00:00:00'\n",
      " '2015-09-09 00:00:00' '2015-11-02 00:00:00' '2015-11-08 00:00:00'\n",
      " '2015-11-10 00:00:00' '2015-11-12 00:00:00' '2015-11-17 00:00:00'\n",
      " '2015-11-23 00:00:00' '2015-11-24 00:00:00' '2015-12-05 00:00:00'\n",
      " '2015-12-06 00:00:00' '2015-12-17 00:00:00' '2015-12-27 00:00:00'\n",
      " '2015-12-30 00:00:00' '2016-01-02 00:00:00' '2016-01-03 00:00:00'\n",
      " '2016-01-06 00:00:00' '2016-01-07 00:00:00' '2016-01-08 00:00:00'\n",
      " '2016-01-10 00:00:00' '2016-01-13 00:00:00' '2016-01-15 00:00:00'\n",
      " '2016-01-16 00:00:00' '2016-01-17 00:00:00' '2016-01-18 00:00:00'\n",
      " '2016-01-19 00:00:00' '2016-01-20 00:00:00' '2016-01-22 00:00:00'\n",
      " '2016-01-23 00:00:00' '2016-01-24 00:00:00' '2016-01-25 00:00:00'\n",
      " '2016-01-26 00:00:00' '2016-01-27 00:00:00' '2016-01-28 00:00:00'\n",
      " '2016-01-29 00:00:00' '2016-01-30 00:00:00' '2016-01-31 00:00:00'\n",
      " '2016-02-01 00:00:00' '2016-02-02 00:00:00' '2016-02-03 00:00:00'\n",
      " '2016-02-04 00:00:00' '2016-02-05 00:00:00' '2016-02-06 00:00:00'\n",
      " '2016-02-07 00:00:00' '2016-02-08 00:00:00' '2016-02-09 00:00:00'\n",
      " '2016-02-10 00:00:00' '2016-02-11 00:00:00' '2016-02-12 00:00:00'\n",
      " '2016-02-13 00:00:00' '2016-02-14 00:00:00' '2016-02-15 00:00:00'\n",
      " '2016-02-16 00:00:00' '2016-02-17 00:00:00' '2016-02-18 00:00:00'\n",
      " '2016-02-19 00:00:00' '2016-02-20 00:00:00' '2016-02-21 00:00:00'\n",
      " '2016-02-22 00:00:00' '2016-02-23 00:00:00' '2016-02-24 00:00:00'\n",
      " '2016-02-25 00:00:00' '2016-02-26 00:00:00' '2016-02-27 00:00:00'\n",
      " '2016-02-28 00:00:00' '2016-02-29 00:00:00' '2016-03-01 00:00:00'\n",
      " '2016-03-02 00:00:00' '2016-03-03 00:00:00' '2016-03-04 00:00:00'\n",
      " '2016-03-05 00:00:00' '2016-03-06 00:00:00' '2016-03-07 00:00:00'\n",
      " '2016-03-08 00:00:00' '2016-03-09 00:00:00' '2016-03-10 00:00:00'\n",
      " '2016-03-11 00:00:00' '2016-03-12 00:00:00' '2016-03-13 00:00:00'\n",
      " '2016-03-14 00:00:00' '2016-03-15 00:00:00' '2016-03-16 00:00:00'\n",
      " '2016-03-17 00:00:00' '2016-03-18 00:00:00' '2016-03-19 00:00:00'\n",
      " '2016-03-20 00:00:00' '2016-03-21 00:00:00' '2016-03-22 00:00:00'\n",
      " '2016-03-23 00:00:00' '2016-03-24 00:00:00' '2016-03-25 00:00:00'\n",
      " '2016-03-26 00:00:00' '2016-03-27 00:00:00' '2016-03-28 00:00:00'\n",
      " '2016-03-29 00:00:00' '2016-03-30 00:00:00' '2016-03-31 00:00:00'\n",
      " '2016-04-01 00:00:00' '2016-04-02 00:00:00' '2016-04-03 00:00:00'\n",
      " '2016-04-04 00:00:00' '2016-04-05 00:00:00' '2016-04-06 00:00:00'\n",
      " '2016-04-07 00:00:00']\n",
      "____________________________________\n",
      "Количество уникальных значений столбца NumberOfPictures - 1\n",
      "[0]\n",
      "____________________________________\n",
      "Количество уникальных значений столбца PostalCode - 8143\n",
      "[ 1067  1068  1069 ... 99994 99996 99998]\n",
      "____________________________________\n",
      "Количество уникальных значений столбца LastSeen - 179150\n",
      "['2016-03-05 14:15:08' '2016-03-05 14:15:16' '2016-03-05 14:15:39' ...\n",
      " '2016-04-07 14:58:49' '2016-04-07 14:58:50' '2016-04-07 14:58:51']\n",
      "____________________________________\n"
     ]
    }
   ],
   "source": [
    " check_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66471a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>RegistrationYear</th>\n",
       "      <th>Power</th>\n",
       "      <th>Kilometer</th>\n",
       "      <th>RegistrationMonth</th>\n",
       "      <th>NumberOfPictures</th>\n",
       "      <th>PostalCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>354369.000000</td>\n",
       "      <td>354369.000000</td>\n",
       "      <td>354369.000000</td>\n",
       "      <td>354369.000000</td>\n",
       "      <td>354369.000000</td>\n",
       "      <td>354369.0</td>\n",
       "      <td>354369.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4416.656776</td>\n",
       "      <td>2004.234448</td>\n",
       "      <td>110.094337</td>\n",
       "      <td>128211.172535</td>\n",
       "      <td>5.714645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50508.689087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4514.158514</td>\n",
       "      <td>90.227958</td>\n",
       "      <td>189.850405</td>\n",
       "      <td>37905.341530</td>\n",
       "      <td>3.726421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25783.096248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1067.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1050.000000</td>\n",
       "      <td>1999.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>125000.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30165.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2700.000000</td>\n",
       "      <td>2003.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>150000.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49413.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6400.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>150000.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71083.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20000.000000</td>\n",
       "      <td>9999.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>150000.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99998.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Price  RegistrationYear          Power      Kilometer  \\\n",
       "count  354369.000000     354369.000000  354369.000000  354369.000000   \n",
       "mean     4416.656776       2004.234448     110.094337  128211.172535   \n",
       "std      4514.158514         90.227958     189.850405   37905.341530   \n",
       "min         0.000000       1000.000000       0.000000    5000.000000   \n",
       "25%      1050.000000       1999.000000      69.000000  125000.000000   \n",
       "50%      2700.000000       2003.000000     105.000000  150000.000000   \n",
       "75%      6400.000000       2008.000000     143.000000  150000.000000   \n",
       "max     20000.000000       9999.000000   20000.000000  150000.000000   \n",
       "\n",
       "       RegistrationMonth  NumberOfPictures     PostalCode  \n",
       "count      354369.000000          354369.0  354369.000000  \n",
       "mean            5.714645               0.0   50508.689087  \n",
       "std             3.726421               0.0   25783.096248  \n",
       "min             0.000000               0.0    1067.000000  \n",
       "25%             3.000000               0.0   30165.000000  \n",
       "50%             6.000000               0.0   49413.000000  \n",
       "75%             9.000000               0.0   71083.000000  \n",
       "max            12.000000               0.0   99998.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d7a51e",
   "metadata": {},
   "source": [
    "- `DateCrawled`, `DateCreated`, `LastSeen` - данные предоставлены о скаченных анкетах, создания анкеты, активности пользователя за период март-апрель 2016 года.\n",
    "- `Price` - в данных оцене есть аномальные значения цены\n",
    "- `RegistrationYear` - не корректные значения года выпуска. Первый в мире автомобиль с двигателем внутреннего сгорания (как и автомобиль Даймлера), построенный в 1885 году, впервые серийный выпуск легковых машин начался в апреле 1894 года.Вряд ли имеем в выборки столь раритетные авто.\n",
    "- `Power` - Скоректируем на основе реально возможный значений в машинах. На сегодняшний день самым мощным легковым автомобилем в мире назван Dagger GT компании TranStar Racing LLC. Только представьте: мощность его двигателя превышает 2000 лошадиных сил. Для небольших машин (до гольф-класса включительно) вполне достаточно мотора в 100-130 сил.\n",
    "- `Model` - на первый взгляд названия моделей выглядят верно. Даже если это не так, то можно предположить, что это ошибка сервиса, скорее всего пользователь выбирает их из предложенного списка.\n",
    "- `Kilometer` -В рекомендациях по эксплуатации и обслуживанию транспортных средств упоминается, что нормальным пробегом можно назвать 75 тыс. км для автомобиля возрастом пять лет. Если машина десятилетняя, то нормальным для нее считается пробег 150 тыс. км. Но у нас и не такое может быть, поэтому будем считать максимальный размер в выборке по пробегу нормальным.\n",
    "- `RegistrationMonth` - в месяце регистрации указан нулевой месяц, но так как этот столбец будем удалять за ненадобностью, иследовать причину ошибки и исправлять не будем.\n",
    "- `NumberOfPictures` - этот столбец не заполнен, у всех объектов стоит значение ноль. То же будет удален."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b00c9c",
   "metadata": {},
   "source": [
    "**Вывод**\n",
    "\n",
    "- провели общий обзор данных,\n",
    "\n",
    "- определили информативные и не информативные столбцы,\n",
    "\n",
    "- в данных есть пропуски, обработаем их в следующем разделе,\n",
    "\n",
    "- в данных есть аномальные значения, обработаем из в следующем разделе."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c25481c",
   "metadata": {},
   "source": [
    "### Предобработка данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da653c60",
   "metadata": {},
   "source": [
    "<a id = 'none'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74123b58",
   "metadata": {},
   "source": [
    "#### Обработка пропусков"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c836a9",
   "metadata": {},
   "source": [
    "Так как в процессе обучения моделей машинного обучения мы будем работать с алгоритмами, которые обладают механизмами для работы с пропущенными значениями, создадим копию начального датасета, в котором не будем удалять пропуски в данных, удалим только выявленные дубликаты объектов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9a3bd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_missing = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe509ab",
   "metadata": {},
   "source": [
    "Поскольку не понятно, как восстановить данные в пропусках по столбцу  `Model` и чтоб не удалять объекты, заменим пропуски в этом столбце на категорию \"other\", эта категория уже есть в списке уникальных значений в столбце `Model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d66bb5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Model'].fillna('other', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ccb17c",
   "metadata": {},
   "source": [
    "Будем считать, что если у объекта отсутствуют данные о ремонте машины, ремонта не было, Заменим пропуски на значение `no`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a72f4adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Repaired'].fillna('no', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae25b6c",
   "metadata": {},
   "source": [
    "Попробуем восстановить пропуски в столбцах `VehicleType`, `Gearbox`, `FuelType`, по столбцам `Brand`, `Model`.\n",
    "\n",
    "Для этого определим моду этих признаков (`VehicleType`, `Gearbox`, `FuelType`) в выборке сгруппированной по столбцам `Brand`, `Model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00c1ab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_fill = ['VehicleType','Gearbox','FuelType']\n",
    "\n",
    "for column in columns_to_fill:    \n",
    "    median_per_group = df.groupby(by = ['Brand', 'Model'])[column].transform(lambda x: x.mode().iloc[0])    \n",
    "    df[column].fillna(median_per_group, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1e18a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество строк - объектов с пропусками 0\n",
      "Процент пропусков по всему датафрейму 0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>процент пропусков</th>\n",
       "      <th>количество пропусков</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VehicleType</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RegistrationYear</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gearbox</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kilometer</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RegistrationMonth</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FuelType</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brand</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Repaired</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateCreated</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumberOfPictures</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PostalCode</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LastSeen</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   процент пропусков  количество пропусков\n",
       "Price                            0.0                     0\n",
       "VehicleType                      0.0                     0\n",
       "RegistrationYear                 0.0                     0\n",
       "Gearbox                          0.0                     0\n",
       "Power                            0.0                     0\n",
       "Model                            0.0                     0\n",
       "Kilometer                        0.0                     0\n",
       "RegistrationMonth                0.0                     0\n",
       "FuelType                         0.0                     0\n",
       "Brand                            0.0                     0\n",
       "Repaired                         0.0                     0\n",
       "DateCreated                      0.0                     0\n",
       "NumberOfPictures                 0.0                     0\n",
       "PostalCode                       0.0                     0\n",
       "LastSeen                         0.0                     0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Количество строк - объектов с пропусками', df.isnull().sum().sum())    \n",
    "print('Процент пропусков по всему датафрейму', (df.isnull().sum().sum ()/len(df)*100).round(2))  \n",
    "\n",
    "pd.DataFrame({'процент пропусков':round((df.isna().mean()*100), ), \n",
    "              'количество пропусков': df.isnull().sum()}).sort_values(by = 'процент пропусков').tail(15) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daa7731",
   "metadata": {},
   "source": [
    "<a id = 'duplicates'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7f0484",
   "metadata": {},
   "source": [
    "#### Дубликаты"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8314e44",
   "metadata": {},
   "source": [
    "Посмотрим на количество дубликатов в основном датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fee79960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c7dace",
   "metadata": {},
   "source": [
    "Удалим эти объекты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04494e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b41c31",
   "metadata": {},
   "source": [
    "Найдем неявные дубликаты. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b56cdd4",
   "metadata": {},
   "source": [
    "Будем искать по всем столбцам кроме столбцов `Repaired`, `NumberOfPictures`, `LastSeen`, `DateCrawled`, `DateCreated`, `LastSeen` (тех стобцов, что планируем в дальнейшем удалить) - если заполнялись две (или более)одинаковые анкеты, то в одной из них могло не быть фото; могут быть разные данные или отсутствовать данные о ремонте; даты скачивания анкеты и дата создания анкеты совпадают по году, месяцу и дню; дата последней активности может отличаться у одинаковых анкет - если это случайно созданный дубликат, то анкета может быть заброшена и активности по ней нет. Можно предположить, что если такие неявные дубликаты будут обнаружены, то пользователь намерено или не намерено создавал одинаковые анкеты по одному и тому же авто в разное время (или практически одновременно). \n",
    "\n",
    "А так как предсказывать цену мы будем по техническим характеристикам объектов, то при совпадении всех технических параметров, а так же года регистрации и  цены - можно считать такие анкеты-дубликатами. \n",
    "\n",
    "Так же возьмем во внимание, то факт, что линейные модели чувствительны к  множественным наблюдениям с одинаковыми признаками. Процесс обучения модели может занять больше времени, если в данных много дубликатов. Каждый дубликат будет участвовать в обучении модели, что может увеличить количество шагов обучения.\n",
    "\n",
    "Если в дубликатах присутствуют разные значения целевой переменной, то они могут оказать большее влияние на веса признаков, так как алгоритмы бустинга будут стремиться учесть их как можно лучше. Не смотря на это, так как предсказывать будем цену и цена авто все же зависит от собственника, на факт различия цены в дубликатах все же рекомендуем не обращать внимание и выбирать дубликаты с одинаковой ценой.\n",
    "\n",
    "Проверим датасет на наличие дубликатов по тех характеристиками авто, году регистрации и цене."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4fc3752c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34979"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_duplicate = df.duplicated(subset = ['Price', 'VehicleType', 'RegistrationYear', 'Gearbox', 'Power', 'Model', 'Kilometer', \n",
    "                       'RegistrationMonth', 'FuelType', 'Brand'])\n",
    "\n",
    "df_duplicate.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8293118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateCrawled</th>\n",
       "      <th>Price</th>\n",
       "      <th>VehicleType</th>\n",
       "      <th>RegistrationYear</th>\n",
       "      <th>Gearbox</th>\n",
       "      <th>Power</th>\n",
       "      <th>Model</th>\n",
       "      <th>Kilometer</th>\n",
       "      <th>RegistrationMonth</th>\n",
       "      <th>FuelType</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Repaired</th>\n",
       "      <th>DateCreated</th>\n",
       "      <th>NumberOfPictures</th>\n",
       "      <th>PostalCode</th>\n",
       "      <th>LastSeen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>230741</th>\n",
       "      <td>2016-03-19 12:37:35</td>\n",
       "      <td>0</td>\n",
       "      <td>sedan</td>\n",
       "      <td>1000</td>\n",
       "      <td>manual</td>\n",
       "      <td>0</td>\n",
       "      <td>other</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>petrol</td>\n",
       "      <td>sonstige_autos</td>\n",
       "      <td>no</td>\n",
       "      <td>2016-03-19 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>36304</td>\n",
       "      <td>2016-03-19 12:37:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151725</th>\n",
       "      <td>2016-04-03 11:58:40</td>\n",
       "      <td>400</td>\n",
       "      <td>sedan</td>\n",
       "      <td>1000</td>\n",
       "      <td>auto</td>\n",
       "      <td>0</td>\n",
       "      <td>other</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>petrol</td>\n",
       "      <td>mercedes_benz</td>\n",
       "      <td>no</td>\n",
       "      <td>2016-04-03 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>8060</td>\n",
       "      <td>2016-04-07 13:16:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325739</th>\n",
       "      <td>2016-03-30 11:36:35</td>\n",
       "      <td>400</td>\n",
       "      <td>sedan</td>\n",
       "      <td>1000</td>\n",
       "      <td>auto</td>\n",
       "      <td>0</td>\n",
       "      <td>other</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>petrol</td>\n",
       "      <td>mercedes_benz</td>\n",
       "      <td>no</td>\n",
       "      <td>2016-03-30 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>8060</td>\n",
       "      <td>2016-04-01 06:16:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139360</th>\n",
       "      <td>2016-04-02 16:56:39</td>\n",
       "      <td>450</td>\n",
       "      <td>bus</td>\n",
       "      <td>1800</td>\n",
       "      <td>manual</td>\n",
       "      <td>1800</td>\n",
       "      <td>other</td>\n",
       "      <td>5000</td>\n",
       "      <td>2</td>\n",
       "      <td>petrol</td>\n",
       "      <td>mitsubishi</td>\n",
       "      <td>no</td>\n",
       "      <td>2016-04-02 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>63322</td>\n",
       "      <td>2016-04-04 14:46:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75845</th>\n",
       "      <td>2016-03-25 23:27:02</td>\n",
       "      <td>0</td>\n",
       "      <td>sedan</td>\n",
       "      <td>1910</td>\n",
       "      <td>manual</td>\n",
       "      <td>0</td>\n",
       "      <td>other</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "      <td>petrol</td>\n",
       "      <td>sonstige_autos</td>\n",
       "      <td>no</td>\n",
       "      <td>2016-03-25 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>35606</td>\n",
       "      <td>2016-03-29 03:44:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118038</th>\n",
       "      <td>2016-03-17 00:38:40</td>\n",
       "      <td>0</td>\n",
       "      <td>sedan</td>\n",
       "      <td>1910</td>\n",
       "      <td>manual</td>\n",
       "      <td>0</td>\n",
       "      <td>other</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>petrol</td>\n",
       "      <td>sonstige_autos</td>\n",
       "      <td>no</td>\n",
       "      <td>2016-03-16 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>91364</td>\n",
       "      <td>2016-03-23 19:45:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134473</th>\n",
       "      <td>2016-03-08 10:50:05</td>\n",
       "      <td>0</td>\n",
       "      <td>sedan</td>\n",
       "      <td>1910</td>\n",
       "      <td>manual</td>\n",
       "      <td>0</td>\n",
       "      <td>other</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>petrol</td>\n",
       "      <td>sonstige_autos</td>\n",
       "      <td>no</td>\n",
       "      <td>2016-03-08 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>6108</td>\n",
       "      <td>2016-03-08 17:47:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170747</th>\n",
       "      <td>2016-03-25 19:51:02</td>\n",
       "      <td>0</td>\n",
       "      <td>sedan</td>\n",
       "      <td>1910</td>\n",
       "      <td>manual</td>\n",
       "      <td>0</td>\n",
       "      <td>other</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "      <td>petrol</td>\n",
       "      <td>sonstige_autos</td>\n",
       "      <td>no</td>\n",
       "      <td>2016-03-25 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>35606</td>\n",
       "      <td>2016-03-28 22:47:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181793</th>\n",
       "      <td>2016-03-20 13:45:42</td>\n",
       "      <td>0</td>\n",
       "      <td>sedan</td>\n",
       "      <td>1910</td>\n",
       "      <td>manual</td>\n",
       "      <td>0</td>\n",
       "      <td>other</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>petrol</td>\n",
       "      <td>sonstige_autos</td>\n",
       "      <td>no</td>\n",
       "      <td>2016-03-20 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>39218</td>\n",
       "      <td>2016-03-21 16:17:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193266</th>\n",
       "      <td>2016-03-13 12:46:46</td>\n",
       "      <td>0</td>\n",
       "      <td>sedan</td>\n",
       "      <td>1910</td>\n",
       "      <td>manual</td>\n",
       "      <td>0</td>\n",
       "      <td>other</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>petrol</td>\n",
       "      <td>sonstige_autos</td>\n",
       "      <td>no</td>\n",
       "      <td>2016-03-13 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1454</td>\n",
       "      <td>2016-03-13 12:46:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                DateCrawled  Price VehicleType  RegistrationYear Gearbox  \\\n",
       "230741  2016-03-19 12:37:35      0       sedan              1000  manual   \n",
       "151725  2016-04-03 11:58:40    400       sedan              1000    auto   \n",
       "325739  2016-03-30 11:36:35    400       sedan              1000    auto   \n",
       "139360  2016-04-02 16:56:39    450         bus              1800  manual   \n",
       "75845   2016-03-25 23:27:02      0       sedan              1910  manual   \n",
       "118038  2016-03-17 00:38:40      0       sedan              1910  manual   \n",
       "134473  2016-03-08 10:50:05      0       sedan              1910  manual   \n",
       "170747  2016-03-25 19:51:02      0       sedan              1910  manual   \n",
       "181793  2016-03-20 13:45:42      0       sedan              1910  manual   \n",
       "193266  2016-03-13 12:46:46      0       sedan              1910  manual   \n",
       "\n",
       "        Power  Model  Kilometer  RegistrationMonth FuelType           Brand  \\\n",
       "230741      0  other       5000                  0   petrol  sonstige_autos   \n",
       "151725      0  other       5000                  0   petrol   mercedes_benz   \n",
       "325739      0  other       5000                  0   petrol   mercedes_benz   \n",
       "139360   1800  other       5000                  2   petrol      mitsubishi   \n",
       "75845       0  other       5000                  1   petrol  sonstige_autos   \n",
       "118038      0  other       5000                  0   petrol  sonstige_autos   \n",
       "134473      0  other       5000                  0   petrol  sonstige_autos   \n",
       "170747      0  other       5000                  1   petrol  sonstige_autos   \n",
       "181793      0  other       5000                  0   petrol  sonstige_autos   \n",
       "193266      0  other       5000                  0   petrol  sonstige_autos   \n",
       "\n",
       "       Repaired          DateCreated  NumberOfPictures  PostalCode  \\\n",
       "230741       no  2016-03-19 00:00:00                 0       36304   \n",
       "151725       no  2016-04-03 00:00:00                 0        8060   \n",
       "325739       no  2016-03-30 00:00:00                 0        8060   \n",
       "139360       no  2016-04-02 00:00:00                 0       63322   \n",
       "75845        no  2016-03-25 00:00:00                 0       35606   \n",
       "118038       no  2016-03-16 00:00:00                 0       91364   \n",
       "134473       no  2016-03-08 00:00:00                 0        6108   \n",
       "170747       no  2016-03-25 00:00:00                 0       35606   \n",
       "181793       no  2016-03-20 00:00:00                 0       39218   \n",
       "193266       no  2016-03-13 00:00:00                 0        1454   \n",
       "\n",
       "                   LastSeen  \n",
       "230741  2016-03-19 12:37:35  \n",
       "151725  2016-04-07 13:16:56  \n",
       "325739  2016-04-01 06:16:46  \n",
       "139360  2016-04-04 14:46:21  \n",
       "75845   2016-03-29 03:44:26  \n",
       "118038  2016-03-23 19:45:34  \n",
       "134473  2016-03-08 17:47:19  \n",
       "170747  2016-03-28 22:47:00  \n",
       "181793  2016-03-21 16:17:17  \n",
       "193266  2016-03-13 12:46:46  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df_duplicate].sort_values(['RegistrationYear', 'Price']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cdad922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateCrawled</th>\n",
       "      <th>Price</th>\n",
       "      <th>VehicleType</th>\n",
       "      <th>RegistrationYear</th>\n",
       "      <th>Gearbox</th>\n",
       "      <th>Power</th>\n",
       "      <th>Model</th>\n",
       "      <th>Kilometer</th>\n",
       "      <th>RegistrationMonth</th>\n",
       "      <th>FuelType</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Repaired</th>\n",
       "      <th>DateCreated</th>\n",
       "      <th>NumberOfPictures</th>\n",
       "      <th>PostalCode</th>\n",
       "      <th>LastSeen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>349349</th>\n",
       "      <td>2016-03-23 18:37:57</td>\n",
       "      <td>14900</td>\n",
       "      <td>sedan</td>\n",
       "      <td>2018</td>\n",
       "      <td>manual</td>\n",
       "      <td>0</td>\n",
       "      <td>1er</td>\n",
       "      <td>40000</td>\n",
       "      <td>3</td>\n",
       "      <td>petrol</td>\n",
       "      <td>bmw</td>\n",
       "      <td>no</td>\n",
       "      <td>2016-03-23 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>24850</td>\n",
       "      <td>2016-03-25 02:19:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337171</th>\n",
       "      <td>2016-03-14 21:56:49</td>\n",
       "      <td>16800</td>\n",
       "      <td>sedan</td>\n",
       "      <td>2018</td>\n",
       "      <td>manual</td>\n",
       "      <td>260</td>\n",
       "      <td>golf</td>\n",
       "      <td>90000</td>\n",
       "      <td>10</td>\n",
       "      <td>petrol</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>no</td>\n",
       "      <td>2016-03-14 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>55590</td>\n",
       "      <td>2016-03-27 00:46:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332451</th>\n",
       "      <td>2016-03-17 20:56:28</td>\n",
       "      <td>0</td>\n",
       "      <td>sedan</td>\n",
       "      <td>2019</td>\n",
       "      <td>manual</td>\n",
       "      <td>0</td>\n",
       "      <td>other</td>\n",
       "      <td>150000</td>\n",
       "      <td>0</td>\n",
       "      <td>petrol</td>\n",
       "      <td>sonstige_autos</td>\n",
       "      <td>no</td>\n",
       "      <td>2016-03-17 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>26655</td>\n",
       "      <td>2016-03-17 21:42:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57074</th>\n",
       "      <td>2016-03-28 11:45:31</td>\n",
       "      <td>1200</td>\n",
       "      <td>wagon</td>\n",
       "      <td>2019</td>\n",
       "      <td>manual</td>\n",
       "      <td>140</td>\n",
       "      <td>156</td>\n",
       "      <td>150000</td>\n",
       "      <td>5</td>\n",
       "      <td>petrol</td>\n",
       "      <td>alfa_romeo</td>\n",
       "      <td>no</td>\n",
       "      <td>2016-03-28 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>91058</td>\n",
       "      <td>2016-04-06 13:45:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334967</th>\n",
       "      <td>2016-03-20 17:53:51</td>\n",
       "      <td>12000</td>\n",
       "      <td>sedan</td>\n",
       "      <td>4000</td>\n",
       "      <td>manual</td>\n",
       "      <td>500</td>\n",
       "      <td>golf</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>petrol</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>no</td>\n",
       "      <td>2016-03-20 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>57392</td>\n",
       "      <td>2016-04-07 00:46:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231437</th>\n",
       "      <td>2016-03-31 08:36:44</td>\n",
       "      <td>200</td>\n",
       "      <td>small</td>\n",
       "      <td>5000</td>\n",
       "      <td>manual</td>\n",
       "      <td>0</td>\n",
       "      <td>corsa</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>petrol</td>\n",
       "      <td>opel</td>\n",
       "      <td>yes</td>\n",
       "      <td>2016-03-31 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>18528</td>\n",
       "      <td>2016-04-04 02:15:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326556</th>\n",
       "      <td>2016-03-29 21:36:16</td>\n",
       "      <td>7999</td>\n",
       "      <td>sedan</td>\n",
       "      <td>5911</td>\n",
       "      <td>auto</td>\n",
       "      <td>75</td>\n",
       "      <td>golf</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "      <td>petrol</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>no</td>\n",
       "      <td>2016-03-29 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>41462</td>\n",
       "      <td>2016-04-06 07:46:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118047</th>\n",
       "      <td>2016-03-27 01:57:32</td>\n",
       "      <td>4900</td>\n",
       "      <td>sedan</td>\n",
       "      <td>6000</td>\n",
       "      <td>manual</td>\n",
       "      <td>52</td>\n",
       "      <td>other</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "      <td>petrol</td>\n",
       "      <td>sonstige_autos</td>\n",
       "      <td>no</td>\n",
       "      <td>2016-03-27 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>38259</td>\n",
       "      <td>2016-04-06 22:44:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177353</th>\n",
       "      <td>2016-03-22 16:38:54</td>\n",
       "      <td>0</td>\n",
       "      <td>sedan</td>\n",
       "      <td>9999</td>\n",
       "      <td>manual</td>\n",
       "      <td>0</td>\n",
       "      <td>other</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "      <td>petrol</td>\n",
       "      <td>sonstige_autos</td>\n",
       "      <td>no</td>\n",
       "      <td>2016-03-22 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>32689</td>\n",
       "      <td>2016-03-22 16:38:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184598</th>\n",
       "      <td>2016-03-09 21:36:17</td>\n",
       "      <td>0</td>\n",
       "      <td>sedan</td>\n",
       "      <td>9999</td>\n",
       "      <td>manual</td>\n",
       "      <td>0</td>\n",
       "      <td>other</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "      <td>petrol</td>\n",
       "      <td>sonstige_autos</td>\n",
       "      <td>no</td>\n",
       "      <td>2016-03-09 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>32689</td>\n",
       "      <td>2016-03-13 01:46:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                DateCrawled  Price VehicleType  RegistrationYear Gearbox  \\\n",
       "349349  2016-03-23 18:37:57  14900       sedan              2018  manual   \n",
       "337171  2016-03-14 21:56:49  16800       sedan              2018  manual   \n",
       "332451  2016-03-17 20:56:28      0       sedan              2019  manual   \n",
       "57074   2016-03-28 11:45:31   1200       wagon              2019  manual   \n",
       "334967  2016-03-20 17:53:51  12000       sedan              4000  manual   \n",
       "231437  2016-03-31 08:36:44    200       small              5000  manual   \n",
       "326556  2016-03-29 21:36:16   7999       sedan              5911    auto   \n",
       "118047  2016-03-27 01:57:32   4900       sedan              6000  manual   \n",
       "177353  2016-03-22 16:38:54      0       sedan              9999  manual   \n",
       "184598  2016-03-09 21:36:17      0       sedan              9999  manual   \n",
       "\n",
       "        Power  Model  Kilometer  RegistrationMonth FuelType           Brand  \\\n",
       "349349      0    1er      40000                  3   petrol             bmw   \n",
       "337171    260   golf      90000                 10   petrol      volkswagen   \n",
       "332451      0  other     150000                  0   petrol  sonstige_autos   \n",
       "57074     140    156     150000                  5   petrol      alfa_romeo   \n",
       "334967    500   golf       5000                  0   petrol      volkswagen   \n",
       "231437      0  corsa       5000                  0   petrol            opel   \n",
       "326556     75   golf      10000                  0   petrol      volkswagen   \n",
       "118047     52  other      10000                  0   petrol  sonstige_autos   \n",
       "177353      0  other      10000                  0   petrol  sonstige_autos   \n",
       "184598      0  other      10000                  0   petrol  sonstige_autos   \n",
       "\n",
       "       Repaired          DateCreated  NumberOfPictures  PostalCode  \\\n",
       "349349       no  2016-03-23 00:00:00                 0       24850   \n",
       "337171       no  2016-03-14 00:00:00                 0       55590   \n",
       "332451       no  2016-03-17 00:00:00                 0       26655   \n",
       "57074        no  2016-03-28 00:00:00                 0       91058   \n",
       "334967       no  2016-03-20 00:00:00                 0       57392   \n",
       "231437      yes  2016-03-31 00:00:00                 0       18528   \n",
       "326556       no  2016-03-29 00:00:00                 0       41462   \n",
       "118047       no  2016-03-27 00:00:00                 0       38259   \n",
       "177353       no  2016-03-22 00:00:00                 0       32689   \n",
       "184598       no  2016-03-09 00:00:00                 0       32689   \n",
       "\n",
       "                   LastSeen  \n",
       "349349  2016-03-25 02:19:06  \n",
       "337171  2016-03-27 00:46:07  \n",
       "332451  2016-03-17 21:42:46  \n",
       "57074   2016-04-06 13:45:56  \n",
       "334967  2016-04-07 00:46:30  \n",
       "231437  2016-04-04 02:15:27  \n",
       "326556  2016-04-06 07:46:20  \n",
       "118047  2016-04-06 22:44:47  \n",
       "177353  2016-03-22 16:38:54  \n",
       "184598  2016-03-13 01:46:23  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df_duplicate].sort_values(['RegistrationYear', 'Price']).tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bf16d8",
   "metadata": {},
   "source": [
    "Обнаружено неявных дубликатов в 34843 объектах, что составляет 9.8% от общей выборки. \n",
    "\n",
    "Среди отобранных объектов - дубликатов наблюдаем объекты с пропусками по характеристикам типа-модели авто, техническим характеристикам, анамальные значения в столбце год регистрации, отсутвие данных о цене, отсутствие наименования модели. \n",
    "\n",
    "Удалим эти дубликаты. Для этого создадим копию датасета и в дальнейшем будем обучать модели на нескольких наборах данных: выборка с пропусками, выборка без пропусков и усеченная выборка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "781b9f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cut = df.copy()\n",
    "\n",
    "df_cut = df_cut.drop_duplicates(subset=['Price', 'VehicleType', 'RegistrationYear', 'Gearbox', 'Power', 'Model', 'Kilometer', \n",
    "                                'RegistrationMonth', 'FuelType', 'Brand']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa7d5fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cut.duplicated(subset = ['Price', 'VehicleType', 'RegistrationYear', 'Gearbox', 'Power', 'Model', 'Kilometer', \n",
    "                       'RegistrationMonth', 'FuelType', 'Brand']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aefac2b",
   "metadata": {},
   "source": [
    "<a id = 'with_missing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d160f6c2",
   "metadata": {},
   "source": [
    "#### Выбросы и аномальные значения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4bfc98",
   "metadata": {},
   "source": [
    "Посмотрим на выбросы по значимым столбцами:`Price`, `Power`, `Kilometer`, `RegistrationYear`.\n",
    "\n",
    "Постоим боксплот для каждого из вышеперечисленного признака, вызовем метод `boxplot_df()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6320c877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 6000x1500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boxplot_df(df_cut[['Price', 'Power', 'Kilometer', 'RegistrationYear']], 'Боксплот для признаков датасета')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b630d36",
   "metadata": {},
   "source": [
    "Наблюдаем выбросы на во всех столбцах.\n",
    "\n",
    "Выбросы в данных могут существенно влиять на производительность и качество модели градиентного бустинга.\n",
    "\n",
    "Найдем и посчитаем выбросы по каждому столбцу, вызовем метод `find_outliers()`. Удалять выбросы будем только на усеченной выборке `df_cut`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d31f6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество строк, выбранных для удаления в столбце Price :  17356\n",
      "Нижняя граница нормального размаха Price :  -6927.5\n",
      "Верхняя граница нормального размаха Price :  14476.5\n",
      "Процент строк, выбранных для удаления в столбце 5.43\n",
      "___________________________________________________________\n",
      "Количество строк, выбранных для удаления в столбце Power :  5347\n",
      "Нижняя граница нормального размаха Power :  -42.5\n",
      "Верхняя граница нормального размаха Power :  249.5\n",
      "Процент строк, выбранных для удаления в столбце 1.77\n",
      "___________________________________________________________\n",
      "Количество строк, выбранных для удаления в столбце Kilometer :  40226\n",
      "Нижняя граница нормального размаха Kilometer :  87500.0\n",
      "Верхняя граница нормального размаха Kilometer :  187500.0\n",
      "Процент строк, выбранных для удаления в столбце 13.56\n",
      "___________________________________________________________\n",
      "Количество строк, выбранных для удаления в столбце RegistrationYear :  16292\n",
      "Нижняя граница нормального размаха RegistrationYear :  1988.5\n",
      "Верхняя граница нормального размаха RegistrationYear :  2016.5\n",
      "Процент строк, выбранных для удаления в столбце 6.35\n",
      "___________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# столбцы с выбросами\n",
    "outlier_columns = [(df_cut,\"Price\"), (df_cut,\"Power\"), (df_cut,\"Kilometer\"), (df_cut,\"RegistrationYear\")]\n",
    "\n",
    "# удалим выбросы, возовем метод find_outliers()\n",
    "for data_cut,column in outlier_columns:    \n",
    "    indexes = find_outliers(data_cut,column)    \n",
    "    data_cut = data_cut.drop(indexes,axis = 0,inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5caa345",
   "metadata": {},
   "source": [
    "Рассмотрим границы нормального размаха, которые использовали при удалении выбросов на усеченной выборке `df_cut`. Проанализиреум их.\n",
    "\n",
    "Если руководствоваться \"здравым смыслом\" и параметрыми сайтов продажи б/у автомобилей, то необходимо удалить в основной выборке (с пропусками и без пропусков в данных) и из усеченной выборки объекты с ценой до 300 евро, ограничить года регистрации авто с 1900-2016гг (хотя авто сайты рекомендую ограничится 1980г), мощность от 50 л.с., а максимальную 500 л.с. \n",
    "\n",
    "В этом контексте можно попробовать не удалять данные, а привести все аномалии к нормальным с точки зрения \"здравого смысла\".\n",
    "\n",
    "По этапно будем выбирать значения, не соотвествующие установленной нами норме, и производить замену на медианные значения в группе авто такого же бренда и модели. \n",
    "\n",
    "Возможно в результате получим значения ниже или выше указанных порогов по некоторым объектам. Это будет говорить о том, что такой год или размер мощности все же соответствует данной модели (бренду) и это значение справедливо, так как ошибиться в анкете большинство пользователей не могли."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5545eade",
   "metadata": {},
   "source": [
    "Посмотрим сколько у нас объектов с ценой до 300 евро."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "865fa06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Процент объектов с ценой меньше 300 евро -  6.54\n",
      "Процент объектов с ценой меньше 300 евро на усеченной выборке -  6.54\n"
     ]
    }
   ],
   "source": [
    "print('Процент объектов с ценой меньше 300 евро - ', (df.query('Price < 300')['Price'].count()/len(df)*100).round(2))\n",
    "\n",
    "print('Процент объектов с ценой меньше 300 евро на усеченной выборке - ', \\\n",
    "      (df.query('Price < 300')['Price'].count()/len(df)*100).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329d945f",
   "metadata": {},
   "source": [
    "6,5% данных конечно много, но так как предсказывать будем цену на авто, то восстанавливать цену по другим параметрам не целесообразно. Удалим эти данные из выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5cf75239",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.query('Price >= 300')\n",
    "df_cut = df_cut.query('Price >= 300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3326103",
   "metadata": {},
   "source": [
    "Посмотрим гистограммы распределения признаков `RegistrationYear` и `Power`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69fc73f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    317677.000000\n",
       "mean       2002.746176\n",
       "std           6.803505\n",
       "min        1910.000000\n",
       "25%        1999.000000\n",
       "50%        2003.000000\n",
       "75%        2007.000000\n",
       "max        2016.000000\n",
       "Name: RegistrationYear, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtred_autos_registration_year = df.query('1894 < RegistrationYear < 2017')\n",
    "filtred_autos_registration_year['RegistrationYear'].hist(bins=50, figsize=(12,5))\n",
    "plt.title('Гистограмма признака RegistrationYear', fontsize=14)\n",
    "plt.xticks(fontsize=12)  \n",
    "plt.yticks(fontsize=12)\n",
    "filtred_autos_registration_year['RegistrationYear'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece0fabc",
   "metadata": {},
   "source": [
    "По левому хвосту гистограммы видно, что разумно использовать левую границу в 1965 год - это старые, но возможно еще работающие машины."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee7a5f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    330926.000000\n",
       "mean        109.673163\n",
       "std          63.058595\n",
       "min           0.000000\n",
       "25%          75.000000\n",
       "50%         105.000000\n",
       "75%         143.000000\n",
       "max         999.000000\n",
       "Name: Power, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtred_autos_power = df.query('Power < 1000')\n",
    "filtred_autos_power['Power'].hist(bins=30, figsize=(12,5))\n",
    "plt.title('Гистограмма признака Power', fontsize=14)\n",
    "plt.xticks(fontsize=12)  \n",
    "plt.yticks(fontsize=12)\n",
    "filtred_autos_power['Power'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a588581",
   "metadata": {},
   "source": [
    "График показывает, что достоточно много авто имеют мощность 0. По правому хвосту выбреме границу в 400 л.с., по левому 26л.с."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9aaff5",
   "metadata": {},
   "source": [
    "Посмотрим какое количество объектов имеют аномальные значения как по времени регистрации , так и по мощности авто."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9198fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3684"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('(RegistrationYear < 1965 or RegistrationYear > 2016) and (Power < 26 or Power > 400) ')['Power'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7c9ee7",
   "metadata": {},
   "source": [
    "Так же отметим количество объектов, данные которых отклоняются от границ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bdcce7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "575"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('(RegistrationYear < 1965 ) ')['Power'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "300a7136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13488"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('(RegistrationYear > 2016 ) ')['Power'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b99d922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32467"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('(Power < 26 ) ')['Power'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a3983c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "669"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('(Power > 400 ) ')['Power'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a31cb93",
   "metadata": {},
   "source": [
    "Удалим объекты, в который отклонения от указанных границ встречаются одновременном в двух значимых столбцах. Удаление проведем на выборке без пропусков в данных. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bcf6adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Условие для выборки без пропусков\n",
    "condition = ((df['RegistrationYear'] < 1965)|(df['RegistrationYear'] > 2016)) & ((df['Power'] < 26)|(df['Power'] > 400))\n",
    "# Получение индексов строк, соответствующих условию\n",
    "indexes_to_drop = df[condition].index\n",
    "# Удаление строк по индексам\n",
    "df.drop(indexes_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0987e63b",
   "metadata": {},
   "source": [
    "Ограничим дату регистрации авто только в основных выборках (без пропусков в данных),  в усеченной выборке таких данных уже нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1f0a09fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычисление медианных значений в группировке для выборки очищенной от пропусков\n",
    "grouped_median = df.groupby(['Brand', 'Model'])['RegistrationYear'].transform('median')\n",
    "\n",
    "# Замена значений, меньших или больше указанного значения, на медианное значение\n",
    "condition = df['RegistrationYear'] < 1965\n",
    "df.loc[condition, 'RegistrationYear'] = grouped_median[condition]\n",
    "condition = df['RegistrationYear'] > 2016\n",
    "df.loc[condition, 'RegistrationYear'] = grouped_median[condition]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b5790",
   "metadata": {},
   "source": [
    "Ограничим мощность авто в основных выборках (без пропусков в данных)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1a3450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_median = df.groupby(['Brand', 'Model'])['Power'].transform('median')\n",
    "condition = df['Power'] < 26\n",
    "df.loc[condition, 'Power'] = grouped_median[condition]\n",
    "condition = df['Power'] > 400\n",
    "df.loc[condition, 'Power'] = grouped_median[condition]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597e0668",
   "metadata": {},
   "source": [
    "Посмотрим, что в результате замены у нас получилось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce22ca83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('(RegistrationYear < 1965 ) ')['Power'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7c248b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('(RegistrationYear > 2016 ) ')['Power'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2e69b8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('(Power > 400 ) ')['Power'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b51f60d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('(Power < 26 ) ')['Power'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e708fab",
   "metadata": {},
   "source": [
    "По мощности 1 объекта сохранили мощность ниже 26л.с., рассмотрим данные подробнее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dacd4d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateCrawled</th>\n",
       "      <th>Price</th>\n",
       "      <th>VehicleType</th>\n",
       "      <th>RegistrationYear</th>\n",
       "      <th>Gearbox</th>\n",
       "      <th>Power</th>\n",
       "      <th>Model</th>\n",
       "      <th>Kilometer</th>\n",
       "      <th>RegistrationMonth</th>\n",
       "      <th>FuelType</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Repaired</th>\n",
       "      <th>DateCreated</th>\n",
       "      <th>NumberOfPictures</th>\n",
       "      <th>PostalCode</th>\n",
       "      <th>LastSeen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>234296</th>\n",
       "      <td>2016-03-30 11:39:08</td>\n",
       "      <td>3800</td>\n",
       "      <td>wagon</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>manual</td>\n",
       "      <td>0.0</td>\n",
       "      <td>serie_1</td>\n",
       "      <td>30000</td>\n",
       "      <td>0</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>land_rover</td>\n",
       "      <td>no</td>\n",
       "      <td>2016-03-30 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>49824</td>\n",
       "      <td>2016-03-30 11:39:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                DateCrawled  Price VehicleType  RegistrationYear Gearbox  \\\n",
       "234296  2016-03-30 11:39:08   3800       wagon            1978.0  manual   \n",
       "\n",
       "        Power    Model  Kilometer  RegistrationMonth  FuelType       Brand  \\\n",
       "234296    0.0  serie_1      30000                  0  gasoline  land_rover   \n",
       "\n",
       "       Repaired          DateCreated  NumberOfPictures  PostalCode  \\\n",
       "234296       no  2016-03-30 00:00:00                 0       49824   \n",
       "\n",
       "                   LastSeen  \n",
       "234296  2016-03-30 11:39:08  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('(Power < 26 ) ').sort_values('Power')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb28ffc9",
   "metadata": {},
   "source": [
    "Удалим этот объект с мощностью ноль бренда `land_rover`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1a6c97ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['Power'] == 0].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ac68f4",
   "metadata": {},
   "source": [
    "Проверим еще раз основные выборки на дубликаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "138241cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fc2b61",
   "metadata": {},
   "source": [
    "Дубликатов в основной выборке нет."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775e6c21",
   "metadata": {},
   "source": [
    "Удалим все, определенными нами ранее, аномальные значения на усеченной выборке.\n",
    "\n",
    "Помимо цены ниже 300 дол (эти объекты были удалены ранее), имеем зачения мощности меньше 26л.с., удалим их на усеченной выборке.\n",
    "\n",
    "Определенные ранее аномальные значения по дате регистрации авто были удалены на этой выборке при поиске выбросов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3a934bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21216"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cut.query('(Power < 26 ) ')['Power'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cd195859",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cut = df_cut.query('Power >= 26')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40f57c7",
   "metadata": {},
   "source": [
    "Удалим, как и планировали ранее, не информативные столбцы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "48a15cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['DateCrawled', 'DateCreated', 'LastSeen', 'RegistrationMonth',\n",
    "                        'NumberOfPictures', 'PostalCode'], axis=1)\n",
    "df_with_missing = df_with_missing.drop(columns = ['DateCrawled', 'DateCreated', 'LastSeen', 'RegistrationMonth', \n",
    "                                                  'NumberOfPictures', 'PostalCode'], axis=1)\n",
    "df_cut = df_cut.drop(columns = ['DateCrawled', 'DateCreated', 'LastSeen', 'RegistrationMonth', \n",
    "                                'NumberOfPictures', 'PostalCode'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ef3d26",
   "metadata": {},
   "source": [
    "Проверим усеченную выборку на дубликаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "93361ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19011"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cut.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e521d0",
   "metadata": {},
   "source": [
    "Получили в результате преобразований еще большой объем дубликатов. Удалим их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9864fac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cut = df_cut.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6bf436e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 327516 entries, 0 to 354368\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   Price             327516 non-null  int64  \n",
      " 1   VehicleType       327516 non-null  object \n",
      " 2   RegistrationYear  327516 non-null  float64\n",
      " 3   Gearbox           327516 non-null  object \n",
      " 4   Power             327516 non-null  float64\n",
      " 5   Model             327516 non-null  object \n",
      " 6   Kilometer         327516 non-null  int64  \n",
      " 7   FuelType          327516 non-null  object \n",
      " 8   Brand             327516 non-null  object \n",
      " 9   Repaired          327516 non-null  object \n",
      "dtypes: float64(2), int64(2), object(6)\n",
      "memory usage: 27.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eaf6adce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 354369 entries, 0 to 354368\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   Price             354369 non-null  int64 \n",
      " 1   VehicleType       316879 non-null  object\n",
      " 2   RegistrationYear  354369 non-null  int64 \n",
      " 3   Gearbox           334536 non-null  object\n",
      " 4   Power             354369 non-null  int64 \n",
      " 5   Model             334664 non-null  object\n",
      " 6   Kilometer         354369 non-null  int64 \n",
      " 7   FuelType          321474 non-null  object\n",
      " 8   Brand             354369 non-null  object\n",
      " 9   Repaired          283215 non-null  object\n",
      "dtypes: int64(4), object(6)\n",
      "memory usage: 27.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_with_missing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "88de1b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 184294 entries, 2 to 319384\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   Price             184294 non-null  int64 \n",
      " 1   VehicleType       184294 non-null  object\n",
      " 2   RegistrationYear  184294 non-null  int64 \n",
      " 3   Gearbox           184294 non-null  object\n",
      " 4   Power             184294 non-null  int64 \n",
      " 5   Model             184294 non-null  object\n",
      " 6   Kilometer         184294 non-null  int64 \n",
      " 7   FuelType          184294 non-null  object\n",
      " 8   Brand             184294 non-null  object\n",
      " 9   Repaired          184294 non-null  object\n",
      "dtypes: int64(4), object(6)\n",
      "memory usage: 15.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_cut.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f011aed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Процент уделенных объектов в основной выборке 7.58\n",
      "Процент уделенных объектов в основной выборке с пропусками даных 0.00\n",
      "Процент уделенных объектов в усеченной выборке 47.99\n"
     ]
    }
   ],
   "source": [
    "print('Процент уделенных объектов в основной выборке {:.2f}'.format((df_old - df.shape[0])/df_old*100))\n",
    "print('Процент уделенных объектов в основной выборке с пропусками даных {:.2f}'.\\\n",
    "      format((df_old - df_with_missing.shape[0])/df_old*100))\n",
    "\n",
    "print('Процент уделенных объектов в усеченной выборке {:.2f}'.format((df_old - df_cut.shape[0])/df_old*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f25d33",
   "metadata": {},
   "source": [
    "#### Смена типа данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51f6611",
   "metadata": {},
   "source": [
    "В результате преобразований сменился тип некоторых данных, приведем тип `float` в `int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "19e64e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RegistrationYear'] = df['RegistrationYear'].astype('int64')\n",
    "df['Power'] = df['Power'].astype('int64')\n",
    "df_with_missing['RegistrationYear'] = df_with_missing['RegistrationYear'].astype('int64')\n",
    "df_with_missing['Power'] = df_with_missing['Power'].astype('int64')\n",
    "df_cut['Power'] = df_cut['Power'].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493dbfe2",
   "metadata": {},
   "source": [
    "Выделим категориальные столбцы. Сменим тип на `category`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "82682934",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features  = ['VehicleType', 'Gearbox', 'Model', 'FuelType', 'Brand', 'Repaired']\n",
    "df[cat_features] = df[cat_features].astype('category')\n",
    "df_with_missing[cat_features] = df_with_missing[cat_features].astype('category')\n",
    "df_cut[cat_features] = df_cut[cat_features].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7f7058",
   "metadata": {},
   "source": [
    "<a id = 'corr'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1967b50f",
   "metadata": {},
   "source": [
    "#### Корреляция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "645f3d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>RegistrationYear</th>\n",
       "      <th>Power</th>\n",
       "      <th>Kilometer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.465216</td>\n",
       "      <td>0.500157</td>\n",
       "      <td>-0.370739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RegistrationYear</th>\n",
       "      <td>0.465216</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.115872</td>\n",
       "      <td>-0.260317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power</th>\n",
       "      <td>0.500157</td>\n",
       "      <td>0.115872</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.108876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kilometer</th>\n",
       "      <td>-0.370739</td>\n",
       "      <td>-0.260317</td>\n",
       "      <td>0.108876</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Price  RegistrationYear     Power  Kilometer\n",
       "Price             1.000000          0.465216  0.500157  -0.370739\n",
       "RegistrationYear  0.465216          1.000000  0.115872  -0.260317\n",
       "Power             0.500157          0.115872  1.000000   0.108876\n",
       "Kilometer        -0.370739         -0.260317  0.108876   1.000000"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Price', 'RegistrationYear', 'Power', 'Kilometer']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cf5b5c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>RegistrationYear</th>\n",
       "      <th>Power</th>\n",
       "      <th>Kilometer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026916</td>\n",
       "      <td>0.158872</td>\n",
       "      <td>-0.333199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RegistrationYear</th>\n",
       "      <td>0.026916</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.000828</td>\n",
       "      <td>-0.053447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power</th>\n",
       "      <td>0.158872</td>\n",
       "      <td>-0.000828</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kilometer</th>\n",
       "      <td>-0.333199</td>\n",
       "      <td>-0.053447</td>\n",
       "      <td>0.024002</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Price  RegistrationYear     Power  Kilometer\n",
       "Price             1.000000          0.026916  0.158872  -0.333199\n",
       "RegistrationYear  0.026916          1.000000 -0.000828  -0.053447\n",
       "Power             0.158872         -0.000828  1.000000   0.024002\n",
       "Kilometer        -0.333199         -0.053447  0.024002   1.000000"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_missing[['Price', 'RegistrationYear', 'Power', 'Kilometer']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ade18e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>RegistrationYear</th>\n",
       "      <th>Power</th>\n",
       "      <th>Kilometer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.535200</td>\n",
       "      <td>0.512237</td>\n",
       "      <td>-0.215413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RegistrationYear</th>\n",
       "      <td>0.535200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.122590</td>\n",
       "      <td>-0.208971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power</th>\n",
       "      <td>0.512237</td>\n",
       "      <td>0.122590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.140048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kilometer</th>\n",
       "      <td>-0.215413</td>\n",
       "      <td>-0.208971</td>\n",
       "      <td>0.140048</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Price  RegistrationYear     Power  Kilometer\n",
       "Price             1.000000          0.535200  0.512237  -0.215413\n",
       "RegistrationYear  0.535200          1.000000  0.122590  -0.208971\n",
       "Power             0.512237          0.122590  1.000000   0.140048\n",
       "Kilometer        -0.215413         -0.208971  0.140048   1.000000"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cut[['Price', 'RegistrationYear', 'Power', 'Kilometer']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68722d89",
   "metadata": {},
   "source": [
    "Матрица корреляции показывает, что между независимыми признаками `RegistrationYear`, `Power` и целевой переменной `Price` существуют хорошая положительная корреляция, а между независимым признаком `Kilometer` и целевой переменной `Price` наблюдается слабая отрицательная корреляция. Это подтвержает очевидные вещи: чем новее авто и больше мощность двигателя, тем цена выше; чем больше пребег, тем цена ниже. Между независимыми признаками наблюдается очень слабая положительная корреляция, только у признаков `RegistrationYear` и `Kilometer` вполне обосновано видим очень слабую отрицательную корреляцию.\n",
    "\n",
    "Данные наблюдения говорят о том, что линейные моделям легче будет определить линейную зависимость между независимыми признаками и целевой переменной.\n",
    "\n",
    "Матрица корреляции показала, что частичная обработка данных повысила корреляцию независимых признаков между собой и  корреляцию целевой переменной и независимых признаков. При этом глубокая очистка данных от дубликатов, выбросов и анамальных значений положительно повляла на структуру данные, уменьшив корреляцию между независимыми признаками и повысила корреляцию целевой переменной и независимых признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14843cab",
   "metadata": {},
   "source": [
    "**Вывод**\n",
    "\n",
    "Провели предобработку данных:\n",
    "\n",
    "- обработали пропуски, пропуски заполнили,\n",
    "\n",
    "- дополнительно создали датасет, копию начального, пропуски в котором не удаляли,\n",
    "\n",
    "- нашли и удалили дубликаты,\n",
    "\n",
    "- создали копию основного датасета. На копии (усеченная выборка) были удалены неявные дубликаты,\n",
    "\n",
    "- на копии датасета так же удалили выбросы,  \n",
    "\n",
    "- рассмотрели \"разумные\" значения в столбцах на полных (с пропусками и без пропусков данных) и усеченной выборке. Удалили данные, где аномальные значения встречаются в нескольких значимых столбцах, остальные данные привели в медианным по бренду и модели.\n",
    "\n",
    "- в дальнейшем будем рассматривать три выборки, сравним влияние дубликатов и выбросов на результат работы моделей мшинного обучения, посмотрим как модели работают с пропусками.\n",
    "\n",
    "- изучили корреляцию между независимыми признаками и целевой переменной."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f4b219",
   "metadata": {},
   "source": [
    "## Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe996851",
   "metadata": {},
   "source": [
    "Выберем следующие регрессоры для обучения модели:\n",
    "\n",
    "- `LinearRegression`\n",
    "\n",
    "- `Ridge`\n",
    "\n",
    "- `DecisionTreeRegressor`\n",
    "\n",
    "- `CatBoostRegressor`\n",
    "\n",
    "- `LGBMRegressor`\n",
    "\n",
    "В качестве метрики для моделей будем использовать  `RMSE`. Качество моделей будем проверять на кроссвалидации.\n",
    "\n",
    "Подготовим три типа признаков для кроссвалидации:\n",
    "\n",
    "- признаки без изменений с маркировкой `X_y_set_`,\n",
    "\n",
    "- с закодированными категориальными признаками с маркировкой `X_y_coder_`,\n",
    "\n",
    "- с масштабированные количественные признаки и закодированными категориальными признаками с маркировкой `X_y_coder_scaled_`.\n",
    "\n",
    "Так как `CatBoostRegressor` и  `LGBMRegressor` автоматически обрабатывают категориальные признаки, делая их более подходящими для градиентного бустинга без необходимости предварительного преобразования в числовые значения будем использовать наборы датасетов с маркировкой `X_y_set_`.\n",
    "\n",
    "Так же `CatBoostRegressor` и  `LGBMRegressor` способны работать с данными в исходной шкале без необходимости масштабирования признаков, но можно проверить результат работы на масштабированных и не масштабированных признаках, так же предварительное кодирование категориальных признаков может помочь модели работать более эффективно и точно.Дополнительно будем использовать наборы датасетов с маркировкой `X_y_coder_scaled_`.\n",
    "\n",
    "`DecisionTreeRegressor`, работает с категориальными признаками, представленными в числовом виде, будем использовать наборы выборок с маркировкой `X_y_coder_scaled_` (на кодированных и масштабированных признаках).\n",
    "\n",
    "`Ridge` как и линейная модель `LinearRegression`, предполагает числовые входы, поэтому категориальные признаки   следует закодировать числовыми значениями, значит будем использовать наборы датасетов с маркировкой `X_y_coder_scaled_` (для масштабированных признаков с обязательной кодировкой категориальных признаков). \n",
    "\n",
    "~~Для кодировки категориальных признаков воспользуемся методом `OHE`.~~\n",
    "\n",
    "Для кодировки категориальных признаков воспользуемся методом `OneHotEncoder` и  `OrdinalEncoder`.\n",
    "\n",
    "Для `LinearRegression` и `Ridge` будем использовать `OneHotEncoder`, а для алгоритмов `DecisionTreeRegressor`, `CatBoostRegressor` и  `LGBMRegressor` воспользуемся `OrdinalEncoder`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2968a6f7",
   "metadata": {},
   "source": [
    "<a id = 'cod_scaler'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76c44d8",
   "metadata": {},
   "source": [
    "### Подготовка выборок"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1289d0e2",
   "metadata": {},
   "source": [
    "Выделим независимые признаки и целевой признак `Price`. Вызовем метод `select_target()` и получим наборы выборок.\n",
    "\n",
    "Напомним, что у нас есть три выборки: \n",
    "\n",
    "- `df_with_missing` - не удаляли неявные дубликаты, удалены аномальные значения, есть пропуски в данных\n",
    "\n",
    "- `df` - не удаляли неявные дубликаты, удалены аномальные значения, пропусков в данных нет\n",
    "\n",
    "- `df_cut` - удалены неявные дубликаты, удалены выбросы и аномальные значения, пропусков в данных нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "84b6d45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем объект класса, передавая cat_features\n",
    "model_evaluator = ModelEvaluator(cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "65195acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# без пропусков в данных\n",
    "X_y_set = model_evaluator.select_target(df)\n",
    "\n",
    "# с пропусками в данных\n",
    "X_y_set_with_missing = model_evaluator.select_target(df_with_missing)\n",
    "\n",
    "# без пропусков в данных, без выбросов\n",
    "X_y_set_cut = model_evaluator.select_target(df_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f311297a",
   "metadata": {},
   "source": [
    "Проверим размеры полученных выборок в наборах, где \n",
    "\n",
    "- под индексом - 0 - тренировочная выборка независимых признаков, \n",
    "\n",
    "- под индексом 1 - целевой признак тренировочной выборки, \n",
    "\n",
    "- под индексом 2 - тестовая выборка независимых признаков, \n",
    "\n",
    "- под индексом 3 - целевой признак тестовой выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b6cd06c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((229261, 9), (229261,), (98255, 9), (98255,))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_y_set[0].shape, X_y_set[1].shape, X_y_set[2].shape, X_y_set[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ca95fdfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((248058, 9), (248058,), (106311, 9), (106311,))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_y_set_with_missing[0].shape, X_y_set_with_missing[1].shape, X_y_set_with_missing[2].shape, X_y_set_with_missing[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "57d0afbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((129005, 9), (129005,), (55289, 9), (55289,))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_y_set_cut[0].shape, X_y_set_cut[1].shape, X_y_set_cut[2].shape, X_y_set_cut[3].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb65361e",
   "metadata": {},
   "source": [
    "Вызовем метод `scaled_features()` для масштабирования данных и получения наборов выборок с масштабированными данными.\n",
    "\n",
    "Так как пропущенные значения могут влиять на вычисление среднего и стандартного отклонения, что может повлиять на масштабирование, если оно выполняется до заполнения пропусков, то масштабирование данных будем проводить по выборкам, где нет пропусков в данных, а именно `X_y_set` и `X_y_set_cut`. \n",
    "\n",
    "Выборки с масштабированными признаками будем использовать как промежуточные, на этих выборках проведем кодирование категориальных признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d7a442e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y_scaled_set =  model_evaluator.scaled_features(X_y_set)\n",
    "X_y_scaled_set_cut = model_evaluator.scaled_features(X_y_set_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1002912a",
   "metadata": {},
   "source": [
    "Вызовем метод `encode_method_ohe()` для проведения кодировки категориальных признаков.\n",
    "\n",
    "Так как пропущенные значения могут дать дополнительные классы при кодировке данных, то кодирование категориальных признаков будем проводить по выборкам, где нет пропусков в данных (с маштабированными данными `X_y_scaled_set` и `X_y_scaled_set_cut`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "67d1af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y_scaled_ohe_set =  model_evaluator.encode_method_ohe(X_y_scaled_set)\n",
    "X_y_scaled_ohe_set_cut = model_evaluator.encode_method_ohe(X_y_scaled_set_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5d74c435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>304194</th>\n",
       "      <th>30418</th>\n",
       "      <th>201995</th>\n",
       "      <th>10301</th>\n",
       "      <th>6331</th>\n",
       "      <th>169492</th>\n",
       "      <th>219341</th>\n",
       "      <th>90099</th>\n",
       "      <th>342637</th>\n",
       "      <th>351874</th>\n",
       "      <th>...</th>\n",
       "      <th>238533</th>\n",
       "      <th>50063</th>\n",
       "      <th>139742</th>\n",
       "      <th>158489</th>\n",
       "      <th>210291</th>\n",
       "      <th>208349</th>\n",
       "      <th>19236</th>\n",
       "      <th>30421</th>\n",
       "      <th>300668</th>\n",
       "      <th>269773</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VehicleType_convertible</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VehicleType_sedan</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VehicleType_wagon</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VehicleType_coupe</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VehicleType_suv</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brand_chrysler</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brand_subaru</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brand_lancia</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brand_lada</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Repaired_yes</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>305 rows × 229261 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         304194  30418   201995  10301   6331    169492  \\\n",
       "VehicleType_convertible     0.0     0.0     0.0     1.0     1.0     0.0   \n",
       "VehicleType_sedan           0.0     0.0     0.0     0.0     0.0     1.0   \n",
       "VehicleType_wagon           0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "VehicleType_coupe           0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "VehicleType_suv             0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...                         ...     ...     ...     ...     ...     ...   \n",
       "Brand_chrysler              0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "Brand_subaru                0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "Brand_lancia                0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "Brand_lada                  0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "Repaired_yes                0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "                         219341  90099   342637  351874  ...  238533  50063   \\\n",
       "VehicleType_convertible     0.0     0.0     0.0     0.0  ...     0.0     0.0   \n",
       "VehicleType_sedan           0.0     0.0     1.0     0.0  ...     1.0     0.0   \n",
       "VehicleType_wagon           0.0     1.0     0.0     1.0  ...     0.0     0.0   \n",
       "VehicleType_coupe           0.0     0.0     0.0     0.0  ...     0.0     0.0   \n",
       "VehicleType_suv             0.0     0.0     0.0     0.0  ...     0.0     0.0   \n",
       "...                         ...     ...     ...     ...  ...     ...     ...   \n",
       "Brand_chrysler              0.0     0.0     0.0     0.0  ...     0.0     0.0   \n",
       "Brand_subaru                0.0     0.0     0.0     0.0  ...     0.0     0.0   \n",
       "Brand_lancia                0.0     0.0     0.0     0.0  ...     0.0     0.0   \n",
       "Brand_lada                  0.0     0.0     0.0     0.0  ...     0.0     0.0   \n",
       "Repaired_yes                0.0     0.0     0.0     0.0  ...     1.0     0.0   \n",
       "\n",
       "                         139742  158489  210291  208349  19236   30421   \\\n",
       "VehicleType_convertible     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "VehicleType_sedan           0.0     0.0     1.0     1.0     0.0     0.0   \n",
       "VehicleType_wagon           0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "VehicleType_coupe           0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "VehicleType_suv             0.0     0.0     0.0     0.0     1.0     0.0   \n",
       "...                         ...     ...     ...     ...     ...     ...   \n",
       "Brand_chrysler              0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "Brand_subaru                0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "Brand_lancia                0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "Brand_lada                  0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "Repaired_yes                0.0     0.0     0.0     1.0     0.0     0.0   \n",
       "\n",
       "                         300668  269773  \n",
       "VehicleType_convertible     0.0     0.0  \n",
       "VehicleType_sedan           0.0     1.0  \n",
       "VehicleType_wagon           0.0     0.0  \n",
       "VehicleType_coupe           0.0     0.0  \n",
       "VehicleType_suv             0.0     0.0  \n",
       "...                         ...     ...  \n",
       "Brand_chrysler              0.0     0.0  \n",
       "Brand_subaru                0.0     0.0  \n",
       "Brand_lancia                0.0     0.0  \n",
       "Brand_lada                  0.0     0.0  \n",
       "Repaired_yes                0.0     0.0  \n",
       "\n",
       "[305 rows x 229261 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_y_scaled_ohe_set[0].T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be46adca",
   "metadata": {},
   "source": [
    "Вызовем метод `encode_method_ordinal()` для проведения кодировки категориальных признаков.\n",
    "\n",
    "Так как пропущенные значения могут дать дополнительные классы при кодировке данных, то кодирование категориальных признаков будем проводить по выборкам, где нет пропусков в данных (с маштабированными данными `X_y_scaled_set` и `X_y_scaled_set_cut`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3a38b524",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y_scaled_ordinal_set =  model_evaluator.encode_method_ordinal(X_y_scaled_set)\n",
    "X_y_scaled_ordinal_set_cut = model_evaluator.encode_method_ordinal(X_y_scaled_set_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "37fca3ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>304194</th>\n",
       "      <th>30418</th>\n",
       "      <th>201995</th>\n",
       "      <th>10301</th>\n",
       "      <th>6331</th>\n",
       "      <th>169492</th>\n",
       "      <th>219341</th>\n",
       "      <th>90099</th>\n",
       "      <th>342637</th>\n",
       "      <th>351874</th>\n",
       "      <th>...</th>\n",
       "      <th>238533</th>\n",
       "      <th>50063</th>\n",
       "      <th>139742</th>\n",
       "      <th>158489</th>\n",
       "      <th>210291</th>\n",
       "      <th>208349</th>\n",
       "      <th>19236</th>\n",
       "      <th>30421</th>\n",
       "      <th>300668</th>\n",
       "      <th>269773</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VehicleType</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RegistrationYear</th>\n",
       "      <td>-0.439735</td>\n",
       "      <td>0.028068</td>\n",
       "      <td>1.899279</td>\n",
       "      <td>1.431476</td>\n",
       "      <td>-0.751604</td>\n",
       "      <td>0.807739</td>\n",
       "      <td>-0.439735</td>\n",
       "      <td>-0.439735</td>\n",
       "      <td>-1.375341</td>\n",
       "      <td>-0.595670</td>\n",
       "      <td>...</td>\n",
       "      <td>1.119608</td>\n",
       "      <td>0.339936</td>\n",
       "      <td>-1.375341</td>\n",
       "      <td>-1.063472</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>0.651805</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>0.339936</td>\n",
       "      <td>0.495870</td>\n",
       "      <td>-0.283801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gearbox</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power</th>\n",
       "      <td>-1.199071</td>\n",
       "      <td>-0.869488</td>\n",
       "      <td>-0.578679</td>\n",
       "      <td>0.002938</td>\n",
       "      <td>-0.869488</td>\n",
       "      <td>1.108010</td>\n",
       "      <td>-1.160296</td>\n",
       "      <td>-0.074611</td>\n",
       "      <td>-1.160296</td>\n",
       "      <td>0.584555</td>\n",
       "      <td>...</td>\n",
       "      <td>4.151806</td>\n",
       "      <td>-0.869488</td>\n",
       "      <td>-1.005198</td>\n",
       "      <td>-1.354169</td>\n",
       "      <td>-0.346032</td>\n",
       "      <td>-1.160296</td>\n",
       "      <td>1.360044</td>\n",
       "      <td>0.216197</td>\n",
       "      <td>0.177423</td>\n",
       "      <td>0.099874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <td>83.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>233.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kilometer</th>\n",
       "      <td>0.582418</td>\n",
       "      <td>-0.092563</td>\n",
       "      <td>0.582418</td>\n",
       "      <td>-2.117507</td>\n",
       "      <td>0.582418</td>\n",
       "      <td>0.582418</td>\n",
       "      <td>-0.092563</td>\n",
       "      <td>0.582418</td>\n",
       "      <td>0.582418</td>\n",
       "      <td>0.582418</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.092563</td>\n",
       "      <td>0.582418</td>\n",
       "      <td>0.582418</td>\n",
       "      <td>-0.092563</td>\n",
       "      <td>0.582418</td>\n",
       "      <td>-1.037537</td>\n",
       "      <td>0.582418</td>\n",
       "      <td>0.582418</td>\n",
       "      <td>-0.092563</td>\n",
       "      <td>0.582418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FuelType</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brand</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Repaired</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 229261 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     304194      30418       201995     10301      6331    \\\n",
       "VehicleType        5.000000    5.000000    5.000000   1.000000   1.000000   \n",
       "RegistrationYear  -0.439735    0.028068    1.899279   1.431476  -0.751604   \n",
       "Gearbox            1.000000    1.000000    1.000000   1.000000   1.000000   \n",
       "Power             -1.199071   -0.869488   -0.578679   0.002938  -0.869488   \n",
       "Model             83.000000  173.000000  166.000000   8.000000  42.000000   \n",
       "Kilometer          0.582418   -0.092563    0.582418  -2.117507   0.582418   \n",
       "FuelType           6.000000    6.000000    6.000000   6.000000   6.000000   \n",
       "Brand             24.000000   38.000000    5.000000  25.000000  24.000000   \n",
       "Repaired           0.000000    0.000000    0.000000   0.000000   0.000000   \n",
       "\n",
       "                    169492      219341      90099       342637     351874  \\\n",
       "VehicleType       4.000000    5.000000    7.000000    4.000000   7.000000   \n",
       "RegistrationYear  0.807739   -0.439735   -0.439735   -1.375341  -0.595670   \n",
       "Gearbox           1.000000    1.000000    1.000000    1.000000   1.000000   \n",
       "Power             1.108010   -1.160296   -0.074611   -1.160296   0.584555   \n",
       "Model             6.000000  128.000000  170.000000  116.000000  31.000000   \n",
       "Kilometer         0.582418   -0.092563    0.582418    0.582418   0.582418   \n",
       "FuelType          2.000000    6.000000    2.000000    6.000000   2.000000   \n",
       "Brand             2.000000   10.000000   38.000000   38.000000   1.000000   \n",
       "Repaired          0.000000    0.000000    0.000000    0.000000   0.000000   \n",
       "\n",
       "                  ...      238533      50063       139742      158489  \\\n",
       "VehicleType       ...    4.000000    5.000000    5.000000    5.000000   \n",
       "RegistrationYear  ...    1.119608    0.339936   -1.375341   -1.063472   \n",
       "Gearbox           ...    0.000000    1.000000    1.000000    1.000000   \n",
       "Power             ...    4.151806   -0.869488   -1.005198   -1.354169   \n",
       "Model             ...  171.000000  120.000000  102.000000  102.000000   \n",
       "Kilometer         ...   -0.092563    0.582418    0.582418   -0.092563   \n",
       "FuelType          ...    6.000000    2.000000    6.000000    6.000000   \n",
       "Brand             ...   38.000000   30.000000   10.000000   10.000000   \n",
       "Repaired          ...    1.000000    0.000000    0.000000    0.000000   \n",
       "\n",
       "                     210291      208349      19236       30421       300668  \\\n",
       "VehicleType        4.000000    4.000000    6.000000    0.000000    0.000000   \n",
       "RegistrationYear  -0.127867    0.651805   -0.127867    0.339936    0.495870   \n",
       "Gearbox            1.000000    1.000000    0.000000    1.000000    1.000000   \n",
       "Power             -0.346032   -1.160296    1.360044    0.216197    0.177423   \n",
       "Model             11.000000  173.000000  145.000000  223.000000  234.000000   \n",
       "Kilometer          0.582418   -1.037537    0.582418    0.582418   -0.092563   \n",
       "FuelType           6.000000    6.000000    2.000000    2.000000    6.000000   \n",
       "Brand              2.000000   38.000000   20.000000   38.000000   36.000000   \n",
       "Repaired           0.000000    1.000000    0.000000    0.000000    0.000000   \n",
       "\n",
       "                      269773  \n",
       "VehicleType         4.000000  \n",
       "RegistrationYear   -0.283801  \n",
       "Gearbox             1.000000  \n",
       "Power               0.099874  \n",
       "Model             233.000000  \n",
       "Kilometer           0.582418  \n",
       "FuelType            6.000000  \n",
       "Brand              24.000000  \n",
       "Repaired            0.000000  \n",
       "\n",
       "[9 rows x 229261 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_y_scaled_ordinal_set[0].T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c3726a",
   "metadata": {},
   "source": [
    "<a id = 'fit'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15245466",
   "metadata": {},
   "source": [
    "### Обучение алгоритмов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e653e5e",
   "metadata": {},
   "source": [
    "В обучении алгоритмов и поиске оптимальной модели будем использовать следующие выборки в паре с алгоритмами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8d3a4d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Выборки/Алгоритмы</th>\n",
       "      <th>CatBoostRegressor</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <th>Ridge</th>\n",
       "      <th>LinearRegression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X_y_set</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X_y_set_with_missing</th>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X_y_set_cut</th>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X_y_scaled_ohe_set</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X_y_scaled_ohe_set_cut</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X_y_scaled_ordinal_set</th>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X_y_scaled_ordinal_set_cut</th>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td>+</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Выборки/Алгоритмы          CatBoostRegressor LGBMRegressor  \\\n",
       "X_y_set                                                      \n",
       "X_y_set_with_missing                       +             +   \n",
       "X_y_set_cut                                +             +   \n",
       "X_y_scaled_ohe_set                                           \n",
       "X_y_scaled_ohe_set_cut                                       \n",
       "X_y_scaled_ordinal_set                     +             +   \n",
       "X_y_scaled_ordinal_set_cut                 +             +   \n",
       "\n",
       "Выборки/Алгоритмы          DecisionTreeRegressor Ridge LinearRegression  \n",
       "X_y_set                                                                  \n",
       "X_y_set_with_missing                                                     \n",
       "X_y_set_cut                                                              \n",
       "X_y_scaled_ohe_set                                   +                +  \n",
       "X_y_scaled_ohe_set_cut                               +                +  \n",
       "X_y_scaled_ordinal_set                         +                         \n",
       "X_y_scaled_ordinal_set_cut                     +                         "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создаем пустой датафрейм\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Добавляем столбцы\n",
    "df['Выборки/Алгоритмы'] = ['CatBoostRegressor', 'LGBMRegressor', 'DecisionTreeRegressor', 'Ridge', 'LinearRegression']\n",
    "df['X_y_set'] = [' ', ' ', ' ', ' ', ' ']\n",
    "df['X_y_set_with_missing'] = ['+', '+', ' ', ' ', ' ']\n",
    "df['X_y_set_cut'] = ['+', '+', ' ', ' ', ' ']\n",
    "df['X_y_scaled_ohe_set'] = [' ', ' ', ' ', '+', '+']\n",
    "df['X_y_scaled_ohe_set_cut'] = [' ', ' ', ' ', '+', '+']\n",
    "df['X_y_scaled_ordinal_set'] = ['+', '+', '+', ' ', ' ']\n",
    "df['X_y_scaled_ordinal_set_cut'] = ['+', '+', '+', ' ', ' ']\n",
    "df = df.set_index('Выборки/Алгоритмы')\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5b865314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем накопительную таблицу для сбора данных по всем лучшим моделям алгоритмов\n",
    "tab_metrics = pd.DataFrame(columns = ['Алгоритм модели', 'Набор данных', 'RMSE_fit', 'Время обучения', 'Время предсказания'])\n",
    "\n",
    "# создаеем массив лучших моделей\n",
    "best_model_fit = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1d6f744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем объекты классов алгоритмов машинного обучения\n",
    "cat_boost_regressor = CatBoostRegressor(random_state = random_seed, verbose = 10, iterations = 40)\n",
    "lgbm_regressor = LGBMRegressor(n_estimators = 40, random_state = random_seed, extra_trees = True)\n",
    "# этот объект создается для работы с закодированными признаками\n",
    "lgbm_regressor_cod = LGBMRegressor(n_estimators = 40, random_state = random_seed)\n",
    "linear_regression = LinearRegression()\n",
    "ridge = Ridge(random_state = random_seed, solver = 'sparse_cg')\n",
    "decision_tree_regressor = DecisionTreeRegressor(random_state = random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9c77b34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание словаря выборок для алгоритмов CatBoostRegressor\n",
    "datasets1 = {'X_y_set_cut': X_y_set_cut}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "82866d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 3042.3770134\ttotal: 172ms\tremaining: 6.69s\n",
      "10:\tlearn: 1986.5532000\ttotal: 368ms\tremaining: 969ms\n",
      "20:\tlearn: 1674.2764897\ttotal: 520ms\tremaining: 470ms\n",
      "30:\tlearn: 1549.3815348\ttotal: 642ms\tremaining: 186ms\n",
      "39:\tlearn: 1490.2433186\ttotal: 765ms\tremaining: 0us\n",
      "0:\tlearn: 3038.6908385\ttotal: 15.5ms\tremaining: 605ms\n",
      "10:\tlearn: 1978.0433441\ttotal: 153ms\tremaining: 403ms\n",
      "20:\tlearn: 1666.3866834\ttotal: 306ms\tremaining: 277ms\n",
      "30:\tlearn: 1540.4190851\ttotal: 446ms\tremaining: 129ms\n",
      "39:\tlearn: 1479.8675175\ttotal: 555ms\tremaining: 0us\n",
      "0:\tlearn: 3034.6384565\ttotal: 18.9ms\tremaining: 736ms\n",
      "10:\tlearn: 1978.6605998\ttotal: 151ms\tremaining: 399ms\n",
      "20:\tlearn: 1665.8227808\ttotal: 291ms\tremaining: 264ms\n",
      "30:\tlearn: 1538.1541678\ttotal: 417ms\tremaining: 121ms\n",
      "39:\tlearn: 1481.7475834\ttotal: 550ms\tremaining: 0us\n",
      "0:\tlearn: 3040.9989813\ttotal: 15.6ms\tremaining: 609ms\n",
      "10:\tlearn: 1987.1463870\ttotal: 155ms\tremaining: 409ms\n",
      "20:\tlearn: 1666.6506457\ttotal: 286ms\tremaining: 259ms\n",
      "30:\tlearn: 1538.8933108\ttotal: 428ms\tremaining: 124ms\n",
      "39:\tlearn: 1478.0433538\ttotal: 543ms\tremaining: 0us\n",
      "0:\tlearn: 3041.1273392\ttotal: 23.2ms\tremaining: 906ms\n",
      "10:\tlearn: 1976.5002887\ttotal: 175ms\tremaining: 461ms\n",
      "20:\tlearn: 1663.0353787\ttotal: 314ms\tremaining: 284ms\n",
      "30:\tlearn: 1536.5890164\ttotal: 445ms\tremaining: 129ms\n",
      "39:\tlearn: 1478.6808610\ttotal: 568ms\tremaining: 0us\n",
      "0:\tlearn: 2363.6259754\ttotal: 15.4ms\tremaining: 602ms\n",
      "10:\tlearn: 1437.7185845\ttotal: 151ms\tremaining: 399ms\n",
      "20:\tlearn: 1369.5806384\ttotal: 315ms\tremaining: 285ms\n",
      "30:\tlearn: 1336.5797082\ttotal: 467ms\tremaining: 136ms\n",
      "39:\tlearn: 1316.4476385\ttotal: 587ms\tremaining: 0us\n",
      "0:\tlearn: 2356.5734000\ttotal: 18.8ms\tremaining: 732ms\n",
      "10:\tlearn: 1448.2036880\ttotal: 149ms\tremaining: 394ms\n",
      "20:\tlearn: 1369.6429354\ttotal: 290ms\tremaining: 263ms\n",
      "30:\tlearn: 1338.1061781\ttotal: 422ms\tremaining: 123ms\n",
      "39:\tlearn: 1314.4841236\ttotal: 560ms\tremaining: 0us\n",
      "0:\tlearn: 2351.9416508\ttotal: 15.5ms\tremaining: 606ms\n",
      "10:\tlearn: 1432.5568689\ttotal: 152ms\tremaining: 401ms\n",
      "20:\tlearn: 1365.0253971\ttotal: 281ms\tremaining: 254ms\n",
      "30:\tlearn: 1334.6896569\ttotal: 418ms\tremaining: 121ms\n",
      "39:\tlearn: 1312.8635626\ttotal: 535ms\tremaining: 0us\n",
      "0:\tlearn: 2379.7029116\ttotal: 20.8ms\tremaining: 812ms\n",
      "10:\tlearn: 1433.7989897\ttotal: 157ms\tremaining: 414ms\n",
      "20:\tlearn: 1367.2330998\ttotal: 306ms\tremaining: 277ms\n",
      "30:\tlearn: 1329.4751279\ttotal: 442ms\tremaining: 128ms\n",
      "39:\tlearn: 1309.3435427\ttotal: 569ms\tremaining: 0us\n",
      "0:\tlearn: 2360.4456598\ttotal: 15.8ms\tremaining: 615ms\n",
      "10:\tlearn: 1433.9867689\ttotal: 152ms\tremaining: 401ms\n",
      "20:\tlearn: 1367.8438559\ttotal: 287ms\tremaining: 260ms\n",
      "30:\tlearn: 1333.4601328\ttotal: 438ms\tremaining: 127ms\n",
      "39:\tlearn: 1312.7821901\ttotal: 556ms\tremaining: 0us\n",
      "0:\tlearn: 3028.5838836\ttotal: 19.9ms\tremaining: 774ms\n",
      "10:\tlearn: 1907.4745723\ttotal: 192ms\tremaining: 506ms\n",
      "20:\tlearn: 1593.9501388\ttotal: 374ms\tremaining: 339ms\n",
      "30:\tlearn: 1474.2111095\ttotal: 550ms\tremaining: 160ms\n",
      "39:\tlearn: 1422.2475489\ttotal: 716ms\tremaining: 0us\n",
      "0:\tlearn: 3025.9887829\ttotal: 20.1ms\tremaining: 784ms\n",
      "10:\tlearn: 1899.2445129\ttotal: 202ms\tremaining: 533ms\n",
      "20:\tlearn: 1582.4074407\ttotal: 376ms\tremaining: 340ms\n",
      "30:\tlearn: 1466.9683514\ttotal: 561ms\tremaining: 163ms\n",
      "39:\tlearn: 1416.7957320\ttotal: 731ms\tremaining: 0us\n",
      "0:\tlearn: 3021.7836618\ttotal: 20.6ms\tremaining: 803ms\n",
      "10:\tlearn: 1903.8739066\ttotal: 194ms\tremaining: 512ms\n",
      "20:\tlearn: 1581.8448374\ttotal: 412ms\tremaining: 373ms\n",
      "30:\tlearn: 1468.0076051\ttotal: 578ms\tremaining: 168ms\n",
      "39:\tlearn: 1419.3545002\ttotal: 749ms\tremaining: 0us\n",
      "0:\tlearn: 3023.1854178\ttotal: 19.4ms\tremaining: 756ms\n",
      "10:\tlearn: 1901.9295039\ttotal: 204ms\tremaining: 539ms\n",
      "20:\tlearn: 1587.7513737\ttotal: 377ms\tremaining: 341ms\n",
      "30:\tlearn: 1472.3562002\ttotal: 556ms\tremaining: 161ms\n",
      "39:\tlearn: 1417.1727797\ttotal: 710ms\tremaining: 0us\n",
      "0:\tlearn: 3027.2649374\ttotal: 21.5ms\tremaining: 840ms\n",
      "10:\tlearn: 1902.3994563\ttotal: 194ms\tremaining: 511ms\n",
      "20:\tlearn: 1584.3308930\ttotal: 377ms\tremaining: 341ms\n",
      "30:\tlearn: 1465.4931036\ttotal: 545ms\tremaining: 158ms\n",
      "39:\tlearn: 1416.5854908\ttotal: 715ms\tremaining: 0us\n",
      "0:\tlearn: 2292.4049529\ttotal: 20ms\tremaining: 781ms\n",
      "10:\tlearn: 1385.6256234\ttotal: 214ms\tremaining: 565ms\n",
      "20:\tlearn: 1326.4840668\ttotal: 391ms\tremaining: 354ms\n",
      "30:\tlearn: 1294.5480582\ttotal: 576ms\tremaining: 167ms\n",
      "39:\tlearn: 1272.5097665\ttotal: 737ms\tremaining: 0us\n",
      "0:\tlearn: 2290.9550695\ttotal: 18.4ms\tremaining: 718ms\n",
      "10:\tlearn: 1383.9154057\ttotal: 202ms\tremaining: 534ms\n",
      "20:\tlearn: 1314.5316619\ttotal: 388ms\tremaining: 351ms\n",
      "30:\tlearn: 1284.1014365\ttotal: 563ms\tremaining: 163ms\n",
      "39:\tlearn: 1263.5379024\ttotal: 731ms\tremaining: 0us\n",
      "0:\tlearn: 2285.5191181\ttotal: 21.5ms\tremaining: 837ms\n",
      "10:\tlearn: 1382.6672117\ttotal: 216ms\tremaining: 570ms\n",
      "20:\tlearn: 1315.6739742\ttotal: 392ms\tremaining: 355ms\n",
      "30:\tlearn: 1285.8279859\ttotal: 577ms\tremaining: 167ms\n",
      "39:\tlearn: 1266.9532095\ttotal: 736ms\tremaining: 0us\n",
      "0:\tlearn: 2288.0431880\ttotal: 19.6ms\tremaining: 766ms\n",
      "10:\tlearn: 1374.3394195\ttotal: 196ms\tremaining: 517ms\n",
      "20:\tlearn: 1318.9160630\ttotal: 422ms\tremaining: 382ms\n",
      "30:\tlearn: 1289.1946360\ttotal: 598ms\tremaining: 173ms\n",
      "39:\tlearn: 1266.0946588\ttotal: 766ms\tremaining: 0us\n",
      "0:\tlearn: 2288.7908952\ttotal: 19.7ms\tremaining: 767ms\n",
      "10:\tlearn: 1382.4760100\ttotal: 196ms\tremaining: 517ms\n",
      "20:\tlearn: 1317.4433680\ttotal: 380ms\tremaining: 344ms\n",
      "30:\tlearn: 1284.9366182\ttotal: 577ms\tremaining: 168ms\n",
      "39:\tlearn: 1264.6397376\ttotal: 734ms\tremaining: 0us\n",
      "0:\tlearn: 3017.5999328\ttotal: 33.7ms\tremaining: 1.31s\n",
      "10:\tlearn: 1847.6231910\ttotal: 362ms\tremaining: 955ms\n",
      "20:\tlearn: 1523.9244309\ttotal: 699ms\tremaining: 632ms\n",
      "30:\tlearn: 1412.6932185\ttotal: 1.03s\tremaining: 298ms\n",
      "39:\tlearn: 1369.1028476\ttotal: 1.32s\tremaining: 0us\n",
      "0:\tlearn: 3016.3356926\ttotal: 33.8ms\tremaining: 1.32s\n",
      "10:\tlearn: 1846.3670248\ttotal: 372ms\tremaining: 981ms\n",
      "20:\tlearn: 1523.4458021\ttotal: 699ms\tremaining: 633ms\n",
      "30:\tlearn: 1417.7127545\ttotal: 1.03s\tremaining: 300ms\n",
      "39:\tlearn: 1367.3444219\ttotal: 1.33s\tremaining: 0us\n",
      "0:\tlearn: 3011.4645491\ttotal: 35ms\tremaining: 1.36s\n",
      "10:\tlearn: 1842.8700481\ttotal: 361ms\tremaining: 952ms\n",
      "20:\tlearn: 1523.0276290\ttotal: 713ms\tremaining: 645ms\n",
      "30:\tlearn: 1413.7975632\ttotal: 1.06s\tremaining: 308ms\n",
      "39:\tlearn: 1364.6325441\ttotal: 1.35s\tremaining: 0us\n",
      "0:\tlearn: 3011.4885331\ttotal: 34.7ms\tremaining: 1.35s\n",
      "10:\tlearn: 1845.8202332\ttotal: 367ms\tremaining: 967ms\n",
      "20:\tlearn: 1520.8218887\ttotal: 700ms\tremaining: 633ms\n",
      "30:\tlearn: 1410.4886568\ttotal: 1.03s\tremaining: 301ms\n",
      "39:\tlearn: 1364.6915148\ttotal: 1.33s\tremaining: 0us\n",
      "0:\tlearn: 3016.5169740\ttotal: 35.1ms\tremaining: 1.37s\n",
      "10:\tlearn: 1845.1680019\ttotal: 418ms\tremaining: 1.1s\n",
      "20:\tlearn: 1521.5108158\ttotal: 749ms\tremaining: 678ms\n",
      "30:\tlearn: 1411.8010002\ttotal: 1.08s\tremaining: 313ms\n",
      "39:\tlearn: 1361.5828505\ttotal: 1.39s\tremaining: 0us\n",
      "0:\tlearn: 2233.5074793\ttotal: 35.8ms\tremaining: 1.4s\n",
      "10:\tlearn: 1337.4044383\ttotal: 361ms\tremaining: 952ms\n",
      "20:\tlearn: 1281.8142302\ttotal: 696ms\tremaining: 630ms\n",
      "30:\tlearn: 1249.3215745\ttotal: 1.03s\tremaining: 300ms\n",
      "39:\tlearn: 1226.8997994\ttotal: 1.35s\tremaining: 0us\n",
      "0:\tlearn: 2238.8774202\ttotal: 33.7ms\tremaining: 1.31s\n",
      "10:\tlearn: 1335.5785709\ttotal: 363ms\tremaining: 957ms\n",
      "20:\tlearn: 1275.8373260\ttotal: 709ms\tremaining: 641ms\n",
      "30:\tlearn: 1245.3960290\ttotal: 1.04s\tremaining: 302ms\n",
      "39:\tlearn: 1225.1838480\ttotal: 1.34s\tremaining: 0us\n",
      "0:\tlearn: 2229.9616956\ttotal: 41.7ms\tremaining: 1.63s\n",
      "10:\tlearn: 1337.1489555\ttotal: 400ms\tremaining: 1.05s\n",
      "20:\tlearn: 1277.1266579\ttotal: 735ms\tremaining: 665ms\n",
      "30:\tlearn: 1245.1729561\ttotal: 1.06s\tremaining: 308ms\n",
      "39:\tlearn: 1221.5353540\ttotal: 1.37s\tremaining: 0us\n",
      "0:\tlearn: 2225.5028542\ttotal: 34.4ms\tremaining: 1.34s\n",
      "10:\tlearn: 1338.1205817\ttotal: 375ms\tremaining: 988ms\n",
      "20:\tlearn: 1276.8529739\ttotal: 714ms\tremaining: 646ms\n",
      "30:\tlearn: 1244.6747023\ttotal: 1.03s\tremaining: 298ms\n",
      "39:\tlearn: 1227.6079663\ttotal: 1.32s\tremaining: 0us\n",
      "0:\tlearn: 2231.0885325\ttotal: 34.9ms\tremaining: 1.36s\n",
      "10:\tlearn: 1331.8023182\ttotal: 368ms\tremaining: 971ms\n",
      "20:\tlearn: 1270.4101193\ttotal: 730ms\tremaining: 661ms\n",
      "30:\tlearn: 1238.0800073\ttotal: 1.08s\tremaining: 313ms\n",
      "39:\tlearn: 1216.5738498\ttotal: 1.38s\tremaining: 0us\n",
      "0:\tlearn: 2244.2875444\ttotal: 41.6ms\tremaining: 1.62s\n",
      "10:\tlearn: 1331.0504905\ttotal: 441ms\tremaining: 1.16s\n",
      "20:\tlearn: 1283.0358716\ttotal: 840ms\tremaining: 760ms\n",
      "30:\tlearn: 1248.5288871\ttotal: 1.24s\tremaining: 360ms\n",
      "39:\tlearn: 1226.3254068\ttotal: 1.6s\tremaining: 0us\n",
      "0:\tlearn: 2244.2875444\ttotal: 41.5ms\tremaining: 1.62s\n",
      "10:\tlearn: 1331.0504905\ttotal: 437ms\tremaining: 1.15s\n",
      "20:\tlearn: 1283.0358716\ttotal: 835ms\tremaining: 756ms\n",
      "30:\tlearn: 1248.5288871\ttotal: 1.24s\tremaining: 360ms\n",
      "39:\tlearn: 1226.3254068\ttotal: 1.59s\tremaining: 0us\n",
      "Параметры лучшей модели алгоритма <catboost.core.CatBoostRegressor object at 0x0000022C288A7C70> {'depth': 10, 'learning_rate': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Создание словаря параметров алгоритма CatBoostRegressor под выборки из datasets1\n",
    "params_cat_boost_regressor = {'learning_rate': [0.1, 0.5], 'depth': [6, 8, 10]}\n",
    "tab_metrics, best_model_fit = model_evaluator.evaluate_models(datasets1, cat_boost_regressor, params_cat_boost_regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cf569d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y_set_with_missing[0][cat_features] = X_y_set_with_missing[0][cat_features].astype(str)\n",
    "X_y_set_with_missing[2][cat_features] = X_y_set_with_missing[2][cat_features].astype(str)\n",
    "\n",
    "# Создание словаря выборок для алгоритмов CatBoostRegressor\n",
    "datasets1_1 = {'X_y_set_with_missing': X_y_set_with_missing}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2a6c43ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 4238.8591939\ttotal: 103ms\tremaining: 4.02s\n",
      "10:\tlearn: 2760.7658368\ttotal: 1.01s\tremaining: 2.67s\n",
      "20:\tlearn: 2312.6026670\ttotal: 1.86s\tremaining: 1.69s\n",
      "30:\tlearn: 2140.1983027\ttotal: 2.73s\tremaining: 793ms\n",
      "39:\tlearn: 2077.3794669\ttotal: 3.5s\tremaining: 0us\n",
      "0:\tlearn: 4247.5121565\ttotal: 119ms\tremaining: 4.63s\n",
      "10:\tlearn: 2770.5656984\ttotal: 1.07s\tremaining: 2.81s\n",
      "20:\tlearn: 2315.0071591\ttotal: 2.01s\tremaining: 1.82s\n",
      "30:\tlearn: 2146.8845406\ttotal: 2.92s\tremaining: 847ms\n",
      "39:\tlearn: 2079.4064147\ttotal: 3.74s\tremaining: 0us\n",
      "0:\tlearn: 4238.7924072\ttotal: 101ms\tremaining: 3.94s\n",
      "10:\tlearn: 2764.9677471\ttotal: 1.05s\tremaining: 2.78s\n",
      "20:\tlearn: 2312.4694266\ttotal: 1.95s\tremaining: 1.76s\n",
      "30:\tlearn: 2143.3149697\ttotal: 2.85s\tremaining: 826ms\n",
      "39:\tlearn: 2075.5620282\ttotal: 3.74s\tremaining: 0us\n",
      "0:\tlearn: 4238.0260263\ttotal: 107ms\tremaining: 4.18s\n",
      "10:\tlearn: 2761.3209546\ttotal: 1.08s\tremaining: 2.84s\n",
      "20:\tlearn: 2305.7817197\ttotal: 2.01s\tremaining: 1.82s\n",
      "30:\tlearn: 2141.7428737\ttotal: 2.9s\tremaining: 841ms\n",
      "39:\tlearn: 2073.9548297\ttotal: 3.71s\tremaining: 0us\n",
      "0:\tlearn: 4238.6052760\ttotal: 106ms\tremaining: 4.14s\n",
      "10:\tlearn: 2756.6349267\ttotal: 1.08s\tremaining: 2.84s\n",
      "20:\tlearn: 2301.1031334\ttotal: 2.01s\tremaining: 1.81s\n",
      "30:\tlearn: 2136.8045918\ttotal: 2.92s\tremaining: 849ms\n",
      "39:\tlearn: 2070.8338047\ttotal: 3.74s\tremaining: 0us\n",
      "0:\tlearn: 3281.7806016\ttotal: 99.6ms\tremaining: 3.88s\n",
      "10:\tlearn: 2048.7446561\ttotal: 1.07s\tremaining: 2.82s\n",
      "20:\tlearn: 1958.1968538\ttotal: 2s\tremaining: 1.81s\n",
      "30:\tlearn: 1910.8637586\ttotal: 2.91s\tremaining: 845ms\n",
      "39:\tlearn: 1879.6850065\ttotal: 3.74s\tremaining: 0us\n",
      "0:\tlearn: 3283.9973097\ttotal: 105ms\tremaining: 4.08s\n",
      "10:\tlearn: 2047.7885007\ttotal: 1.06s\tremaining: 2.8s\n",
      "20:\tlearn: 1966.4242244\ttotal: 2s\tremaining: 1.81s\n",
      "30:\tlearn: 1905.5089845\ttotal: 2.93s\tremaining: 850ms\n",
      "39:\tlearn: 1872.5304248\ttotal: 3.77s\tremaining: 0us\n",
      "0:\tlearn: 3291.2774018\ttotal: 104ms\tremaining: 4.07s\n",
      "10:\tlearn: 2050.5426558\ttotal: 1.1s\tremaining: 2.89s\n",
      "20:\tlearn: 1962.0866558\ttotal: 2.06s\tremaining: 1.87s\n",
      "30:\tlearn: 1911.3202207\ttotal: 3.02s\tremaining: 877ms\n",
      "39:\tlearn: 1879.2610674\ttotal: 3.86s\tremaining: 0us\n",
      "0:\tlearn: 3276.5389729\ttotal: 107ms\tremaining: 4.17s\n",
      "10:\tlearn: 2048.0953618\ttotal: 1.09s\tremaining: 2.88s\n",
      "20:\tlearn: 1961.6233316\ttotal: 2.07s\tremaining: 1.87s\n",
      "30:\tlearn: 1908.6375510\ttotal: 3.03s\tremaining: 880ms\n",
      "39:\tlearn: 1881.3311094\ttotal: 3.88s\tremaining: 0us\n",
      "0:\tlearn: 3313.2087812\ttotal: 109ms\tremaining: 4.23s\n",
      "10:\tlearn: 2046.0170600\ttotal: 1.09s\tremaining: 2.88s\n",
      "20:\tlearn: 1947.0665410\ttotal: 2.08s\tremaining: 1.88s\n",
      "30:\tlearn: 1905.0913577\ttotal: 3.05s\tremaining: 885ms\n",
      "39:\tlearn: 1874.6154365\ttotal: 3.91s\tremaining: 0us\n",
      "0:\tlearn: 4220.7955152\ttotal: 134ms\tremaining: 5.22s\n",
      "10:\tlearn: 2672.6882126\ttotal: 1.4s\tremaining: 3.68s\n",
      "20:\tlearn: 2215.8051146\ttotal: 2.67s\tremaining: 2.42s\n",
      "30:\tlearn: 2052.5287192\ttotal: 3.94s\tremaining: 1.14s\n",
      "39:\tlearn: 1990.5991456\ttotal: 5.07s\tremaining: 0us\n",
      "0:\tlearn: 4230.3963247\ttotal: 131ms\tremaining: 5.09s\n",
      "10:\tlearn: 2676.0289259\ttotal: 1.43s\tremaining: 3.78s\n",
      "20:\tlearn: 2217.8754277\ttotal: 2.72s\tremaining: 2.46s\n",
      "30:\tlearn: 2054.2808788\ttotal: 3.98s\tremaining: 1.16s\n",
      "39:\tlearn: 1992.2548756\ttotal: 5.1s\tremaining: 0us\n",
      "0:\tlearn: 4221.9904231\ttotal: 130ms\tremaining: 5.05s\n",
      "10:\tlearn: 2670.3827572\ttotal: 1.43s\tremaining: 3.77s\n",
      "20:\tlearn: 2220.4952120\ttotal: 2.72s\tremaining: 2.46s\n",
      "30:\tlearn: 2053.8468926\ttotal: 3.99s\tremaining: 1.16s\n",
      "39:\tlearn: 1995.3324679\ttotal: 5.17s\tremaining: 0us\n",
      "0:\tlearn: 4223.5853238\ttotal: 132ms\tremaining: 5.13s\n",
      "10:\tlearn: 2670.8144922\ttotal: 1.63s\tremaining: 4.31s\n",
      "20:\tlearn: 2212.6481425\ttotal: 3s\tremaining: 2.72s\n",
      "30:\tlearn: 2048.3859382\ttotal: 4.32s\tremaining: 1.25s\n",
      "39:\tlearn: 1991.1012183\ttotal: 5.49s\tremaining: 0us\n",
      "0:\tlearn: 4216.8588499\ttotal: 133ms\tremaining: 5.17s\n",
      "10:\tlearn: 2671.8030024\ttotal: 1.43s\tremaining: 3.78s\n",
      "20:\tlearn: 2217.1990225\ttotal: 2.75s\tremaining: 2.49s\n",
      "30:\tlearn: 2049.8793525\ttotal: 4.03s\tremaining: 1.17s\n",
      "39:\tlearn: 1991.8570408\ttotal: 5.22s\tremaining: 0us\n",
      "0:\tlearn: 3188.4990673\ttotal: 128ms\tremaining: 5.01s\n",
      "10:\tlearn: 1969.1451421\ttotal: 1.51s\tremaining: 3.98s\n",
      "20:\tlearn: 1881.8123564\ttotal: 2.83s\tremaining: 2.56s\n",
      "30:\tlearn: 1828.2882053\ttotal: 4.14s\tremaining: 1.2s\n",
      "39:\tlearn: 1794.0769489\ttotal: 5.4s\tremaining: 0us\n",
      "0:\tlearn: 3195.5358152\ttotal: 131ms\tremaining: 5.11s\n",
      "10:\tlearn: 1966.2564443\ttotal: 1.49s\tremaining: 3.93s\n",
      "20:\tlearn: 1878.1128760\ttotal: 2.79s\tremaining: 2.53s\n",
      "30:\tlearn: 1822.2265485\ttotal: 4.11s\tremaining: 1.19s\n",
      "39:\tlearn: 1787.9506830\ttotal: 5.33s\tremaining: 0us\n",
      "0:\tlearn: 3204.8351862\ttotal: 131ms\tremaining: 5.1s\n",
      "10:\tlearn: 1967.1088189\ttotal: 1.51s\tremaining: 3.99s\n",
      "20:\tlearn: 1879.0951911\ttotal: 2.85s\tremaining: 2.58s\n",
      "30:\tlearn: 1826.7743033\ttotal: 4.21s\tremaining: 1.22s\n",
      "39:\tlearn: 1793.6778294\ttotal: 5.41s\tremaining: 0us\n",
      "0:\tlearn: 3201.8173414\ttotal: 135ms\tremaining: 5.28s\n",
      "10:\tlearn: 1974.3649198\ttotal: 1.48s\tremaining: 3.91s\n",
      "20:\tlearn: 1894.4636577\ttotal: 2.85s\tremaining: 2.58s\n",
      "30:\tlearn: 1837.3039090\ttotal: 4.19s\tremaining: 1.22s\n",
      "39:\tlearn: 1804.0459732\ttotal: 5.43s\tremaining: 0us\n",
      "0:\tlearn: 3201.5254948\ttotal: 125ms\tremaining: 4.86s\n",
      "10:\tlearn: 1966.7760644\ttotal: 1.51s\tremaining: 3.99s\n",
      "20:\tlearn: 1878.0670231\ttotal: 2.85s\tremaining: 2.58s\n",
      "30:\tlearn: 1833.0921147\ttotal: 4.19s\tremaining: 1.22s\n",
      "39:\tlearn: 1802.8882263\ttotal: 5.41s\tremaining: 0us\n",
      "0:\tlearn: 4208.7036121\ttotal: 193ms\tremaining: 7.53s\n",
      "10:\tlearn: 2617.3066214\ttotal: 1.91s\tremaining: 5.03s\n",
      "20:\tlearn: 2137.0116301\ttotal: 3.57s\tremaining: 3.23s\n",
      "30:\tlearn: 1978.2517319\ttotal: 5.3s\tremaining: 1.54s\n",
      "39:\tlearn: 1917.7818131\ttotal: 6.83s\tremaining: 0us\n",
      "0:\tlearn: 4222.7906296\ttotal: 155ms\tremaining: 6.04s\n",
      "10:\tlearn: 2621.8542030\ttotal: 1.95s\tremaining: 5.13s\n",
      "20:\tlearn: 2138.7338502\ttotal: 3.6s\tremaining: 3.26s\n",
      "30:\tlearn: 1975.0691609\ttotal: 5.3s\tremaining: 1.54s\n",
      "39:\tlearn: 1917.3822545\ttotal: 6.88s\tremaining: 0us\n",
      "0:\tlearn: 4211.7848419\ttotal: 157ms\tremaining: 6.12s\n",
      "10:\tlearn: 2618.1557414\ttotal: 1.93s\tremaining: 5.08s\n",
      "20:\tlearn: 2146.8897809\ttotal: 3.77s\tremaining: 3.42s\n",
      "30:\tlearn: 1988.7983689\ttotal: 5.7s\tremaining: 1.65s\n",
      "39:\tlearn: 1927.6373155\ttotal: 7.37s\tremaining: 0us\n",
      "0:\tlearn: 4213.9839971\ttotal: 179ms\tremaining: 6.98s\n",
      "10:\tlearn: 2613.5075441\ttotal: 2.01s\tremaining: 5.31s\n",
      "20:\tlearn: 2141.0731547\ttotal: 3.81s\tremaining: 3.44s\n",
      "30:\tlearn: 1979.9748525\ttotal: 5.76s\tremaining: 1.67s\n",
      "39:\tlearn: 1923.5460046\ttotal: 7.56s\tremaining: 0us\n",
      "0:\tlearn: 4207.3498196\ttotal: 219ms\tremaining: 8.55s\n",
      "10:\tlearn: 2616.8257528\ttotal: 2.23s\tremaining: 5.87s\n",
      "20:\tlearn: 2140.2145850\ttotal: 3.97s\tremaining: 3.6s\n",
      "30:\tlearn: 1977.7856292\ttotal: 5.75s\tremaining: 1.67s\n",
      "39:\tlearn: 1920.5409370\ttotal: 7.36s\tremaining: 0us\n",
      "0:\tlearn: 3124.4243410\ttotal: 178ms\tremaining: 6.95s\n",
      "10:\tlearn: 1895.3122441\ttotal: 2.01s\tremaining: 5.29s\n",
      "20:\tlearn: 1812.6765160\ttotal: 3.8s\tremaining: 3.44s\n",
      "30:\tlearn: 1754.7672478\ttotal: 5.76s\tremaining: 1.67s\n",
      "39:\tlearn: 1718.0604837\ttotal: 7.42s\tremaining: 0us\n",
      "0:\tlearn: 3155.4327298\ttotal: 168ms\tremaining: 6.54s\n",
      "10:\tlearn: 1889.8277186\ttotal: 2s\tremaining: 5.26s\n",
      "20:\tlearn: 1798.9099366\ttotal: 3.78s\tremaining: 3.42s\n",
      "30:\tlearn: 1746.1826916\ttotal: 5.56s\tremaining: 1.61s\n",
      "39:\tlearn: 1708.2051405\ttotal: 7.17s\tremaining: 0us\n",
      "0:\tlearn: 3151.2091353\ttotal: 169ms\tremaining: 6.58s\n",
      "10:\tlearn: 1908.7347483\ttotal: 2.04s\tremaining: 5.38s\n",
      "20:\tlearn: 1809.1355048\ttotal: 3.84s\tremaining: 3.48s\n",
      "30:\tlearn: 1754.9025045\ttotal: 5.65s\tremaining: 1.64s\n",
      "39:\tlearn: 1715.0820863\ttotal: 7.27s\tremaining: 0us\n",
      "0:\tlearn: 3151.2471647\ttotal: 156ms\tremaining: 6.07s\n",
      "10:\tlearn: 1898.4491567\ttotal: 1.96s\tremaining: 5.16s\n",
      "20:\tlearn: 1795.7690110\ttotal: 3.78s\tremaining: 3.42s\n",
      "30:\tlearn: 1738.3658186\ttotal: 5.56s\tremaining: 1.61s\n",
      "39:\tlearn: 1703.5456882\ttotal: 7.13s\tremaining: 0us\n",
      "0:\tlearn: 3151.3558158\ttotal: 164ms\tremaining: 6.39s\n",
      "10:\tlearn: 1890.0829183\ttotal: 2.04s\tremaining: 5.38s\n",
      "20:\tlearn: 1805.8142618\ttotal: 3.81s\tremaining: 3.45s\n",
      "30:\tlearn: 1742.6206541\ttotal: 5.69s\tremaining: 1.65s\n",
      "39:\tlearn: 1700.2853876\ttotal: 7.55s\tremaining: 0us\n",
      "0:\tlearn: 3138.4519294\ttotal: 247ms\tremaining: 9.62s\n",
      "10:\tlearn: 1894.3145768\ttotal: 2.61s\tremaining: 6.89s\n",
      "20:\tlearn: 1802.3590596\ttotal: 4.96s\tremaining: 4.48s\n",
      "30:\tlearn: 1748.9776484\ttotal: 7.22s\tremaining: 2.1s\n",
      "39:\tlearn: 1712.2859878\ttotal: 9.37s\tremaining: 0us\n",
      "0:\tlearn: 3138.4519294\ttotal: 276ms\tremaining: 10.8s\n",
      "10:\tlearn: 1894.3145768\ttotal: 2.52s\tremaining: 6.63s\n",
      "20:\tlearn: 1802.3590596\ttotal: 4.67s\tremaining: 4.23s\n",
      "30:\tlearn: 1748.9776484\ttotal: 6.79s\tremaining: 1.97s\n",
      "39:\tlearn: 1712.2859878\ttotal: 8.79s\tremaining: 0us\n",
      "Параметры лучшей модели алгоритма <catboost.core.CatBoostRegressor object at 0x0000022C288A7C70> {'cat_features': ['VehicleType', 'Gearbox', 'Model', 'FuelType', 'Brand', 'Repaired'], 'depth': 10, 'learning_rate': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Создание словаря параметров алгоритма CatBoostRegressor под выборки из datasets1_1\n",
    "params_cat_boost_regressor = {'learning_rate': [0.1, 0.5], 'depth': [6, 8, 10], 'cat_features': [cat_features]}\n",
    "tab_metrics, best_model_fit = model_evaluator.evaluate_models(datasets1_1, cat_boost_regressor, params_cat_boost_regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9ea8f9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Алгоритм модели</th>\n",
       "      <th>Набор данных</th>\n",
       "      <th>RMSE_fit</th>\n",
       "      <th>Время обучения</th>\n",
       "      <th>Время предсказания</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;catboost.core.CatBoostRegressor object at 0x0...</td>\n",
       "      <td>X_y_set_cut</td>\n",
       "      <td>1282.51</td>\n",
       "      <td>1.696325</td>\n",
       "      <td>0.031263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;catboost.core.CatBoostRegressor object at 0x0...</td>\n",
       "      <td>X_y_set_with_missing</td>\n",
       "      <td>1817.00</td>\n",
       "      <td>9.865913</td>\n",
       "      <td>0.858024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Алгоритм модели          Набор данных  \\\n",
       "0  <catboost.core.CatBoostRegressor object at 0x0...           X_y_set_cut   \n",
       "1  <catboost.core.CatBoostRegressor object at 0x0...  X_y_set_with_missing   \n",
       "\n",
       "   RMSE_fit  Время обучения  Время предсказания  \n",
       "0   1282.51        1.696325            0.031263  \n",
       "1   1817.00        9.865913            0.858024  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "60f7c7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание словаря выборок для алгоритмов LGBMRegressor\n",
    "datasets1_2 = {'X_y_set_cut': X_y_set_cut}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6d4fd760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006171 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 538\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3787.324610\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005190 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 534\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3780.430216\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004133 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 538\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3781.008420\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004554 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 537\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3780.791297\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006568 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 535\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3784.460031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004302 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 538\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3787.324610\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006269 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 534\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3780.430216\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004929 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 538\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3781.008420\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004560 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 537\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3780.791297\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005339 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 535\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3784.460031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005055 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 538\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3787.324610\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 534\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3780.430216\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004493 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 538\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3781.008420\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004171 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 537\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3780.791297\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004420 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 535\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3784.460031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005945 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 538\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3787.324610\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015345 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 534\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3780.430216\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005749 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 538\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3781.008420\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004603 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 537\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3780.791297\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005279 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 535\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3784.460031\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 538\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3787.324610\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004007 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 534\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3780.430216\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005745 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 538\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3781.008420\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006382 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 537\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3780.791297\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006399 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 535\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3784.460031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007519 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 538\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3787.324610\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005024 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 534\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3780.430216\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 538\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3781.008420\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004009 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 537\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3780.791297\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004468 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 535\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3784.460031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008514 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 543\n",
      "[LightGBM] [Info] Number of data points in the train set: 129005, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3782.802915\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007536 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 543\n",
      "[LightGBM] [Info] Number of data points in the train set: 129005, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3782.802915\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Параметры лучшей модели алгоритма LGBMRegressor(extra_trees=True, n_estimators=40, random_state=123) {'learning_rate': 0.5, 'max_depth': 18}\n"
     ]
    }
   ],
   "source": [
    "# Создание словаря параметров алгоритма LGBMRegressor под выборки из datasets1_2\n",
    "params_lgbm_regressor = {'learning_rate': [0.1, 0.5], 'max_depth': [6, 10, 18]}\n",
    "tab_metrics, best_model_fit = model_evaluator.evaluate_models(datasets1_2, lgbm_regressor, params_lgbm_regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fc62e969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Алгоритм модели</th>\n",
       "      <th>Набор данных</th>\n",
       "      <th>RMSE_fit</th>\n",
       "      <th>Время обучения</th>\n",
       "      <th>Время предсказания</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;catboost.core.CatBoostRegressor object at 0x0...</td>\n",
       "      <td>X_y_set_cut</td>\n",
       "      <td>1282.51</td>\n",
       "      <td>1.696325</td>\n",
       "      <td>0.031263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;catboost.core.CatBoostRegressor object at 0x0...</td>\n",
       "      <td>X_y_set_with_missing</td>\n",
       "      <td>1817.00</td>\n",
       "      <td>9.865913</td>\n",
       "      <td>0.858024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LGBMRegressor(extra_trees=True, n_estimators=4...</td>\n",
       "      <td>X_y_set_cut</td>\n",
       "      <td>1318.87</td>\n",
       "      <td>0.527492</td>\n",
       "      <td>0.327873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Алгоритм модели          Набор данных  \\\n",
       "0  <catboost.core.CatBoostRegressor object at 0x0...           X_y_set_cut   \n",
       "1  <catboost.core.CatBoostRegressor object at 0x0...  X_y_set_with_missing   \n",
       "2  LGBMRegressor(extra_trees=True, n_estimators=4...           X_y_set_cut   \n",
       "\n",
       "   RMSE_fit  Время обучения  Время предсказания  \n",
       "0   1282.51        1.696325            0.031263  \n",
       "1   1817.00        9.865913            0.858024  \n",
       "2   1318.87        0.527492            0.327873  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "606af5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y_set_with_missing[0][cat_features] = X_y_set_with_missing[0][cat_features].astype('category')\n",
    "X_y_set_with_missing[2][cat_features] = X_y_set_with_missing[2][cat_features].astype('category')\n",
    "# Создание словаря выборок для алгоритмов LGBMRegressor\n",
    "datasets1_3 = {'X_y_set_with_missing': X_y_set_with_missing}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "192b16c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010086 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 198446, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4423.674405\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009716 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 678\n",
      "[LightGBM] [Info] Number of data points in the train set: 198446, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4431.431382\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017868 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 198446, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4420.976034\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017476 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 684\n",
      "[LightGBM] [Info] Number of data points in the train set: 198447, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4423.407318\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011187 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 680\n",
      "[LightGBM] [Info] Number of data points in the train set: 198447, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4412.408366\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006410 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 198446, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4423.674405\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020007 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 678\n",
      "[LightGBM] [Info] Number of data points in the train set: 198446, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4431.431382\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017870 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 198446, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4420.976034\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019198 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 684\n",
      "[LightGBM] [Info] Number of data points in the train set: 198447, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4423.407318\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008348 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 680\n",
      "[LightGBM] [Info] Number of data points in the train set: 198447, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4412.408366\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004379 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 198446, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4423.674405\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016697 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 678\n",
      "[LightGBM] [Info] Number of data points in the train set: 198446, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4431.431382\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005621 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 198446, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4420.976034\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017656 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 684\n",
      "[LightGBM] [Info] Number of data points in the train set: 198447, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4423.407318\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004873 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 680\n",
      "[LightGBM] [Info] Number of data points in the train set: 198447, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4412.408366\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005604 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 198446, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4423.674405\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004735 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 678\n",
      "[LightGBM] [Info] Number of data points in the train set: 198446, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4431.431382\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017041 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 198446, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4420.976034\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022375 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 684\n",
      "[LightGBM] [Info] Number of data points in the train set: 198447, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4423.407318\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021208 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 680\n",
      "[LightGBM] [Info] Number of data points in the train set: 198447, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4412.408366\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 198446, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4423.674405\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005630 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 678\n",
      "[LightGBM] [Info] Number of data points in the train set: 198446, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4431.431382\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024452 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 198446, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4420.976034\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017789 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 684\n",
      "[LightGBM] [Info] Number of data points in the train set: 198447, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4423.407318\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008368 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 680\n",
      "[LightGBM] [Info] Number of data points in the train set: 198447, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4412.408366\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032924 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 198446, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4423.674405\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009535 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 678\n",
      "[LightGBM] [Info] Number of data points in the train set: 198446, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4431.431382\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031821 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 681\n",
      "[LightGBM] [Info] Number of data points in the train set: 198446, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4420.976034\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011958 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 684\n",
      "[LightGBM] [Info] Number of data points in the train set: 198447, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4423.407318\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007417 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 680\n",
      "[LightGBM] [Info] Number of data points in the train set: 198447, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4412.408366\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008795 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 683\n",
      "[LightGBM] [Info] Number of data points in the train set: 248058, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4422.379492\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042046 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 683\n",
      "[LightGBM] [Info] Number of data points in the train set: 248058, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4422.379492\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Параметры лучшей модели алгоритма LGBMRegressor(extra_trees=True, n_estimators=40, random_state=123) {'learning_rate': 0.5, 'max_depth': 18}\n"
     ]
    }
   ],
   "source": [
    "# Создание словаря параметров алгоритма LGBMRegressor под выборки из datasets1_3\n",
    "params_lgbm_regressor = {'learning_rate': [0.1, 0.5], 'max_depth': [6, 10, 18]}\n",
    "tab_metrics, best_model_fit = model_evaluator.evaluate_models(datasets1_3, lgbm_regressor, params_lgbm_regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8aa8e0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Алгоритм модели</th>\n",
       "      <th>Набор данных</th>\n",
       "      <th>RMSE_fit</th>\n",
       "      <th>Время обучения</th>\n",
       "      <th>Время предсказания</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;catboost.core.CatBoostRegressor object at 0x0...</td>\n",
       "      <td>X_y_set_cut</td>\n",
       "      <td>1282.51</td>\n",
       "      <td>1.696325</td>\n",
       "      <td>0.031263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;catboost.core.CatBoostRegressor object at 0x0...</td>\n",
       "      <td>X_y_set_with_missing</td>\n",
       "      <td>1817.00</td>\n",
       "      <td>9.865913</td>\n",
       "      <td>0.858024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LGBMRegressor(extra_trees=True, n_estimators=4...</td>\n",
       "      <td>X_y_set_cut</td>\n",
       "      <td>1318.87</td>\n",
       "      <td>0.527492</td>\n",
       "      <td>0.327873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LGBMRegressor(extra_trees=True, n_estimators=4...</td>\n",
       "      <td>X_y_set_with_missing</td>\n",
       "      <td>1962.97</td>\n",
       "      <td>1.478813</td>\n",
       "      <td>0.696383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Алгоритм модели          Набор данных  \\\n",
       "0  <catboost.core.CatBoostRegressor object at 0x0...           X_y_set_cut   \n",
       "1  <catboost.core.CatBoostRegressor object at 0x0...  X_y_set_with_missing   \n",
       "2  LGBMRegressor(extra_trees=True, n_estimators=4...           X_y_set_cut   \n",
       "3  LGBMRegressor(extra_trees=True, n_estimators=4...  X_y_set_with_missing   \n",
       "\n",
       "   RMSE_fit  Время обучения  Время предсказания  \n",
       "0   1282.51        1.696325            0.031263  \n",
       "1   1817.00        9.865913            0.858024  \n",
       "2   1318.87        0.527492            0.327873  \n",
       "3   1962.97        1.478813            0.696383  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b8585f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание словаря выборок для алгоритмов CatBoostRegressor, LGBMRegressor\n",
    "datasets2 = {'X_y_scaled_ordinal_set': X_y_scaled_ordinal_set, 'X_y_scaled_ordinal_set_cut': X_y_scaled_ordinal_set_cut}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "285341c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 4235.4718344\ttotal: 64.9ms\tremaining: 2.53s\n",
      "10:\tlearn: 2715.2346246\ttotal: 581ms\tremaining: 1.53s\n",
      "20:\tlearn: 2287.8964533\ttotal: 1.12s\tremaining: 1.01s\n",
      "30:\tlearn: 2125.1328758\ttotal: 1.66s\tremaining: 482ms\n",
      "39:\tlearn: 2050.3373150\ttotal: 2.09s\tremaining: 0us\n",
      "0:\tlearn: 4226.5632953\ttotal: 52.5ms\tremaining: 2.05s\n",
      "10:\tlearn: 2713.8692299\ttotal: 561ms\tremaining: 1.48s\n",
      "20:\tlearn: 2285.4270787\ttotal: 1.07s\tremaining: 967ms\n",
      "30:\tlearn: 2121.1883169\ttotal: 1.55s\tremaining: 450ms\n",
      "39:\tlearn: 2051.0292134\ttotal: 1.98s\tremaining: 0us\n",
      "0:\tlearn: 4229.2878548\ttotal: 56.3ms\tremaining: 2.19s\n",
      "10:\tlearn: 2721.9249287\ttotal: 547ms\tremaining: 1.44s\n",
      "20:\tlearn: 2283.9885665\ttotal: 1.07s\tremaining: 969ms\n",
      "30:\tlearn: 2126.9785310\ttotal: 1.57s\tremaining: 455ms\n",
      "39:\tlearn: 2059.3706522\ttotal: 2.06s\tremaining: 0us\n",
      "0:\tlearn: 4231.0275812\ttotal: 54.2ms\tremaining: 2.11s\n",
      "10:\tlearn: 2720.4083687\ttotal: 547ms\tremaining: 1.44s\n",
      "20:\tlearn: 2287.7117649\ttotal: 1.05s\tremaining: 955ms\n",
      "30:\tlearn: 2131.5680482\ttotal: 1.54s\tremaining: 447ms\n",
      "39:\tlearn: 2054.2864482\ttotal: 1.96s\tremaining: 0us\n",
      "0:\tlearn: 4229.2480487\ttotal: 50.6ms\tremaining: 1.97s\n",
      "10:\tlearn: 2717.4639048\ttotal: 552ms\tremaining: 1.46s\n",
      "20:\tlearn: 2285.1522634\ttotal: 1.07s\tremaining: 969ms\n",
      "30:\tlearn: 2127.7806446\ttotal: 1.63s\tremaining: 474ms\n",
      "39:\tlearn: 2055.9241537\ttotal: 2.05s\tremaining: 0us\n",
      "0:\tlearn: 3247.9520513\ttotal: 51.6ms\tremaining: 2.01s\n",
      "10:\tlearn: 1998.8209836\ttotal: 593ms\tremaining: 1.56s\n",
      "20:\tlearn: 1894.4400588\ttotal: 1.12s\tremaining: 1.01s\n",
      "30:\tlearn: 1846.3944019\ttotal: 1.64s\tremaining: 476ms\n",
      "39:\tlearn: 1813.3276014\ttotal: 2.1s\tremaining: 0us\n",
      "0:\tlearn: 3234.5944695\ttotal: 52ms\tremaining: 2.03s\n",
      "10:\tlearn: 1994.1847406\ttotal: 554ms\tremaining: 1.46s\n",
      "20:\tlearn: 1891.8984820\ttotal: 1.09s\tremaining: 986ms\n",
      "30:\tlearn: 1841.6832252\ttotal: 1.57s\tremaining: 456ms\n",
      "39:\tlearn: 1808.0751253\ttotal: 2.02s\tremaining: 0us\n",
      "0:\tlearn: 3242.5939684\ttotal: 62.4ms\tremaining: 2.43s\n",
      "10:\tlearn: 1997.0987111\ttotal: 589ms\tremaining: 1.55s\n",
      "20:\tlearn: 1896.8135291\ttotal: 1.13s\tremaining: 1.02s\n",
      "30:\tlearn: 1847.8217267\ttotal: 1.64s\tremaining: 476ms\n",
      "39:\tlearn: 1813.2179841\ttotal: 2.09s\tremaining: 0us\n",
      "0:\tlearn: 3244.0381201\ttotal: 52ms\tremaining: 2.03s\n",
      "10:\tlearn: 1988.8363370\ttotal: 557ms\tremaining: 1.47s\n",
      "20:\tlearn: 1886.6849589\ttotal: 1.07s\tremaining: 966ms\n",
      "30:\tlearn: 1837.4972699\ttotal: 1.68s\tremaining: 487ms\n",
      "39:\tlearn: 1810.2794108\ttotal: 2.23s\tremaining: 0us\n",
      "0:\tlearn: 3244.1774908\ttotal: 49.3ms\tremaining: 1.92s\n",
      "10:\tlearn: 2003.9984398\ttotal: 538ms\tremaining: 1.42s\n",
      "20:\tlearn: 1901.2180940\ttotal: 1.07s\tremaining: 971ms\n",
      "30:\tlearn: 1855.0570915\ttotal: 1.58s\tremaining: 460ms\n",
      "39:\tlearn: 1818.6812721\ttotal: 2.06s\tremaining: 0us\n",
      "0:\tlearn: 4217.3901097\ttotal: 57.9ms\tremaining: 2.26s\n",
      "10:\tlearn: 2622.4396647\ttotal: 705ms\tremaining: 1.86s\n",
      "20:\tlearn: 2185.8619471\ttotal: 1.35s\tremaining: 1.22s\n",
      "30:\tlearn: 2026.5755065\ttotal: 1.99s\tremaining: 578ms\n",
      "39:\tlearn: 1957.1163994\ttotal: 2.54s\tremaining: 0us\n",
      "0:\tlearn: 4208.3944808\ttotal: 65.8ms\tremaining: 2.56s\n",
      "10:\tlearn: 2621.3308384\ttotal: 718ms\tremaining: 1.89s\n",
      "20:\tlearn: 2182.1612569\ttotal: 1.47s\tremaining: 1.33s\n",
      "30:\tlearn: 2025.4852822\ttotal: 2.09s\tremaining: 607ms\n",
      "39:\tlearn: 1958.4082287\ttotal: 2.66s\tremaining: 0us\n",
      "0:\tlearn: 4211.4400978\ttotal: 64ms\tremaining: 2.5s\n",
      "10:\tlearn: 2623.1535201\ttotal: 814ms\tremaining: 2.15s\n",
      "20:\tlearn: 2185.6321032\ttotal: 1.5s\tremaining: 1.36s\n",
      "30:\tlearn: 2033.9648506\ttotal: 2.11s\tremaining: 613ms\n",
      "39:\tlearn: 1962.4858606\ttotal: 2.71s\tremaining: 0us\n",
      "0:\tlearn: 4213.0030104\ttotal: 74.4ms\tremaining: 2.9s\n",
      "10:\tlearn: 2627.0808884\ttotal: 705ms\tremaining: 1.86s\n",
      "20:\tlearn: 2184.3098865\ttotal: 1.41s\tremaining: 1.27s\n",
      "30:\tlearn: 2030.0317338\ttotal: 2.02s\tremaining: 587ms\n",
      "39:\tlearn: 1964.8766463\ttotal: 2.57s\tremaining: 0us\n",
      "0:\tlearn: 4211.1793541\ttotal: 70.7ms\tremaining: 2.76s\n",
      "10:\tlearn: 2620.3024513\ttotal: 714ms\tremaining: 1.88s\n",
      "20:\tlearn: 2186.2173334\ttotal: 1.37s\tremaining: 1.24s\n",
      "30:\tlearn: 2028.1236259\ttotal: 2.03s\tremaining: 590ms\n",
      "39:\tlearn: 1957.9584092\ttotal: 2.62s\tremaining: 0us\n",
      "0:\tlearn: 3153.4406704\ttotal: 62.8ms\tremaining: 2.45s\n",
      "10:\tlearn: 1914.7423052\ttotal: 709ms\tremaining: 1.87s\n",
      "20:\tlearn: 1823.7856775\ttotal: 1.37s\tremaining: 1.24s\n",
      "30:\tlearn: 1772.9236051\ttotal: 2s\tremaining: 582ms\n",
      "39:\tlearn: 1738.3211337\ttotal: 2.59s\tremaining: 0us\n",
      "0:\tlearn: 3139.3560496\ttotal: 63.5ms\tremaining: 2.48s\n",
      "10:\tlearn: 1911.5995890\ttotal: 693ms\tremaining: 1.82s\n",
      "20:\tlearn: 1818.5926870\ttotal: 1.37s\tremaining: 1.24s\n",
      "30:\tlearn: 1769.4198278\ttotal: 2.04s\tremaining: 592ms\n",
      "39:\tlearn: 1732.4700774\ttotal: 2.62s\tremaining: 0us\n",
      "0:\tlearn: 3149.2874022\ttotal: 66.8ms\tremaining: 2.6s\n",
      "10:\tlearn: 1906.0916920\ttotal: 705ms\tremaining: 1.86s\n",
      "20:\tlearn: 1822.5690682\ttotal: 1.35s\tremaining: 1.22s\n",
      "30:\tlearn: 1771.5510220\ttotal: 2.01s\tremaining: 584ms\n",
      "39:\tlearn: 1738.1868228\ttotal: 2.66s\tremaining: 0us\n",
      "0:\tlearn: 3149.7726268\ttotal: 67.3ms\tremaining: 2.62s\n",
      "10:\tlearn: 1908.4145346\ttotal: 706ms\tremaining: 1.86s\n",
      "20:\tlearn: 1813.3306072\ttotal: 1.4s\tremaining: 1.27s\n",
      "30:\tlearn: 1764.6923729\ttotal: 2.07s\tremaining: 600ms\n",
      "39:\tlearn: 1735.9360669\ttotal: 2.65s\tremaining: 0us\n",
      "0:\tlearn: 3149.7600935\ttotal: 66ms\tremaining: 2.58s\n",
      "10:\tlearn: 1910.0247740\ttotal: 699ms\tremaining: 1.84s\n",
      "20:\tlearn: 1821.7421788\ttotal: 1.36s\tremaining: 1.23s\n",
      "30:\tlearn: 1771.5346460\ttotal: 2.08s\tremaining: 605ms\n",
      "39:\tlearn: 1737.9114621\ttotal: 2.66s\tremaining: 0us\n",
      "0:\tlearn: 4208.5755467\ttotal: 114ms\tremaining: 4.43s\n",
      "10:\tlearn: 2552.1099872\ttotal: 1.22s\tremaining: 3.22s\n",
      "20:\tlearn: 2104.4982666\ttotal: 2.29s\tremaining: 2.07s\n",
      "30:\tlearn: 1946.4932305\ttotal: 3.38s\tremaining: 981ms\n",
      "39:\tlearn: 1884.4915628\ttotal: 4.36s\tremaining: 0us\n",
      "0:\tlearn: 4200.3479181\ttotal: 99.4ms\tremaining: 3.88s\n",
      "10:\tlearn: 2542.2879036\ttotal: 1.32s\tremaining: 3.49s\n",
      "20:\tlearn: 2092.0881704\ttotal: 2.37s\tremaining: 2.14s\n",
      "30:\tlearn: 1945.4402242\ttotal: 3.46s\tremaining: 1s\n",
      "39:\tlearn: 1879.5367240\ttotal: 4.44s\tremaining: 0us\n",
      "0:\tlearn: 4202.7263980\ttotal: 106ms\tremaining: 4.15s\n",
      "10:\tlearn: 2549.9053430\ttotal: 1.21s\tremaining: 3.2s\n",
      "20:\tlearn: 2099.8511563\ttotal: 2.28s\tremaining: 2.06s\n",
      "30:\tlearn: 1949.6038038\ttotal: 3.37s\tremaining: 978ms\n",
      "39:\tlearn: 1883.6717501\ttotal: 4.38s\tremaining: 0us\n",
      "0:\tlearn: 4202.6874669\ttotal: 114ms\tremaining: 4.45s\n",
      "10:\tlearn: 2552.0345257\ttotal: 1.31s\tremaining: 3.46s\n",
      "20:\tlearn: 2107.2641680\ttotal: 2.45s\tremaining: 2.22s\n",
      "30:\tlearn: 1957.5389132\ttotal: 3.6s\tremaining: 1.05s\n",
      "39:\tlearn: 1891.2277129\ttotal: 4.61s\tremaining: 0us\n",
      "0:\tlearn: 4202.3709987\ttotal: 145ms\tremaining: 5.67s\n",
      "10:\tlearn: 2545.9015350\ttotal: 1.25s\tremaining: 3.28s\n",
      "20:\tlearn: 2099.7732998\ttotal: 2.3s\tremaining: 2.08s\n",
      "30:\tlearn: 1948.6477891\ttotal: 3.4s\tremaining: 987ms\n",
      "39:\tlearn: 1886.0129380\ttotal: 4.36s\tremaining: 0us\n",
      "0:\tlearn: 3105.6731997\ttotal: 107ms\tremaining: 4.16s\n",
      "10:\tlearn: 1843.0295068\ttotal: 1.2s\tremaining: 3.17s\n",
      "20:\tlearn: 1754.6902470\ttotal: 2.32s\tremaining: 2.1s\n",
      "30:\tlearn: 1697.8468680\ttotal: 3.72s\tremaining: 1.08s\n",
      "39:\tlearn: 1659.7666538\ttotal: 4.76s\tremaining: 0us\n",
      "0:\tlearn: 3094.6871171\ttotal: 110ms\tremaining: 4.29s\n",
      "10:\tlearn: 1850.7490990\ttotal: 1.19s\tremaining: 3.13s\n",
      "20:\tlearn: 1756.2421531\ttotal: 2.32s\tremaining: 2.1s\n",
      "30:\tlearn: 1704.6877814\ttotal: 3.38s\tremaining: 982ms\n",
      "39:\tlearn: 1667.0512663\ttotal: 4.37s\tremaining: 0us\n",
      "0:\tlearn: 3102.0477125\ttotal: 102ms\tremaining: 3.97s\n",
      "10:\tlearn: 1845.7193418\ttotal: 1.19s\tremaining: 3.13s\n",
      "20:\tlearn: 1753.1372574\ttotal: 2.34s\tremaining: 2.12s\n",
      "30:\tlearn: 1691.7140987\ttotal: 3.4s\tremaining: 987ms\n",
      "39:\tlearn: 1654.6782443\ttotal: 4.38s\tremaining: 0us\n",
      "0:\tlearn: 3094.2141358\ttotal: 109ms\tremaining: 4.26s\n",
      "10:\tlearn: 1837.9481997\ttotal: 1.23s\tremaining: 3.24s\n",
      "20:\tlearn: 1750.7489635\ttotal: 2.4s\tremaining: 2.17s\n",
      "30:\tlearn: 1695.2865139\ttotal: 3.55s\tremaining: 1.03s\n",
      "39:\tlearn: 1659.9837856\ttotal: 4.52s\tremaining: 0us\n",
      "0:\tlearn: 3102.0254597\ttotal: 107ms\tremaining: 4.17s\n",
      "10:\tlearn: 1847.2831064\ttotal: 1.18s\tremaining: 3.12s\n",
      "20:\tlearn: 1757.9035747\ttotal: 2.26s\tremaining: 2.05s\n",
      "30:\tlearn: 1703.0109552\ttotal: 3.39s\tremaining: 984ms\n",
      "39:\tlearn: 1663.9819131\ttotal: 4.5s\tremaining: 0us\n",
      "0:\tlearn: 3096.2437872\ttotal: 121ms\tremaining: 4.74s\n",
      "10:\tlearn: 1843.0567545\ttotal: 1.53s\tremaining: 4.03s\n",
      "20:\tlearn: 1755.9830374\ttotal: 2.84s\tremaining: 2.57s\n",
      "30:\tlearn: 1706.0672538\ttotal: 4.11s\tremaining: 1.19s\n",
      "39:\tlearn: 1666.1345367\ttotal: 5.25s\tremaining: 0us\n",
      "0:\tlearn: 3096.2437872\ttotal: 128ms\tremaining: 4.98s\n",
      "10:\tlearn: 1843.0567545\ttotal: 1.4s\tremaining: 3.69s\n",
      "20:\tlearn: 1755.9830374\ttotal: 2.76s\tremaining: 2.5s\n",
      "30:\tlearn: 1706.0672538\ttotal: 4.14s\tremaining: 1.2s\n",
      "39:\tlearn: 1666.1345367\ttotal: 5.27s\tremaining: 0us\n",
      "Параметры лучшей модели алгоритма <catboost.core.CatBoostRegressor object at 0x0000022C288A7C70> {'depth': 10, 'learning_rate': 0.5}\n",
      "0:\tlearn: 3042.3770134\ttotal: 34.1ms\tremaining: 1.33s\n",
      "10:\tlearn: 1986.5532000\ttotal: 341ms\tremaining: 899ms\n",
      "20:\tlearn: 1674.2764897\ttotal: 644ms\tremaining: 583ms\n",
      "30:\tlearn: 1549.3815348\ttotal: 971ms\tremaining: 282ms\n",
      "39:\tlearn: 1490.2433186\ttotal: 1.23s\tremaining: 0us\n",
      "0:\tlearn: 3038.6908385\ttotal: 28.7ms\tremaining: 1.12s\n",
      "10:\tlearn: 1978.0433441\ttotal: 364ms\tremaining: 961ms\n",
      "20:\tlearn: 1666.3866834\ttotal: 730ms\tremaining: 661ms\n",
      "30:\tlearn: 1540.4190851\ttotal: 1.03s\tremaining: 299ms\n",
      "39:\tlearn: 1479.8675175\ttotal: 1.29s\tremaining: 0us\n",
      "0:\tlearn: 3034.6384565\ttotal: 29.8ms\tremaining: 1.16s\n",
      "10:\tlearn: 1978.6605998\ttotal: 340ms\tremaining: 896ms\n",
      "20:\tlearn: 1665.8227808\ttotal: 644ms\tremaining: 583ms\n",
      "30:\tlearn: 1538.1541678\ttotal: 951ms\tremaining: 276ms\n",
      "39:\tlearn: 1481.7475834\ttotal: 1.23s\tremaining: 0us\n",
      "0:\tlearn: 3040.9989813\ttotal: 35.1ms\tremaining: 1.37s\n",
      "10:\tlearn: 1987.1463870\ttotal: 351ms\tremaining: 925ms\n",
      "20:\tlearn: 1666.6506457\ttotal: 673ms\tremaining: 609ms\n",
      "30:\tlearn: 1538.8933108\ttotal: 971ms\tremaining: 282ms\n",
      "39:\tlearn: 1478.0433538\ttotal: 1.24s\tremaining: 0us\n",
      "0:\tlearn: 3041.1273392\ttotal: 31.5ms\tremaining: 1.23s\n",
      "10:\tlearn: 1976.5002887\ttotal: 414ms\tremaining: 1.09s\n",
      "20:\tlearn: 1663.0353787\ttotal: 718ms\tremaining: 650ms\n",
      "30:\tlearn: 1536.5890164\ttotal: 1.03s\tremaining: 298ms\n",
      "39:\tlearn: 1478.6808610\ttotal: 1.34s\tremaining: 0us\n",
      "0:\tlearn: 2363.6259754\ttotal: 33.2ms\tremaining: 1.29s\n",
      "10:\tlearn: 1437.7185845\ttotal: 326ms\tremaining: 860ms\n",
      "20:\tlearn: 1369.5806384\ttotal: 651ms\tremaining: 589ms\n",
      "30:\tlearn: 1336.5797082\ttotal: 959ms\tremaining: 279ms\n",
      "39:\tlearn: 1316.4476385\ttotal: 1.25s\tremaining: 0us\n",
      "0:\tlearn: 2356.5734000\ttotal: 33ms\tremaining: 1.29s\n",
      "10:\tlearn: 1448.2036880\ttotal: 344ms\tremaining: 906ms\n",
      "20:\tlearn: 1369.6429354\ttotal: 660ms\tremaining: 597ms\n",
      "30:\tlearn: 1338.1061781\ttotal: 979ms\tremaining: 284ms\n",
      "39:\tlearn: 1314.4841236\ttotal: 1.29s\tremaining: 0us\n",
      "0:\tlearn: 2351.9416508\ttotal: 32.3ms\tremaining: 1.26s\n",
      "10:\tlearn: 1432.5568689\ttotal: 340ms\tremaining: 895ms\n",
      "20:\tlearn: 1365.0253971\ttotal: 771ms\tremaining: 698ms\n",
      "30:\tlearn: 1334.6896569\ttotal: 1.08s\tremaining: 315ms\n",
      "39:\tlearn: 1312.8635626\ttotal: 1.36s\tremaining: 0us\n",
      "0:\tlearn: 2379.7029116\ttotal: 34.3ms\tremaining: 1.34s\n",
      "10:\tlearn: 1433.7989897\ttotal: 375ms\tremaining: 989ms\n",
      "20:\tlearn: 1367.2330998\ttotal: 693ms\tremaining: 627ms\n",
      "30:\tlearn: 1329.4751279\ttotal: 1.01s\tremaining: 294ms\n",
      "39:\tlearn: 1309.3435427\ttotal: 1.29s\tremaining: 0us\n",
      "0:\tlearn: 2360.4456598\ttotal: 30.7ms\tremaining: 1.2s\n",
      "10:\tlearn: 1433.9867689\ttotal: 349ms\tremaining: 920ms\n",
      "20:\tlearn: 1367.8438559\ttotal: 669ms\tremaining: 605ms\n",
      "30:\tlearn: 1333.4601328\ttotal: 976ms\tremaining: 283ms\n",
      "39:\tlearn: 1312.7821901\ttotal: 1.25s\tremaining: 0us\n",
      "0:\tlearn: 3028.5838836\ttotal: 41.4ms\tremaining: 1.61s\n",
      "10:\tlearn: 1907.4745723\ttotal: 446ms\tremaining: 1.18s\n",
      "20:\tlearn: 1593.9501388\ttotal: 987ms\tremaining: 893ms\n",
      "30:\tlearn: 1474.2111095\ttotal: 1.41s\tremaining: 410ms\n",
      "39:\tlearn: 1422.2475489\ttotal: 1.77s\tremaining: 0us\n",
      "0:\tlearn: 3025.9887829\ttotal: 43.6ms\tremaining: 1.7s\n",
      "10:\tlearn: 1899.2445129\ttotal: 542ms\tremaining: 1.43s\n",
      "20:\tlearn: 1582.4074407\ttotal: 953ms\tremaining: 862ms\n",
      "30:\tlearn: 1466.9683514\ttotal: 1.35s\tremaining: 392ms\n",
      "39:\tlearn: 1416.7957320\ttotal: 1.71s\tremaining: 0us\n",
      "0:\tlearn: 3021.7836618\ttotal: 46.5ms\tremaining: 1.81s\n",
      "10:\tlearn: 1903.8739066\ttotal: 451ms\tremaining: 1.19s\n",
      "20:\tlearn: 1581.8448374\ttotal: 858ms\tremaining: 776ms\n",
      "30:\tlearn: 1468.0076051\ttotal: 1.26s\tremaining: 367ms\n",
      "39:\tlearn: 1419.3545002\ttotal: 1.62s\tremaining: 0us\n",
      "0:\tlearn: 3023.1854178\ttotal: 39.8ms\tremaining: 1.55s\n",
      "10:\tlearn: 1901.9295039\ttotal: 471ms\tremaining: 1.24s\n",
      "20:\tlearn: 1587.7513737\ttotal: 870ms\tremaining: 787ms\n",
      "30:\tlearn: 1472.3562002\ttotal: 1.26s\tremaining: 366ms\n",
      "39:\tlearn: 1417.1727797\ttotal: 1.61s\tremaining: 0us\n",
      "0:\tlearn: 3027.2649374\ttotal: 47.2ms\tremaining: 1.84s\n",
      "10:\tlearn: 1902.3994563\ttotal: 469ms\tremaining: 1.24s\n",
      "20:\tlearn: 1584.3308930\ttotal: 871ms\tremaining: 788ms\n",
      "30:\tlearn: 1465.4931036\ttotal: 1.26s\tremaining: 366ms\n",
      "39:\tlearn: 1416.5854908\ttotal: 1.62s\tremaining: 0us\n",
      "0:\tlearn: 2292.4049529\ttotal: 46.8ms\tremaining: 1.83s\n",
      "10:\tlearn: 1385.6256234\ttotal: 491ms\tremaining: 1.29s\n",
      "20:\tlearn: 1326.4840668\ttotal: 880ms\tremaining: 796ms\n",
      "30:\tlearn: 1294.5480582\ttotal: 1.27s\tremaining: 369ms\n",
      "39:\tlearn: 1272.5097665\ttotal: 1.59s\tremaining: 0us\n",
      "0:\tlearn: 2290.9550695\ttotal: 27.2ms\tremaining: 1.06s\n",
      "10:\tlearn: 1383.9154057\ttotal: 387ms\tremaining: 1.02s\n",
      "20:\tlearn: 1314.5316619\ttotal: 695ms\tremaining: 628ms\n",
      "30:\tlearn: 1284.1014365\ttotal: 967ms\tremaining: 281ms\n",
      "39:\tlearn: 1263.5379024\ttotal: 1.22s\tremaining: 0us\n",
      "0:\tlearn: 2285.5191181\ttotal: 29.6ms\tremaining: 1.15s\n",
      "10:\tlearn: 1382.6672117\ttotal: 330ms\tremaining: 869ms\n",
      "20:\tlearn: 1315.6739742\ttotal: 608ms\tremaining: 550ms\n",
      "30:\tlearn: 1285.8279859\ttotal: 883ms\tremaining: 256ms\n",
      "39:\tlearn: 1266.9532095\ttotal: 1.14s\tremaining: 0us\n",
      "0:\tlearn: 2288.0431880\ttotal: 25.8ms\tremaining: 1s\n",
      "10:\tlearn: 1374.3394195\ttotal: 308ms\tremaining: 812ms\n",
      "20:\tlearn: 1318.9160630\ttotal: 592ms\tremaining: 536ms\n",
      "30:\tlearn: 1289.1946360\ttotal: 865ms\tremaining: 251ms\n",
      "39:\tlearn: 1266.0946588\ttotal: 1.11s\tremaining: 0us\n",
      "0:\tlearn: 2288.7908952\ttotal: 27.7ms\tremaining: 1.08s\n",
      "10:\tlearn: 1382.4760100\ttotal: 305ms\tremaining: 804ms\n",
      "20:\tlearn: 1317.4433680\ttotal: 576ms\tremaining: 521ms\n",
      "30:\tlearn: 1284.9366182\ttotal: 852ms\tremaining: 247ms\n",
      "39:\tlearn: 1264.6397376\ttotal: 1.29s\tremaining: 0us\n",
      "0:\tlearn: 3017.5999328\ttotal: 46.5ms\tremaining: 1.81s\n",
      "10:\tlearn: 1847.6231910\ttotal: 526ms\tremaining: 1.39s\n",
      "20:\tlearn: 1523.9244309\ttotal: 1.02s\tremaining: 920ms\n",
      "30:\tlearn: 1412.6932185\ttotal: 1.51s\tremaining: 440ms\n",
      "39:\tlearn: 1369.1028476\ttotal: 1.94s\tremaining: 0us\n",
      "0:\tlearn: 3016.3356926\ttotal: 47.6ms\tremaining: 1.86s\n",
      "10:\tlearn: 1846.3670248\ttotal: 737ms\tremaining: 1.94s\n",
      "20:\tlearn: 1523.4458021\ttotal: 1.24s\tremaining: 1.12s\n",
      "30:\tlearn: 1417.7127545\ttotal: 1.72s\tremaining: 498ms\n",
      "39:\tlearn: 1367.3444219\ttotal: 2.14s\tremaining: 0us\n",
      "0:\tlearn: 3011.4645491\ttotal: 48.6ms\tremaining: 1.9s\n",
      "10:\tlearn: 1842.8700481\ttotal: 517ms\tremaining: 1.36s\n",
      "20:\tlearn: 1523.0276290\ttotal: 994ms\tremaining: 899ms\n",
      "30:\tlearn: 1413.7975632\ttotal: 1.46s\tremaining: 425ms\n",
      "39:\tlearn: 1364.6325441\ttotal: 1.91s\tremaining: 0us\n",
      "0:\tlearn: 3011.4885331\ttotal: 51.2ms\tremaining: 2s\n",
      "10:\tlearn: 1845.8202332\ttotal: 537ms\tremaining: 1.42s\n",
      "20:\tlearn: 1520.8218887\ttotal: 1.03s\tremaining: 935ms\n",
      "30:\tlearn: 1410.4886568\ttotal: 1.5s\tremaining: 437ms\n",
      "39:\tlearn: 1364.6915148\ttotal: 1.93s\tremaining: 0us\n",
      "0:\tlearn: 3016.5169740\ttotal: 43.8ms\tremaining: 1.71s\n",
      "10:\tlearn: 1845.1680019\ttotal: 511ms\tremaining: 1.35s\n",
      "20:\tlearn: 1521.5108158\ttotal: 988ms\tremaining: 894ms\n",
      "30:\tlearn: 1411.8010002\ttotal: 1.46s\tremaining: 425ms\n",
      "39:\tlearn: 1361.5828505\ttotal: 1.9s\tremaining: 0us\n",
      "0:\tlearn: 2233.5074793\ttotal: 49.7ms\tremaining: 1.94s\n",
      "10:\tlearn: 1337.4044383\ttotal: 526ms\tremaining: 1.39s\n",
      "20:\tlearn: 1281.8142302\ttotal: 1.18s\tremaining: 1.06s\n",
      "30:\tlearn: 1249.3215745\ttotal: 1.65s\tremaining: 478ms\n",
      "39:\tlearn: 1226.8997994\ttotal: 2.08s\tremaining: 0us\n",
      "0:\tlearn: 2238.8774202\ttotal: 49.1ms\tremaining: 1.92s\n",
      "10:\tlearn: 1335.5785709\ttotal: 726ms\tremaining: 1.91s\n",
      "20:\tlearn: 1275.8373260\ttotal: 1.23s\tremaining: 1.11s\n",
      "30:\tlearn: 1245.3960290\ttotal: 1.71s\tremaining: 497ms\n",
      "39:\tlearn: 1225.1838480\ttotal: 2.14s\tremaining: 0us\n",
      "0:\tlearn: 2229.9616956\ttotal: 48.2ms\tremaining: 1.88s\n",
      "10:\tlearn: 1337.1489555\ttotal: 515ms\tremaining: 1.36s\n",
      "20:\tlearn: 1277.1266579\ttotal: 1.02s\tremaining: 921ms\n",
      "30:\tlearn: 1245.1729561\ttotal: 1.48s\tremaining: 431ms\n",
      "39:\tlearn: 1221.5353540\ttotal: 1.92s\tremaining: 0us\n",
      "0:\tlearn: 2225.5028542\ttotal: 48.6ms\tremaining: 1.89s\n",
      "10:\tlearn: 1338.1205817\ttotal: 521ms\tremaining: 1.37s\n",
      "20:\tlearn: 1276.8529739\ttotal: 999ms\tremaining: 904ms\n",
      "30:\tlearn: 1244.6747023\ttotal: 1.44s\tremaining: 417ms\n",
      "39:\tlearn: 1227.6079663\ttotal: 1.85s\tremaining: 0us\n",
      "0:\tlearn: 2231.0885325\ttotal: 48.1ms\tremaining: 1.87s\n",
      "10:\tlearn: 1331.8023182\ttotal: 574ms\tremaining: 1.51s\n",
      "20:\tlearn: 1270.4101193\ttotal: 1.19s\tremaining: 1.07s\n",
      "30:\tlearn: 1238.0800073\ttotal: 1.67s\tremaining: 486ms\n",
      "39:\tlearn: 1216.5738498\ttotal: 2.12s\tremaining: 0us\n",
      "0:\tlearn: 2244.2875444\ttotal: 62.6ms\tremaining: 2.44s\n",
      "10:\tlearn: 1331.0504905\ttotal: 603ms\tremaining: 1.59s\n",
      "20:\tlearn: 1283.0358716\ttotal: 1.15s\tremaining: 1.04s\n",
      "30:\tlearn: 1248.5288871\ttotal: 1.68s\tremaining: 489ms\n",
      "39:\tlearn: 1226.3254068\ttotal: 2.18s\tremaining: 0us\n",
      "0:\tlearn: 2244.2875444\ttotal: 56.1ms\tremaining: 2.19s\n",
      "10:\tlearn: 1331.0504905\ttotal: 587ms\tremaining: 1.55s\n",
      "20:\tlearn: 1283.0358716\ttotal: 1.13s\tremaining: 1.02s\n",
      "30:\tlearn: 1248.5288871\ttotal: 1.67s\tremaining: 485ms\n",
      "39:\tlearn: 1226.3254068\ttotal: 2.26s\tremaining: 0us\n",
      "Параметры лучшей модели алгоритма <catboost.core.CatBoostRegressor object at 0x0000022C288A7C70> {'depth': 10, 'learning_rate': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Создание словаря параметров алгоритма CatBoostRegressor под выборки из datasets2\n",
    "params_cat_boost_regressor = {'learning_rate': [0.1, 0.5], 'depth': [6, 8, 10]}\n",
    "tab_metrics, best_model_fit = model_evaluator.evaluate_models(datasets2, cat_boost_regressor, params_cat_boost_regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "431beaac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Алгоритм модели</th>\n",
       "      <th>Набор данных</th>\n",
       "      <th>RMSE_fit</th>\n",
       "      <th>Время обучения</th>\n",
       "      <th>Время предсказания</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;catboost.core.CatBoostRegressor object at 0x0...</td>\n",
       "      <td>X_y_set_cut</td>\n",
       "      <td>1282.51</td>\n",
       "      <td>1.696325</td>\n",
       "      <td>0.031263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;catboost.core.CatBoostRegressor object at 0x0...</td>\n",
       "      <td>X_y_set_with_missing</td>\n",
       "      <td>1817.00</td>\n",
       "      <td>9.865913</td>\n",
       "      <td>0.858024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LGBMRegressor(extra_trees=True, n_estimators=4...</td>\n",
       "      <td>X_y_set_cut</td>\n",
       "      <td>1318.87</td>\n",
       "      <td>0.527492</td>\n",
       "      <td>0.327873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LGBMRegressor(extra_trees=True, n_estimators=4...</td>\n",
       "      <td>X_y_set_with_missing</td>\n",
       "      <td>1962.97</td>\n",
       "      <td>1.478813</td>\n",
       "      <td>0.696383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;catboost.core.CatBoostRegressor object at 0x0...</td>\n",
       "      <td>X_y_scaled_ordinal_set</td>\n",
       "      <td>1738.48</td>\n",
       "      <td>5.527415</td>\n",
       "      <td>0.107008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;catboost.core.CatBoostRegressor object at 0x0...</td>\n",
       "      <td>X_y_scaled_ordinal_set_cut</td>\n",
       "      <td>1282.51</td>\n",
       "      <td>2.380046</td>\n",
       "      <td>0.068185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Алгоритм модели  \\\n",
       "0  <catboost.core.CatBoostRegressor object at 0x0...   \n",
       "1  <catboost.core.CatBoostRegressor object at 0x0...   \n",
       "2  LGBMRegressor(extra_trees=True, n_estimators=4...   \n",
       "3  LGBMRegressor(extra_trees=True, n_estimators=4...   \n",
       "4  <catboost.core.CatBoostRegressor object at 0x0...   \n",
       "5  <catboost.core.CatBoostRegressor object at 0x0...   \n",
       "\n",
       "                 Набор данных  RMSE_fit  Время обучения  Время предсказания  \n",
       "0                 X_y_set_cut   1282.51        1.696325            0.031263  \n",
       "1        X_y_set_with_missing   1817.00        9.865913            0.858024  \n",
       "2                 X_y_set_cut   1318.87        0.527492            0.327873  \n",
       "3        X_y_set_with_missing   1962.97        1.478813            0.696383  \n",
       "4      X_y_scaled_ordinal_set   1738.48        5.527415            0.107008  \n",
       "5  X_y_scaled_ordinal_set_cut   1282.51        2.380046            0.068185  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f5048f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028423 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 601\n",
      "[LightGBM] [Info] Number of data points in the train set: 183408, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4742.026078\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021438 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 598\n",
      "[LightGBM] [Info] Number of data points in the train set: 183409, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4734.691111\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019560 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 600\n",
      "[LightGBM] [Info] Number of data points in the train set: 183409, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4737.333953\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009425 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 598\n",
      "[LightGBM] [Info] Number of data points in the train set: 183409, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4738.214973\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014281 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 601\n",
      "[LightGBM] [Info] Number of data points in the train set: 183409, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4730.168912\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018946 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 601\n",
      "[LightGBM] [Info] Number of data points in the train set: 183408, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4742.026078\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015202 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 598\n",
      "[LightGBM] [Info] Number of data points in the train set: 183409, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4734.691111\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013999 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 600\n",
      "[LightGBM] [Info] Number of data points in the train set: 183409, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4737.333953\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006960 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 598\n",
      "[LightGBM] [Info] Number of data points in the train set: 183409, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4738.214973\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013857 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 601\n",
      "[LightGBM] [Info] Number of data points in the train set: 183409, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4730.168912\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020461 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 601\n",
      "[LightGBM] [Info] Number of data points in the train set: 183408, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4742.026078\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007356 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 598\n",
      "[LightGBM] [Info] Number of data points in the train set: 183409, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4734.691111\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019842 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 600\n",
      "[LightGBM] [Info] Number of data points in the train set: 183409, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4737.333953\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007910 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 598\n",
      "[LightGBM] [Info] Number of data points in the train set: 183409, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4738.214973\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020000 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 601\n",
      "[LightGBM] [Info] Number of data points in the train set: 183409, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4730.168912\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010351 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 601\n",
      "[LightGBM] [Info] Number of data points in the train set: 183408, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4742.026078\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014407 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 598\n",
      "[LightGBM] [Info] Number of data points in the train set: 183409, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4734.691111\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025647 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 600\n",
      "[LightGBM] [Info] Number of data points in the train set: 183409, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4737.333953\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025517 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 598\n",
      "[LightGBM] [Info] Number of data points in the train set: 183409, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4738.214973\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015383 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 601\n",
      "[LightGBM] [Info] Number of data points in the train set: 183409, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4730.168912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016728 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 601\n",
      "[LightGBM] [Info] Number of data points in the train set: 183408, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4742.026078\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019388 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 598\n",
      "[LightGBM] [Info] Number of data points in the train set: 183409, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4734.691111\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025417 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 600\n",
      "[LightGBM] [Info] Number of data points in the train set: 183409, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4737.333953\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018797 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 598\n",
      "[LightGBM] [Info] Number of data points in the train set: 183409, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4738.214973\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019654 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 601\n",
      "[LightGBM] [Info] Number of data points in the train set: 183409, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4730.168912\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024369 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 601\n",
      "[LightGBM] [Info] Number of data points in the train set: 183408, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4742.026078\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027598 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 598\n",
      "[LightGBM] [Info] Number of data points in the train set: 183409, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4734.691111\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016177 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 600\n",
      "[LightGBM] [Info] Number of data points in the train set: 183409, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4737.333953\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 598\n",
      "[LightGBM] [Info] Number of data points in the train set: 183409, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4738.214973\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015813 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 601\n",
      "[LightGBM] [Info] Number of data points in the train set: 183409, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4730.168912\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019694 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 601\n",
      "[LightGBM] [Info] Number of data points in the train set: 229261, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4736.487000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019627 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 601\n",
      "[LightGBM] [Info] Number of data points in the train set: 229261, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4736.487000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Параметры лучшей модели алгоритма LGBMRegressor(n_estimators=40, random_state=123) {'learning_rate': 0.5, 'max_depth': 10}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007323 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 536\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3787.324610\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004589 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 533\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3780.430216\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005294 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 535\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3781.008420\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004618 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 536\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3780.791297\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004291 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 532\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3784.460031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008623 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 536\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3787.324610\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004664 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 533\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3780.430216\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004779 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 535\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3781.008420\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004159 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 536\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3780.791297\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005171 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 532\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3784.460031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004224 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 536\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3787.324610\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005425 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 533\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3780.430216\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004892 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 535\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3781.008420\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004718 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 536\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3780.791297\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005960 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 532\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3784.460031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006686 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 536\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3787.324610\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004561 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 533\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3780.430216\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006555 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 535\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3781.008420\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004048 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 536\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3780.791297\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007173 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 532\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3784.460031\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 536\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3787.324610\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005238 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 533\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3780.430216\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006200 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 535\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3781.008420\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004661 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 536\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3780.791297\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 532\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3784.460031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005405 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 536\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3787.324610\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 533\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3780.430216\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004385 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 535\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3781.008420\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 536\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3780.791297\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005720 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 532\n",
      "[LightGBM] [Info] Number of data points in the train set: 103204, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3784.460031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006977 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 541\n",
      "[LightGBM] [Info] Number of data points in the train set: 129005, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3782.802915\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005896 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 541\n",
      "[LightGBM] [Info] Number of data points in the train set: 129005, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 3782.802915\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Параметры лучшей модели алгоритма LGBMRegressor(n_estimators=40, random_state=123) {'learning_rate': 0.5, 'max_depth': 10}\n"
     ]
    }
   ],
   "source": [
    "# Создание словаря параметров алгоритма LGBMRegressor под выборки из datasets2\n",
    "params_lgbm_regressor = {'learning_rate': [0.1, 0.5], 'max_depth': [6, 10, 18]}\n",
    "tab_metrics, best_model_fit = model_evaluator.evaluate_models(datasets2, lgbm_regressor_cod, params_lgbm_regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c579f27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Алгоритм модели</th>\n",
       "      <th>Набор данных</th>\n",
       "      <th>RMSE_fit</th>\n",
       "      <th>Время обучения</th>\n",
       "      <th>Время предсказания</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;catboost.core.CatBoostRegressor object at 0x0...</td>\n",
       "      <td>X_y_set_cut</td>\n",
       "      <td>1282.51</td>\n",
       "      <td>1.696325</td>\n",
       "      <td>0.031263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;catboost.core.CatBoostRegressor object at 0x0...</td>\n",
       "      <td>X_y_set_with_missing</td>\n",
       "      <td>1817.00</td>\n",
       "      <td>9.865913</td>\n",
       "      <td>0.858024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LGBMRegressor(extra_trees=True, n_estimators=4...</td>\n",
       "      <td>X_y_set_cut</td>\n",
       "      <td>1318.87</td>\n",
       "      <td>0.527492</td>\n",
       "      <td>0.327873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LGBMRegressor(extra_trees=True, n_estimators=4...</td>\n",
       "      <td>X_y_set_with_missing</td>\n",
       "      <td>1962.97</td>\n",
       "      <td>1.478813</td>\n",
       "      <td>0.696383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;catboost.core.CatBoostRegressor object at 0x0...</td>\n",
       "      <td>X_y_scaled_ordinal_set</td>\n",
       "      <td>1738.48</td>\n",
       "      <td>5.527415</td>\n",
       "      <td>0.107008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;catboost.core.CatBoostRegressor object at 0x0...</td>\n",
       "      <td>X_y_scaled_ordinal_set_cut</td>\n",
       "      <td>1282.51</td>\n",
       "      <td>2.380046</td>\n",
       "      <td>0.068185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LGBMRegressor(n_estimators=40, random_state=123)</td>\n",
       "      <td>X_y_scaled_ordinal_set</td>\n",
       "      <td>1772.74</td>\n",
       "      <td>0.705230</td>\n",
       "      <td>0.391032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LGBMRegressor(n_estimators=40, random_state=123)</td>\n",
       "      <td>X_y_scaled_ordinal_set_cut</td>\n",
       "      <td>1295.74</td>\n",
       "      <td>0.354567</td>\n",
       "      <td>0.316859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Алгоритм модели  \\\n",
       "0  <catboost.core.CatBoostRegressor object at 0x0...   \n",
       "1  <catboost.core.CatBoostRegressor object at 0x0...   \n",
       "2  LGBMRegressor(extra_trees=True, n_estimators=4...   \n",
       "3  LGBMRegressor(extra_trees=True, n_estimators=4...   \n",
       "4  <catboost.core.CatBoostRegressor object at 0x0...   \n",
       "5  <catboost.core.CatBoostRegressor object at 0x0...   \n",
       "6   LGBMRegressor(n_estimators=40, random_state=123)   \n",
       "7   LGBMRegressor(n_estimators=40, random_state=123)   \n",
       "\n",
       "                 Набор данных  RMSE_fit  Время обучения  Время предсказания  \n",
       "0                 X_y_set_cut   1282.51        1.696325            0.031263  \n",
       "1        X_y_set_with_missing   1817.00        9.865913            0.858024  \n",
       "2                 X_y_set_cut   1318.87        0.527492            0.327873  \n",
       "3        X_y_set_with_missing   1962.97        1.478813            0.696383  \n",
       "4      X_y_scaled_ordinal_set   1738.48        5.527415            0.107008  \n",
       "5  X_y_scaled_ordinal_set_cut   1282.51        2.380046            0.068185  \n",
       "6      X_y_scaled_ordinal_set   1772.74        0.705230            0.391032  \n",
       "7  X_y_scaled_ordinal_set_cut   1295.74        0.354567            0.316859  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7ba9ff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание словаря выборок для алгоритма  DecisionTreeRegressor\n",
    "datasets3 = {'X_y_scaled_ordinal_set': X_y_scaled_ordinal_set, 'X_y_scaled_ordinal_set_cut': X_y_scaled_ordinal_set_cut}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e79bd783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Параметры лучшей модели алгоритма DecisionTreeRegressor(random_state=123) {'max_depth': 14}\n",
      "Параметры лучшей модели алгоритма DecisionTreeRegressor(random_state=123) {'max_depth': 13}\n"
     ]
    }
   ],
   "source": [
    "# Создание словаря параметров алгоритма DecisionTreeRegressor под выборки из datasets3\n",
    "params_dt = {'max_depth': [x for x in range(10, 16)]}\n",
    "tab_metrics, best_model_fit = model_evaluator.evaluate_models(datasets3, decision_tree_regressor, params_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f1bdccfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Алгоритм модели</th>\n",
       "      <th>Набор данных</th>\n",
       "      <th>RMSE_fit</th>\n",
       "      <th>Время обучения</th>\n",
       "      <th>Время предсказания</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;catboost.core.CatBoostRegressor object at 0x0...</td>\n",
       "      <td>X_y_set_cut</td>\n",
       "      <td>1282.51</td>\n",
       "      <td>1.696325</td>\n",
       "      <td>0.031263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;catboost.core.CatBoostRegressor object at 0x0...</td>\n",
       "      <td>X_y_set_with_missing</td>\n",
       "      <td>1817.00</td>\n",
       "      <td>9.865913</td>\n",
       "      <td>0.858024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LGBMRegressor(extra_trees=True, n_estimators=4...</td>\n",
       "      <td>X_y_set_cut</td>\n",
       "      <td>1318.87</td>\n",
       "      <td>0.527492</td>\n",
       "      <td>0.327873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LGBMRegressor(extra_trees=True, n_estimators=4...</td>\n",
       "      <td>X_y_set_with_missing</td>\n",
       "      <td>1962.97</td>\n",
       "      <td>1.478813</td>\n",
       "      <td>0.696383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;catboost.core.CatBoostRegressor object at 0x0...</td>\n",
       "      <td>X_y_scaled_ordinal_set</td>\n",
       "      <td>1738.48</td>\n",
       "      <td>5.527415</td>\n",
       "      <td>0.107008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;catboost.core.CatBoostRegressor object at 0x0...</td>\n",
       "      <td>X_y_scaled_ordinal_set_cut</td>\n",
       "      <td>1282.51</td>\n",
       "      <td>2.380046</td>\n",
       "      <td>0.068185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LGBMRegressor(n_estimators=40, random_state=123)</td>\n",
       "      <td>X_y_scaled_ordinal_set</td>\n",
       "      <td>1772.74</td>\n",
       "      <td>0.705230</td>\n",
       "      <td>0.391032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LGBMRegressor(n_estimators=40, random_state=123)</td>\n",
       "      <td>X_y_scaled_ordinal_set_cut</td>\n",
       "      <td>1295.74</td>\n",
       "      <td>0.354567</td>\n",
       "      <td>0.316859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DecisionTreeRegressor(random_state=123)</td>\n",
       "      <td>X_y_scaled_ordinal_set</td>\n",
       "      <td>1950.76</td>\n",
       "      <td>0.704010</td>\n",
       "      <td>0.062538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DecisionTreeRegressor(random_state=123)</td>\n",
       "      <td>X_y_scaled_ordinal_set_cut</td>\n",
       "      <td>1456.52</td>\n",
       "      <td>0.383798</td>\n",
       "      <td>0.033359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Алгоритм модели  \\\n",
       "0  <catboost.core.CatBoostRegressor object at 0x0...   \n",
       "1  <catboost.core.CatBoostRegressor object at 0x0...   \n",
       "2  LGBMRegressor(extra_trees=True, n_estimators=4...   \n",
       "3  LGBMRegressor(extra_trees=True, n_estimators=4...   \n",
       "4  <catboost.core.CatBoostRegressor object at 0x0...   \n",
       "5  <catboost.core.CatBoostRegressor object at 0x0...   \n",
       "6   LGBMRegressor(n_estimators=40, random_state=123)   \n",
       "7   LGBMRegressor(n_estimators=40, random_state=123)   \n",
       "8            DecisionTreeRegressor(random_state=123)   \n",
       "9            DecisionTreeRegressor(random_state=123)   \n",
       "\n",
       "                 Набор данных  RMSE_fit  Время обучения  Время предсказания  \n",
       "0                 X_y_set_cut   1282.51        1.696325            0.031263  \n",
       "1        X_y_set_with_missing   1817.00        9.865913            0.858024  \n",
       "2                 X_y_set_cut   1318.87        0.527492            0.327873  \n",
       "3        X_y_set_with_missing   1962.97        1.478813            0.696383  \n",
       "4      X_y_scaled_ordinal_set   1738.48        5.527415            0.107008  \n",
       "5  X_y_scaled_ordinal_set_cut   1282.51        2.380046            0.068185  \n",
       "6      X_y_scaled_ordinal_set   1772.74        0.705230            0.391032  \n",
       "7  X_y_scaled_ordinal_set_cut   1295.74        0.354567            0.316859  \n",
       "8      X_y_scaled_ordinal_set   1950.76        0.704010            0.062538  \n",
       "9  X_y_scaled_ordinal_set_cut   1456.52        0.383798            0.033359  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cde8131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание словаря выборок для алгоритма Ridge\n",
    "datasets4 = {'X_y_scaled_ohe_set_cut': X_y_scaled_ohe_set_cut}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "be219a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Недостаточно памяти, удалим некоторые переменные\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cff41681",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_with_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e3958572",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1b7bb954",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_y_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4567098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_y_set_with_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "84d3864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_y_set_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a0a6f8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_y_scaled_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f946e8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_y_scaled_set_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2a9df09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Параметры лучшей модели алгоритма Ridge(random_state=123, solver='sparse_cg') {'alpha': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# Создание словаря параметров алгоритма Ridge под выборку из datasets4\n",
    "params_ridge = {'alpha': np.logspace(-2, 2, 4)}\n",
    "tab_metrics, best_model_fit = model_evaluator.evaluate_models(datasets4, ridge, params_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f95abb4",
   "metadata": {},
   "source": [
    "Выведем таблицу с показателя по лучшим моделям рассмотренных алгоритмов на обучении с кросс-валидацией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5495a9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Алгоритм модели</th>\n",
       "      <th>Набор данных</th>\n",
       "      <th>RMSE_fit</th>\n",
       "      <th>Время обучения</th>\n",
       "      <th>Время предсказания</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;catboost.core.CatBoostRegressor object at 0x0...</td>\n",
       "      <td>X_y_set_cut</td>\n",
       "      <td>1282.51</td>\n",
       "      <td>1.696325</td>\n",
       "      <td>0.031263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;catboost.core.CatBoostRegressor object at 0x0...</td>\n",
       "      <td>X_y_set_with_missing</td>\n",
       "      <td>1817.00</td>\n",
       "      <td>9.865913</td>\n",
       "      <td>0.858024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LGBMRegressor(extra_trees=True, n_estimators=4...</td>\n",
       "      <td>X_y_set_cut</td>\n",
       "      <td>1318.87</td>\n",
       "      <td>0.527492</td>\n",
       "      <td>0.327873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LGBMRegressor(extra_trees=True, n_estimators=4...</td>\n",
       "      <td>X_y_set_with_missing</td>\n",
       "      <td>1962.97</td>\n",
       "      <td>1.478813</td>\n",
       "      <td>0.696383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;catboost.core.CatBoostRegressor object at 0x0...</td>\n",
       "      <td>X_y_scaled_ordinal_set</td>\n",
       "      <td>1738.48</td>\n",
       "      <td>5.527415</td>\n",
       "      <td>0.107008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;catboost.core.CatBoostRegressor object at 0x0...</td>\n",
       "      <td>X_y_scaled_ordinal_set_cut</td>\n",
       "      <td>1282.51</td>\n",
       "      <td>2.380046</td>\n",
       "      <td>0.068185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LGBMRegressor(n_estimators=40, random_state=123)</td>\n",
       "      <td>X_y_scaled_ordinal_set</td>\n",
       "      <td>1772.74</td>\n",
       "      <td>0.705230</td>\n",
       "      <td>0.391032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LGBMRegressor(n_estimators=40, random_state=123)</td>\n",
       "      <td>X_y_scaled_ordinal_set_cut</td>\n",
       "      <td>1295.74</td>\n",
       "      <td>0.354567</td>\n",
       "      <td>0.316859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DecisionTreeRegressor(random_state=123)</td>\n",
       "      <td>X_y_scaled_ordinal_set</td>\n",
       "      <td>1950.76</td>\n",
       "      <td>0.704010</td>\n",
       "      <td>0.062538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DecisionTreeRegressor(random_state=123)</td>\n",
       "      <td>X_y_scaled_ordinal_set_cut</td>\n",
       "      <td>1456.52</td>\n",
       "      <td>0.383798</td>\n",
       "      <td>0.033359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ridge(random_state=123, solver='sparse_cg')</td>\n",
       "      <td>X_y_scaled_ohe_set_cut</td>\n",
       "      <td>1859.73</td>\n",
       "      <td>6.884200</td>\n",
       "      <td>0.212113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Алгоритм модели  \\\n",
       "0   <catboost.core.CatBoostRegressor object at 0x0...   \n",
       "1   <catboost.core.CatBoostRegressor object at 0x0...   \n",
       "2   LGBMRegressor(extra_trees=True, n_estimators=4...   \n",
       "3   LGBMRegressor(extra_trees=True, n_estimators=4...   \n",
       "4   <catboost.core.CatBoostRegressor object at 0x0...   \n",
       "5   <catboost.core.CatBoostRegressor object at 0x0...   \n",
       "6    LGBMRegressor(n_estimators=40, random_state=123)   \n",
       "7    LGBMRegressor(n_estimators=40, random_state=123)   \n",
       "8             DecisionTreeRegressor(random_state=123)   \n",
       "9             DecisionTreeRegressor(random_state=123)   \n",
       "10        Ridge(random_state=123, solver='sparse_cg')   \n",
       "\n",
       "                  Набор данных  RMSE_fit  Время обучения  Время предсказания  \n",
       "0                  X_y_set_cut   1282.51        1.696325            0.031263  \n",
       "1         X_y_set_with_missing   1817.00        9.865913            0.858024  \n",
       "2                  X_y_set_cut   1318.87        0.527492            0.327873  \n",
       "3         X_y_set_with_missing   1962.97        1.478813            0.696383  \n",
       "4       X_y_scaled_ordinal_set   1738.48        5.527415            0.107008  \n",
       "5   X_y_scaled_ordinal_set_cut   1282.51        2.380046            0.068185  \n",
       "6       X_y_scaled_ordinal_set   1772.74        0.705230            0.391032  \n",
       "7   X_y_scaled_ordinal_set_cut   1295.74        0.354567            0.316859  \n",
       "8       X_y_scaled_ordinal_set   1950.76        0.704010            0.062538  \n",
       "9   X_y_scaled_ordinal_set_cut   1456.52        0.383798            0.033359  \n",
       "10      X_y_scaled_ohe_set_cut   1859.73        6.884200            0.212113  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8b041b",
   "metadata": {},
   "source": [
    "По результатам обучения можно сделать следующие выводы:\n",
    "\n",
    "- Алгоритм `CatBoostRegressor` в зависимоти от качества используемых данных показывает одно из самых высоких показателей затрат времени на обучение (1.69-9.87s), при этом дает самые лучшие результы метрики `RMSE` на кроссвалидации на выборке с данными, очищенными от выбросов, аномалий и дубликатов  и на выборке с данными, очищенными от выбросов, аномалий и дубликатов, а так масштабированными количественными признаками и кодированными категориальными признаками (1282.51 - номер 0 в таблице).\n",
    "\n",
    "- Алгоритм `LGBMRegressor` показывает средние затрат времени на обучение (0.35-1.48s), очень хорошие результы метрики `RMSE` на кроссвалидации этот алгоритм показывает на выборке с данными, очищенными от выбросов, аномалий и дубликатов с  масштабированными количественными признаками и кодированными категориальными признаками (1295.74 - номер 7 в таблице) .\n",
    "\n",
    "- Алгоритм `DecisionTreeRegressor` дает самый лучший показатель затрат времени на обучение (0.38-0.70s), хорошие результы метрики `RMSE` на кроссвалидации этот алгоритм показывает на выборке с данными, очищенными от выбросов, аномалий и дубликатов  с масштабированными количественными признаками и кодированными категориальными признаками (1456.52 - номер 9 в таблице).\n",
    "\n",
    "- Алгоритм `Ridge` обучается свысокими затратами времени (6.89s), показывает один из худших качеств модели (метрика `RMSE` на кроссвалидации - 1859.73 - номер 10 в таблице), при этом мы использовали только очищенные данные с масштабированными количественными признаками и кодированными категориальными признаками. \n",
    "\n",
    "Опытным путем проверили, что алгоритмы `CatBoostRegressor` и `LGBMRegressor` могут работать на сырых данных (с пропусками, дубликатами, не преобразованными количественными и категориальными признаками). При этом, алгоритмы смогли показать качество моделей на сырых данных лучше, чем было поставленно в задаче (1817.00, 1962.97).\n",
    "\n",
    "Все бех исключения алгоримы отработали хороши, заявленный клиентом размер метрики `RMSE` (2500) не был превышем не смотря на то, что были использованы данный разной степени обработки.\n",
    "\n",
    "Самое быстрое предсказание мы получаем от модели алгоритма `DecisionTreeRegressor`(0.033s) и от модели алгорима `CatBoostRegressor`(0.031)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5955e724",
   "metadata": {},
   "source": [
    "**Вывод**\n",
    "\n",
    "Лучшей моделью по качеству и затратам времени на обучение и предсказание можно назвать модель алгоритма машинного обучения `LGBMRegressor` на очищенных данных, преобразованных категориальных и количественных признаков (номер 7 в таблице)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd034ea",
   "metadata": {},
   "source": [
    "### Проверка лучшей модели на адекватность"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde9e01",
   "metadata": {},
   "source": [
    "Для сравнения выведем данные показателя класса `DummyRegressor`, что бы показать насколько может быть обманчива точность.\n",
    "\n",
    "Проверим модель на адекватность, сравним качество константной модели с качеством предсказания лучшей модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "142ac0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Лучшая модель RMSE: 1305.11\n",
      "Константная модель RMSE: 3436.07\n"
     ]
    }
   ],
   "source": [
    "model_evaluator.predict_dummy_model(X_y_scaled_ordinal_set_cut, best_model_fit[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b928c2c3",
   "metadata": {},
   "source": [
    "Лучшая модель `LGBMRegressor` показала качество лучше, чем константная модель.\n",
    "\n",
    "Можно сделать вывод:\n",
    "\n",
    "Размер метрики выбранной модели приемлемый (1305.11), так как она меньше чем у константной модели (3436.07)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0ddb5b",
   "metadata": {},
   "source": [
    "<a id = 'test'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d9ebe",
   "metadata": {},
   "source": [
    "## Проверка лучшей модели на тестовой выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa4ed78",
   "metadata": {},
   "source": [
    "Проверим качество отобранной модели алгоритма `LGBMRegressor`на тестовой выборке. Для проверки модели возьмем даные очищенные, с преобразованными категориальными и количественными признаками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "82f03dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Test RMSE 1305.1079419744317\n",
      "Prediction Time 0.5213892459869385\n"
     ]
    }
   ],
   "source": [
    "model_evaluator.test_best_model(X_y_scaled_ordinal_set_cut, best_model_fit[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a6ccb9",
   "metadata": {},
   "source": [
    "На тестовой выборке результаты лучшей модели оказались лучше, чем требовалось в задаче (значение метрики `RMSE` должно быть меньше 2500).\n",
    "\n",
    "Можем констатировать факт, что если важными критериями для оценки модели является одновременно точность, минимальные затраты времени на обучение предсказание, алгоритм машинного обучения `LGBMRegressor` показывает лучшие результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee66716",
   "metadata": {},
   "source": [
    "**Примечание**\n",
    "\n",
    "Проверили эту же модель не на идельных данных, как выше в тексте, а на частично преобразованных данных (то есть в данных есть неявные дубликаты, выбросы). Результат ожидаемо оказался хуже. Если время предсказания увеличилось на 80%, то качество предсказания модели хуже, чем в предложено в задании (выше 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "39fdcc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Test RMSE 2616.542294361984\n",
      "Prediction Time 0.1562819480895996\n"
     ]
    }
   ],
   "source": [
    "model_evaluator.test_best_model(X_y_scaled_ordinal_set, best_model_fit[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed2c32",
   "metadata": {},
   "source": [
    "**Примечание**\n",
    "\n",
    "Попытка использовать алгоритм обучения Линейная Регрессия не увенчался успехом. Метрика `RMSE` показала очень высокие значения - 1.753679e+06. \n",
    "\n",
    "Исправить ситуацию не получилось. С чем связана такой большой размер метрики не понятно. \n",
    "\n",
    "Возможно на такие некорректные результаты повляло то, что в целевой переменной большая дисперсия данных. Модель линейной регрессии чувствительна к выбросам (но выбросы мы удалили)и большой дисперсии в данных. Так же влияние оказывает мультиколлинеарность признаков (но такого не наблюдается). Количественные признаки были масштабированы, а к категориальным признакам применили кодировку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "36861fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание словаря выборок для алгоритмов LinearRegression\n",
    "datasets5 = {'X_y_scaled_ohe_set_cut': X_y_scaled_ohe_set_cut, 'X_y_scaled_ohe_set': X_y_scaled_ohe_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c79fc85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Параметры лучшей модели алгоритма LinearRegression() {}\n",
      "Параметры лучшей модели алгоритма LinearRegression() {}\n"
     ]
    }
   ],
   "source": [
    "params_linear_regression = {}\n",
    "tab_metrics_lr, best_model_fit = model_evaluator.evaluate_models(datasets5, linear_regression, params_linear_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b02dc269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Алгоритм модели</th>\n",
       "      <th>Набор данных</th>\n",
       "      <th>RMSE_fit</th>\n",
       "      <th>Время обучения</th>\n",
       "      <th>Время предсказания</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;catboost.core.CatBoostRegressor object at 0x0...</td>\n",
       "      <td>X_y_set_cut</td>\n",
       "      <td>1282.51</td>\n",
       "      <td>1.696325</td>\n",
       "      <td>0.031263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;catboost.core.CatBoostRegressor object at 0x0...</td>\n",
       "      <td>X_y_set_with_missing</td>\n",
       "      <td>1817.00</td>\n",
       "      <td>9.865913</td>\n",
       "      <td>0.858024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LGBMRegressor(extra_trees=True, n_estimators=4...</td>\n",
       "      <td>X_y_set_cut</td>\n",
       "      <td>1318.87</td>\n",
       "      <td>0.527492</td>\n",
       "      <td>0.327873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LGBMRegressor(extra_trees=True, n_estimators=4...</td>\n",
       "      <td>X_y_set_with_missing</td>\n",
       "      <td>1962.97</td>\n",
       "      <td>1.478813</td>\n",
       "      <td>0.696383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;catboost.core.CatBoostRegressor object at 0x0...</td>\n",
       "      <td>X_y_scaled_ordinal_set</td>\n",
       "      <td>1738.48</td>\n",
       "      <td>5.527415</td>\n",
       "      <td>0.107008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;catboost.core.CatBoostRegressor object at 0x0...</td>\n",
       "      <td>X_y_scaled_ordinal_set_cut</td>\n",
       "      <td>1282.51</td>\n",
       "      <td>2.380046</td>\n",
       "      <td>0.068185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LGBMRegressor(n_estimators=40, random_state=123)</td>\n",
       "      <td>X_y_scaled_ordinal_set</td>\n",
       "      <td>1772.74</td>\n",
       "      <td>0.705230</td>\n",
       "      <td>0.391032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LGBMRegressor(n_estimators=40, random_state=123)</td>\n",
       "      <td>X_y_scaled_ordinal_set_cut</td>\n",
       "      <td>1295.74</td>\n",
       "      <td>0.354567</td>\n",
       "      <td>0.316859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DecisionTreeRegressor(random_state=123)</td>\n",
       "      <td>X_y_scaled_ordinal_set</td>\n",
       "      <td>1950.76</td>\n",
       "      <td>0.704010</td>\n",
       "      <td>0.062538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DecisionTreeRegressor(random_state=123)</td>\n",
       "      <td>X_y_scaled_ordinal_set_cut</td>\n",
       "      <td>1456.52</td>\n",
       "      <td>0.383798</td>\n",
       "      <td>0.033359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ridge(random_state=123, solver='sparse_cg')</td>\n",
       "      <td>X_y_scaled_ohe_set_cut</td>\n",
       "      <td>1859.73</td>\n",
       "      <td>6.884200</td>\n",
       "      <td>0.212113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>X_y_scaled_ohe_set_cut</td>\n",
       "      <td>1859.65</td>\n",
       "      <td>5.156890</td>\n",
       "      <td>0.171923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>X_y_scaled_ohe_set</td>\n",
       "      <td>2667.93</td>\n",
       "      <td>6.553317</td>\n",
       "      <td>0.344253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Алгоритм модели  \\\n",
       "0   <catboost.core.CatBoostRegressor object at 0x0...   \n",
       "1   <catboost.core.CatBoostRegressor object at 0x0...   \n",
       "2   LGBMRegressor(extra_trees=True, n_estimators=4...   \n",
       "3   LGBMRegressor(extra_trees=True, n_estimators=4...   \n",
       "4   <catboost.core.CatBoostRegressor object at 0x0...   \n",
       "5   <catboost.core.CatBoostRegressor object at 0x0...   \n",
       "6    LGBMRegressor(n_estimators=40, random_state=123)   \n",
       "7    LGBMRegressor(n_estimators=40, random_state=123)   \n",
       "8             DecisionTreeRegressor(random_state=123)   \n",
       "9             DecisionTreeRegressor(random_state=123)   \n",
       "10        Ridge(random_state=123, solver='sparse_cg')   \n",
       "11                                 LinearRegression()   \n",
       "12                                 LinearRegression()   \n",
       "\n",
       "                  Набор данных  RMSE_fit  Время обучения  Время предсказания  \n",
       "0                  X_y_set_cut   1282.51        1.696325            0.031263  \n",
       "1         X_y_set_with_missing   1817.00        9.865913            0.858024  \n",
       "2                  X_y_set_cut   1318.87        0.527492            0.327873  \n",
       "3         X_y_set_with_missing   1962.97        1.478813            0.696383  \n",
       "4       X_y_scaled_ordinal_set   1738.48        5.527415            0.107008  \n",
       "5   X_y_scaled_ordinal_set_cut   1282.51        2.380046            0.068185  \n",
       "6       X_y_scaled_ordinal_set   1772.74        0.705230            0.391032  \n",
       "7   X_y_scaled_ordinal_set_cut   1295.74        0.354567            0.316859  \n",
       "8       X_y_scaled_ordinal_set   1950.76        0.704010            0.062538  \n",
       "9   X_y_scaled_ordinal_set_cut   1456.52        0.383798            0.033359  \n",
       "10      X_y_scaled_ohe_set_cut   1859.73        6.884200            0.212113  \n",
       "11      X_y_scaled_ohe_set_cut   1859.65        5.156890            0.171923  \n",
       "12          X_y_scaled_ohe_set   2667.93        6.553317            0.344253  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab_metrics_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c573a7b",
   "metadata": {},
   "source": [
    "Алгоритм линейная регрессия показывает результы качества и времени обучения хуже других моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f548905",
   "metadata": {},
   "source": [
    "## Итоги и выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73562f0c",
   "metadata": {},
   "source": [
    "В рамках данного проекта были предоставлены данные  сервиса по продаже автомобилей с пробегом «Не бит, не крашен». \n",
    "\n",
    "В Данных присутствуют технические характеристики, комплектация и цены на автомобили.\n",
    "\n",
    "Чтобы привлечь новых клиентов, сервис разрабатывает приложение. В приложении можно будет узнать рыночную стоимость своего автомобиля согласно представленным характеристикам авто.\n",
    "\n",
    "Целю проекта является модель алгоритма машинного обучения, которая умеет определять цену.\n",
    "\n",
    "Заказчик в свою очередь определил важные критерии качества модели:\n",
    "\n",
    "- качество предсказания,\n",
    "\n",
    "- время обучения модели,\n",
    "\n",
    "- время предсказания модели.\n",
    "\n",
    "В датафрейме встречаются количественные, категориальные типы и столбцы с датами.\n",
    "\n",
    "В датафрейме 354369 объектов и 16 признаков (характеристик) для них.\n",
    "\n",
    "Целвой признак `Price`.\n",
    "\n",
    "В 5 столбцах обнаружены пропущенные данных - 52%. Самое большой процент пропущенных значений имеем в столбце `Repaired` - 20%, в столбце `VehicleType` - 11%, `FuelType` - 9%, а столбцах `Gearbox` и `Model` - 6%. \n",
    "\n",
    "В данных обнаружены аномальные значения.\n",
    "\n",
    "Определили информативные и не информативные столбцы.\n",
    "\n",
    "Детальную информацию можно посмотреть по ссылке  [изучение данных](#df)\n",
    "\n",
    "Так как в процессе обучения моделей машинного обучения мы работали с алгоритмами, которые обладают механизмами для работы с пропущенными значениями, создали копию начального датасета, в котором не удаляли пропуски в данных, удалили только выявленные дубликаты объектов.\n",
    "\n",
    "Поскольку не было понятно, как восстановить данные в пропусках по столбцу `Model` и чтоб не удалять объекты, замели пропуски в этом столбце на категорию \"other\".\n",
    "\n",
    "Посчитали, что если у объекта отсутствуют данные о ремонте машины, ремонта не было - заменим пропуски на значение `no`.\n",
    "\n",
    "Восстановить пропуски в столбцах `VehicleType`, `Gearbox`, `FuelType`, по столбцам `Brand`, `Model` по моде этих признаков в группировках.\n",
    "\n",
    "Детальную информацию можно посмотреть по ссылке  [обработка пропусков](#none)\n",
    "\n",
    "Обнаружены в таблице дублика как явные, так и не явные. Но в условиях того, что мы удалили ряд столбцов неявные дубликаты становятся явными. Учли этот момент, создали дополнителую выборку, в которй удалили неявные дубликаты. Ими окзалось порядка 35тыс объектов (9,8% от всей выборки).\n",
    "\n",
    "Детальную информацию можно посмотреть по ссылке  [дубликаты](#duplicates)\n",
    "\n",
    "Нашли и удалили выбросы, порядка 18%. При этом рассмотрели границы, по которым удаляли выбросы. Границы некоторых количественных признаков не соотствуют здравому смыслу, в данных есть аномальные значения. Часть объектов с аномальными значениями удалили, например: объекты с ценой авто ниже 300 дол, объекты, где аномольные значения встречаются более чем в одном признаке. Для части объектов с анамальными значения в одном признаке нашли замену (на медианные значения в группе авто такого же бренда и модели).\n",
    "\n",
    "Детальную информацию можно посмотреть по ссылке  [выбросы и аномалии](#with_missing)\n",
    "\n",
    "Матрица корреляции показывает, что между независимыми признаками `RegistrationYear`, `Power` и целевой переменной `Price` существуют хорошая положительная корреляция, а между независимым признаком `Kilometer` и целевой переменной `Price` наблюдается слабая отрицательная корреляция. Предположили, что линейные модели покажут хороший результат.\n",
    "\n",
    "Детальную информацию можно посмотреть по ссылке  [корреляция признаков](#corr)\n",
    "\n",
    "Выбрали следующие регрессоры для обучения модели:\n",
    "\n",
    "- `LinearRegression`\n",
    "\n",
    "- `Ridge`\n",
    "\n",
    "- `DecisionTreeRegressor`\n",
    "\n",
    "- `CatBoostRegressor`\n",
    "\n",
    "- `LGBMRegressor`\n",
    "\n",
    "В качестве метрики для моделей использовать `RMSE`. По условию задания значение метрики `RMSE` должно быть меньше 2500. Качество моделей проверяли на кроссвалидации.\n",
    "\n",
    "Провели подготовку выборок передобучением:\n",
    "\n",
    "- разделили на обущающую и тернировочную,\n",
    "\n",
    "- провели кодирование категориальных признаков,\n",
    "\n",
    "- провели масштабирование количественных признаков.\n",
    "\n",
    "В результате преобразования данных в выборках получили несколько наборов выборок для обучения алгоритмов машиного обучения.\n",
    "\n",
    "Детальную информацию можно посмотреть по ссылке  [подготовка выборок](#cod_scaler)\n",
    "\n",
    "Провеодили обучения на пяти алгоритмах машинного обучения `CatBoostRegressor`, `LGBMRegressor`, `DecisionTreeRegressor`, `Ridge`, `LinearRegression`. \n",
    "\n",
    "Использовали 4 выборки: выборка на сырых данных; выборка на данных , очищенных от выбросов, анамалий и дубликатов; выборка не очищенная от выбросов и дубликатов, со скалированными количественными признаками и кодированными категориальными признаками и выборка на данных , очищенных от выбросов, анамалий и дубликатов, со скалированными количественными признаками и кодированными категориальными признаками.\n",
    "\n",
    "По результатам обучения можно сделать следующие выводы:\n",
    "\n",
    "**Алгоритм `LGBMRegressor` показывает хорошие показатели затрат времени на обучение (0.35-1.48s), одни из лучших результов метрики `RMSE`** на кроссвалидации (1295.74).\n",
    "\n",
    "Алгоритм `CatBoostRegressor` показывает самые высокие показатели затрат времени на обучение (1.69-9.87s) , лучшие среди других моделей результы метрики `RMSE` на кроссвалидации (1282.51).\n",
    "\n",
    "Алгоритм `DecisionTreeRegressor` показывает самые назкие затрат времени на обучение (0.38-0.70s), хорошие результы метрики `RMSE` на кроссвалидации  (1456.52).\n",
    "\n",
    "Алгоритм `Ridge` показал один из худших показателей затрат времени на обучение (6.88s), высокий по сравнению с дугими моделями алгоритмов машинного обучения показатель метрики `RMSE` на кроссвалидации (1859.73).\n",
    "\n",
    "Все модели в результате показали качество лучше, чем было поставлено в задаче.\n",
    "\n",
    "Детальную информацию можно посмотреть по ссылке  [обучение](#fit)\n",
    "\n",
    "На тестовой выборке результаты лучшей модели алгоритма машинного обучения `LGBMRegressor` показали метрику `RMSE` 1305.11 при затратах времени на предсказание 0.52s (значение метрики `RMSE`, согласно задания, должно быть меньше 2500).\n",
    "\n",
    "Можем констатировать факт, что если важными критериями для оценки модели является одновременно точность, минимальные затраты времени на обучение и  предсказание, алгоритм машинного обучения `LGBMRegressor` показывает лучшие результаты.\n",
    "\n",
    "Детальную информацию можно посмотреть по ссылке  [проверка на тесте](#test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "226.983px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
